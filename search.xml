<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[闲聊 | 读书]]></title>
    <url>%2F2018%2F12%2F26%2Frecommend-book%2F</url>
    <content type="text"><![CDATA[记录一些读过的书籍，都颇有感触。 或许以前读书的境界还没够，还未达到一定的水准。最近每每看书都有很多感慨，读书能让你见识更多。作者把自己的亲身实践，通过文字的形式向我们展示，透过这些文字我们可以直接去借鉴。 《战后日本经济史》，这本书是讲日本是如何崛起的，其实我们现在的状况跟九几年的日本很相似，想想还未能为祖国做贡献，颇有无奈。《不抱怨时间》《清单革命》，这两本书如何教你管理自己的时间，对于没有很强的自制力，非常适合，任务的完成总要有个轻重缓急，学会安排自己的时间，让自己过的更充实些。 《人类群星闪耀时》，讲述 13 个非常伟大的故事，在历史上总是有那么几个决定性的英雄人物，在任何时代都是存在的。成为英雄有时也就在那么一瞬间。 《当下的力量》，讲述如何提高自己，文字是字字铿锵有力，里面有讲冥想的部分确实不错，让我们学会接受不完美的自己，感受自己的存在。世界上有六十多亿人口，每个人都有自己的活法，不要试图去效仿别人，做独一无二的自己。 下面还有几本，还没有看完，等看完之后在来更新。《个人印象》《终身成长：重新定义成功的思维模式》《昨日的世界》《终身成长》]]></content>
      <tags>
        <tag>读书</tag>
        <tag>闲聊</tag>
        <tag>笔记</tag>
        <tag>文学</tag>
        <tag>成长</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[投资理财的一些看法]]></title>
    <url>%2F2018%2F10%2F27%2Finverst-think%2F</url>
    <content type="text"><![CDATA[这两天看了关于一些投资理财的书籍，可能是这段时间被穷所迫，充满了对 money 的喜爱，要致力于创造属于自己的财富（脑补）。下面是对「邻家的百万富翁」这本书的一些见解，有些内容是从书中摘录的。 这本书在看着的过程中，传达的思想感觉是节俭，节俭在节俭。并没有在如何投资理财这块做很详细的说明。 书中主要是对积累财富，衣食住行，教育子女，以及遗产分配等讲解如何获取更多的钱。遗产分配想必在我们这里是不需要考虑太多，大部分都是直接继承，没有所谓的遗产锐等。2016 年我们才引入遗产锐。 在积累财富的过程中，提倡的就是节俭，不要为了虚荣心去花没有必要的钱，年轻的时候有条件的话，就应该为投资理财进行提前规划。书中对百万富翁的调查，发现大部分人都是很节俭的，并没有我们想像中的那样炫富，其中很大一部分都是第一代白手起家起来的。 书中关于教育子女这块是非常不错的，讲述了如何教会子女，不要成为“经济门诊病人照顾”，学会不依赖于父母，最后能够在金钱上自立。在教育这块也分析了为什么会有富不过三代的这样说法，其实是很有道理的。 可能会有一些人认为钱是理出来的，而不是省出来的，我也认同这样的观点。当随着我们的收入不断的提高的时候，靠的就是我们是不是能够克已自律，如果在收入增加的时候，也能保持自律，并且能够合理的规划，想必这是最好的结果，本书只是对百万富翁的调查分析。 富有是一种心态而不是一种形式，可我们往往只注重形式，幻想可以挥金如土，可以尽情享受，希望被别人羡慕和赞许，所以很多人希望速富，渴望地位，渴望别人认可，所以在真正富裕之前，我们总是把自己打扮靓丽，买好车，住高端公寓，没事的时候爱挥挥手腕上的名牌手表、微薄晒晒今天又吃了什么山珍海味，和我们在大街上、电视里、电影里见到看上去是“富人”的人一样。 可能会有人不赞同上面的说法，因为人生就要享受，趁年轻更加得赶紧去消费，提升自己的生活品质，挣得多理所当然花得多。尤其是在中国，面子有时很重要很重要很重要，但我相信自己不会暴富，估计也没法年薪成百上千万，也不想为了钱在别人面前强颜欢笑，却同时阉割了自己的个性。我相信脚踏实地的不断积累，不需要为了显示自己富有和高贵的品味而去追求那些豪华高端的玩意，就像作者说的，真正的富人不会让你觉得他像个富人，但有个共通的地方，他们都有一个强大的内心世界。 强大到既不以外表论断他人，也不为自己外在的不足而庸人自扰。要以满足感的延迟来约束自己，过简朴的生活而不抱怨，默默为未来打下坚实的基础。这并非一本励志或洗脑的书，只是通过调查揭示了百万富翁踏实、勇敢、自律的一面，让我们有机会知道，拥有那些华丽的物件不是富有的象征，而自我品格的培养，才是心灵和物质双重富有的唯一出路。 自我反省中，希望与大家共勉。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>理财</tag>
        <tag>看法</tag>
        <tag>思考</tag>
        <tag>反省</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你不努力，就是你的错]]></title>
    <url>%2F2018%2F10%2F23%2Fwork-summary%2F</url>
    <content type="text"><![CDATA[生活每时每刻都是直播，没有预演，也不需要去猜测下一个时刻会发生什么，做好自己的每一步就行了，其它的就交给时间吧。 这两天经历了一些事，谈下自己这两天对工作和事业的思考。 大家选择工作或者未来的事业时，一定要注意两点 1 这是一份具有积累性的工作2 这是你感兴趣的 第一点关于积累，也就是说随着时间越长，你就越值钱，如果你所在的工作职位，没有任何积累性而言，可以考虑一下换了，比如学习，我们从小到大一直都在学习中，而且你也一定相信，现在的你比以前的你更优秀了，如果不是说明你就没有任何学习。 有句名言说的好，如果你现在不觉得一年前的自己是个傻逼，那说明你这一年没学到什么东西。 另外投资理财也是很具有积累性的，也是说经验越多越吃香，可以利用自己的经验给自己加成。 第二点关于兴趣，只有是你感兴趣的事情，你才可能做的更好更多的想着，如果你对它都不感兴趣，三天打鱼，两天晒网想必不会有任何意义，只是在浪费自己的时间，兴趣才是最好的老师，如何你喜欢做这件事，你也会享受做事情，带来的快乐。 跟打游戏一个性质，为何游戏会让人那么难以控制，就是游戏你感兴趣，你玩的开心。所以还是尽量选择自己感兴趣的事情去做，如果你一开始没有这个条件，那就尽量积累自己，等有资本了再去选择。 最后想说的，当自己工作步入稳定，如果觉得自己在这一行不能干到专家，那就要想着发展自己的第二项技能，防止自己失业。修炼第二项技能并不是说辞职，专门去在学习一项技能，而是说在业余时间，去修炼跟自己现在的职业相关的技能。]]></content>
      <tags>
        <tag>兴趣</tag>
        <tag>工作</tag>
        <tag>感触</tag>
        <tag>积累</tag>
        <tag>第二技能</tag>
        <tag>经历</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每个人都应该至少拼一把]]></title>
    <url>%2F2018%2F06%2F26%2Fread-think%2F</url>
    <content type="text"><![CDATA[其实，我们每一个人都是有天赋的，只是你没有去挖掘和坚持而已。很多情况下都是不愿努力在一些原本可能做好的事情上去做的更好，其中一个重要的原因就是我们缺乏必要的信念。我们觉得自己从来不具有这样的能力，或者是自认为已经错过了最佳的年龄，现在在学习已经为时已晚，更别说实现了。还记得我们小时候的梦想吗？又有几个是去坚持和努力实现的，大部分可能早已挥之而去，或者了解了社会，已经对自己说这是不可能实现的，而选择放弃或逃避，过着平常的生活。 我们每个人都有大量的个人经历可以证明这一点：也许我们曾经梦想过精通某项技艺，但是无论我们多么努力，都无法达成愿望。或者，我们曾经认识的一些人在某些事情总是事半功倍。我们往往会说：“这就是天赋，他们生就如此”。 这就是我们很多人都面临的问题，不愿作出改变和前进，而还想着自己要是像他们一样努力，也会有同样的成就的。自己却一直停留在原步，其实只要你迈出第一步，可能就会发现停不下来了。在任何时候我们都要保持学习的精神，去坚持和专注性的练习。专注性的练习就是我们每个人天生就有的自然能力，这种能力也就是我们所说的“天赋”。 一万小时定律，大家可能都听过，也就是不管在任何行业任何领域，你如果坚持了一万小时，你就会在这个领域或行业有所成就，但这并不是每个人都可以做到的，有的人经过了一万个小时，但是还是没有有所成就，那是因为他没有进行刻意的练习。一万小时定律这里说的是仅限于一万小时的刻意练习，而不是其它形式的练习活动。 当你有刻意的去练习或坚持一项任务时，你会发现自己学习时非常有效率，心情也会非常愉快。比如：你要利用自己的业余时间去学习一项技能或者是爱好，当你设定好计划后，能够坚持下去，并进行刻意的练习，一段时间后会取得不错的效果，这时你自己可能会认为原来自己在这方面也是很有天赋的，这也是一种内在的自我激励，让你不断的前进。 例如，如果你在一个领域取得了很大的成就，如果让你再去学习其它技能的时候，不是自己擅长的领域，也就是我们不具有刻意练习的天赋时，相信你也一定能在其它领域取得不错的成就。就好比马云如果不去电商了，你也会相信他能在其它领域取得一定的成就。 我们大多数人只是想要变得更好一些而已，如果想要他人注意到我们的成长过程，我们无需达到接近世界级的水平，一点点进步就会非常明显，这是我们所需要的鼓励和支持，学会去展示自己。 最后一句简短的对话送给大家。 “当你 100 岁时，你唯一后悔的事情就是，60 岁时没有开始练习小提琴”。 “如果那样的话，你到现在就有 40 年的演奏经验了”。]]></content>
      <tags>
        <tag>生活</tag>
        <tag>努力</tag>
        <tag>感悟</tag>
        <tag>天赋</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet API]]></title>
    <url>%2F2018%2F06%2F21%2Fservlet-api%2F</url>
    <content type="text"><![CDATA[Sevelt 的框架的核心是 javax.servlet.Servlet接口，所有的Servlet都必须实现这一接口。在Servlet接口中定义了五个方法，其中有三个方法代表了Servlet的生命周期： init 方法：负责初始化Servlet对象 service 方法：辅助响应客户的请求 destroy 方法：当Servlet对象退出生命周期时，负责释放占用的资源 如果你的Servlet类扩展了HttpServlet类，你通常不必实现service方法，因为HttpServlet类已经实现了service方法，该方法的声明形式如下： protected void service(HttpServletRequest request,HttpServletResponse response)throws ServletException, IOEception; 在HttpServlet的service方法中，首先从HttpServletRequest对象中获取HTTP请求方式的信息，然后在根据请求方式调用相应的方法。例如：如果请求方式为GET，那么调用doGet方法：如果请求方式为POST，那么调用doPost方法。 在servlet中为什么可以直接调用req，resp的相应方法：多态，父类型的引用，可以指向子类的对象。 Servlet的生命周期可以分为三个阶段：1、初始化阶段 —servlet容器启动时自动装载某些Servlet —在servlet容器启动后，客户首先向Servlet发出请求 —Servlet类文件被更新后，重新装载Servlet —Servlet被装载后，Servlet容器创建一个Servlet实例并且调用Servlet的init（）方法进行初始化。在Servlet的整个生命周期中，init方法只会被调用一次。 2、响应客户请求阶段对于到达Servlet容器的客户请求，Servlet容器创建特定于这个请求的ServletRequest对象和ServletResponse对象，然后调用Servlet的service方法，service方法从ServletRequest对象获得客户请求信息、处理该请求，并通过ServletResponse对象相客户返回响应结果。 3、终止阶段当web应用被终止，或Servlet容器终止运行，或Servlet容器重新装载Servlet的新实例时，Servlet容器先调用Servlet的destroy方法，在destroy方法中，可以释放Servlet所占用的资源。 在javax.servlet.Servlet接口中定义了三个方法 init() service() destroy() 他们将分别在Servlet的不同阶段被调用。]]></content>
      <tags>
        <tag>Java</tag>
        <tag>Servlet</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何去学习网络安全]]></title>
    <url>%2F2018%2F06%2F18%2Fwebsafe-summary%2F</url>
    <content type="text"><![CDATA[对于一些不知道怎么去入手学习网络安全的，这里我给出一些学习资料的网址。可以根据这网站里面的视频学习或者通过做里面的挑战来提升自己。遇到不会的，自己可以去Google或百度上搜对应的writeup。然后自己在去实践，这样学习的更快，更有体会。 1、合天网络安全实验室 合天网络安全实验室——里面有各方面的视频，还提供了在线环境的练习，比较适合刚开始学网络安全的人来学习。有的练习是需要合氏币，这个可以通过自己去做里面的练习，会有奖励，还可以每天签到赚合氏币。 2、实验吧 实验吧——这个网站不仅提供了视频的教学还提供了挑战。另外还可以自己出题。 3、白帽学院 白帽学院——快乐学习的平台，这个跟前两个网站差不多。大家自己可以去体验。 4、网络安全实验室 网络安全实验室——里面有各种练习题，大部分是一些基础题，刚开始练习的可以去多做做。 5、360安全播报 360安全播报——可以通过这个去搜索对应的writeup，每一届的比赛都会在这上面公布，可以通过这个去关注有哪些比赛，然后去参加去实践。 6、补天 补天-全球最大的漏洞响应平台——可以通过这个去关注发现的新的漏洞，及时了解最新的信息。 7、XCTF_OJ XCTF_OJ——提供了很多练习题库，里面有很多值得去思考的题目，比较有意思，练习的差不多了可以多做做。 8、Freebuf ——也是比较推荐的网站，属于比较好的，没事的时候可以自己多看看。 9、乌云 乌云——是国内比较好的漏洞网站啦。 10、看雪学院 看雪学院——里面也有很多文章，无聊的时候也可以看看。 11、xfocus ——里面提供了各种工具的下载，还有文章。]]></content>
      <tags>
        <tag>安全</tag>
        <tag>网络</tag>
        <tag>黑客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java学习路线之四个阶段]]></title>
    <url>%2F2018%2F06%2F12%2Fjava-study-summary%2F</url>
    <content type="text"><![CDATA[写这篇总结，主要是记录下自己的学习经历，算是自己对知识的一个回顾。也给想要学习 Java 的提供一些参考，对于一些想要学习Java，又不知道从哪里下手，以及现在有哪些主流的 Java 技术。想必大家学习一门技术，前期都很想看到一些结果或成就，这样就比较容易激励自己学习下去，最好的办法就是实践，实践，实践！先说明一下我的情况，我是从大二才开始接触 Java，然后自己摸索，期间真是摸打滚爬过来的。选的是计算机专业，大一的时候还没有自己的笔记本，所以都是听课过来的，了解了一些概念性的东西，课上老师也有演示过一些程序，懂得思考之后，发现计算机有很多有趣的地方，比如想知道这个东西是如何实现的，为什么会出现这些东西。真不是知道当初天天打游戏的时候，为什么没有发现，要是发现的话，现在说不定早就那啥那啥了。到了大二的时候自己有了笔记本之后，就开始自己摸索，很好奇自己当初拿到笔记本竟然没有去打游戏，而是用来学习。下面开始说正事。 选语言开始学习的时候，经过自己搜索之后，发现有很多种语言，自己完全处于懵逼状态，不知道选哪个，也不知道每种语言都可以用来干什么，后来经过学长的指点还有身边的人都说 Java 好，自己慢慢的就入坑了。在选择要学习一门语言的时候，一定要选自己感兴趣的，而且要有自己的目标，不然的话，后期很难坚持下来，除非你有很强的自律性。每个人前进都是有自己的驱动力，所以找到属于自己的驱动力，才能保证你不断的进步。 Java第一阶段刚接触 Java 的时候，想必大家都是从环境配置开始的，这个里面的坑想必大家都踩过。对于还没有开始的同学，也有可能会经历这个阶段，说可能是因为现在的 Java1.6 之后安装默认是加到环境变量里面的，但是在安装过程中有可能会出现一些意外情况，导致不能加入成功，这是就需要我们手动加入了。在这里特说明下，在大学期间一定要把一些基础的课程学好，比如：计算机系统、算法、编译原理等，这个对后期的学习会有很大的影响，最简单的就是你在添加环境变量的时候，为什么要把路径添加到 Path 下面，而不是添加到其它下面，计算机是如何去识别访问这些东西。 我一开始学习 Java 的时候，是学长分享的视频，这个视频的好处就是在学习 Java 基础的时候，完全脱离一些集成的工具，就是用编辑器（notepad++）写好程序，手动通过命令行去编译，再执行，让你能够了解其中的原理，以及锻炼自己的动手能力。如果一上来就上你用 Eclipse 或者其它工具，把一些底层的东西屏蔽掉，开始学习的时候大家可能连 class 文件都没有见过，只是知道写了这行代码，运行之后它会出来什么结果。 这里给出我当初学习的视频，有点老但是基础知识都是一样的。学习 Java 基础知识的时候，应该多动手，多思考，很多时候，你想当然的事情，等你写出来运行一下，你就会发现不是这么一回事，不信你就试试。在学习视频的时候，有两种学习方法建议：方法一先把视频过一篇，在看视频的时候，记下知识点，看完视频之后，自己对着知识点，自己敲代码实现，实在想不出来的，回过头来在看视频。方法二边看视频边跟着敲代码，这样会比第一种方法相对容易一些，但是如果是这种方法学习的话，要记得回头多复习，不然很容易忘记。两种方法各有好处，第一种方法一开始学习比较慢，但是后面基础有了之后，就会上手很快，而且记得很牢固。第二种方法比第一种方法花的时间要相对的少一些，所以需要我们反复的去回顾。学习完以上内容之后，你应该对 Java 有了一定的了解，你可以使用 Java 语言写出来一些简单的程序，并且是使用最简单的编辑器。这个时候，可以不用着急进入下个阶段，给自己一两天的时间，对学习过的知识进行下总结。 在学习的过程中，你应该注重下面这些知识点，由于是自己总结的，有可能会有不对的地方，若有不对之处，还请指出。 知识点梳理：概念：面向对象的三大基本特征五大基本原则（当初让学长考我的时候第一个问的就是这个）、面向对象、面向过程、什么是多态、什么是继承、什么是封装。 集合：Collection 集合、List 集合、Set 集合、Map 集合 异常：Java 中异常处理机制和应用，自定义异常 IO：File 类，字符流、字节流、转换流、缓冲流、递归 网络编程：Socket 线程：线程的生命周期，Java 线程池，线程同步问题，线程死锁问题 继承和接口：Class，Interface 反射：动态代理 Mysql 和 JDBC 开发：Mysql 数据库，JDBC，DBUtils，DBCP连接池 书籍推荐：Head First Java, Java核心技术 视频获取：微信后台回复「javaweb学习资料」包含后面三个阶段。 Java 第二阶段Java 基础学习完之后，我是开始学习 Javaweb，在一开始的几天比较迷茫，因为感觉自己写的东西没有用处，比如写个计算器什么的，生活中没有什么用，可能是太过于看结果导致的。这个时候应该去做一些有趣的事情，学习新的知识，开发新的大陆，这就是我们的 Web 开发了，主要包括前端页面（HTML/CSS/JS），Servlet/JSP，以及 Mysql 相关的知识。这些视频在上面分享的视频里面已经包括了。 关于页面，这些内容对于 Java 后端来说，不是特别重要，但是你应该尽自己的最大能力让它漂亮，最起码可以入眼，这样的话，页面就不是什么问题了。接下来，就是学习的重头戏了，学习 Servlet/JSP 部分，这也是 Java 后端开发必须非常精通的部分，在学习 Web 这三部分的时候，这个部分是最花时间的。这个阶段学习的时候，要学会使用开发工具，比如 Eclipse 或者 IDEA 来学习。最后一部分，你就要学会使用数据库，Mysql 数据库是不错的入门选择，而且 Java 领域主流的关系型数据就是 Mysql，这部分其实你在学习 JDBC 的时候，就会接触到，因为 JDBC 也是属于数据库的一部分。不仅要学会使用 JDBC 操纵数据库，而且还要学会使用数据库客户端工具，比如 sqlyog，navicat 等。 知识点梳理：前端技术：HTML、CSS、JS、JQuery、Bootstrap JavaWeb 核心内容：Servlet、JSP、XML、HTTP、Ajax、过滤器、拦截器等 Mysql 和 JDBC：复习 推荐书籍：相关的 Web 书籍都可以，可以顺带着看 Java 编程思想 Java 第三阶段这个阶段是在你掌握第二阶段之后开始，如果学习了第二个阶段之后想找工作的话，还需要在学习一些主流的框架知识。目前比较主流的框架是 SSM 框架，既 Spring，SpringMVC，Mybatis。要学会这些框架的搭建，以及用它们作出一个简单的 WEB 项目，包括增删改查的功能。在这里一开始，你可以不用太去关心那些配置文件，以及为什么会这样配置，这个可以留到后面慢慢了解，开始的时候先让自己有个体验，激励自己学习的动力。 搭建这三个框架的时候，一定要记录自己搭建的过程，这个在你工作之后肯定会用到的。在搭建的过程，我们通过网上查找资料或是跟着视频学习，都会接触到 Maven 这个工具，这个工具在你工作之后，也一定会用到的，可以顺带着了解，你不一定要去完全掌握，只要学会使用，知道基本原理就可以。学会使用之后，自己要跟着老师或者从网上去理解更多的东西，比如那些配置文件等。 知识点梳理：Spring 框架：配置文件、IoC 思想、DI 依赖注入、面向切面编程、事务等。 SpringMVC：框架原理、交互、拦截器等。 Maven：安装使用、基本操作。 Mybatis：框架原理、Mybatis 开发 DAO 方式、与其它框架的整合。 推荐书籍：Spring 实战，Effective Java，Java 编程思想 Java 第四阶段这个时候相信你已经能够完成独立开发，并且也工作了，对付工作上面的时候，你的技术一定是可以的。但是这个时候不要对自己进行松懈，你要继续学习，而不是工作只是为了应对工作，你应该提升自己的价值。这个时候可以去看一些比较底层的书籍，比如《深入理解Java虚拟机》，这本书就是全面帮助你了解 Java 虚拟机，这个时候想必你一定知道 Java 是运行在 JVM 上的，你没有任何理由不去了解 JVM。另外，关于并发这方面，推荐《Java并发编程实战》，这本书啃完之后，对并发的理解应该有一定的体会了。 这个阶段要做的远不止这些，我们要去思考我们之前使用的那些框架是怎么回事，以及阅读 Java 经典的一些源码，看懂源码的前提，就是你已经有了一定的基础，当然有基础也不一定一下子就能看懂，看不懂就要去思考，在看源码的过程中，你可能有各种各样的疑问，有疑问就是对的，问自己最多的应该是这里问什么会这样写，而不是那样写吧。这个阶段需要自己对自己有很强的自律去学习，不要看了一半就放弃了。学会看源码之后，自己可以尝试着模仿别人写的比较好的开源项目，造属于自己的轮子，虽说不一定有用，但是对提升自己有一定的好处。 如果你想成为优秀的人，你就要「能别人不能」，也就是说你要找到属于自己的一个领域研究下去，以期在将来，你能够成为这个领域的专家，建立起你的差异性。 最后，请记住，从你入行那一刻起，你就要比别人努力，就要不停的学习。每个人在学习的过程中都有自己的一种方式，在学习的过程中，要学会自己去判断。其实生活中也是一样的，你身边的人形形色色，有的人你喜欢，有的人你讨厌，但是你喜欢的人身上也有缺点，你讨厌的人身上也有其优点，这个时候你要学会从他们身上学习他们的优点，让自己变的更优秀。 PS：如果觉得文章不错的话，还请大家点赞分享下，算是对我的最大支持，我的微信公众号是「funnyZhang」欢迎大家打扰。]]></content>
      <tags>
        <tag>总结</tag>
        <tag>Java</tag>
        <tag>心得</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTPS——SSL配置与实现]]></title>
    <url>%2F2018%2F05%2F22%2Fconfig-ssl%2F</url>
    <content type="text"><![CDATA[1、在对应的云服务器上可以申请到免费的 SSL 证书，这里以腾讯云为例说明。进入腾讯云后台，在产品列表中选择 SSL 证书，我们选择其中的域名型免费版（DV）。 2、填写申请的信息，填写信息之后，确认申请。申请之后，可以选择自动添加解析，也可以手动添加解析，之后后台是机器审核扫描 CA 证书，时间很快，通过之后，会颁发证书，然后我们就可以去使用证书了。 3、配置网站，能够使用 HTTPS 访问，这里以 Nginx 部署为例。3.1 获取证书我们在第二步中，可以将证书下载下来，里面有关于 Nginx 的证书。包括 1_www.domain.com_bundle.crt //证书 2_www.domain.com.key //私钥文件 1__www.domain.com_bundle.crt 文件包括两段证书代码 “-----BEGIN CERTIFICATE-----”和“-----END CERTIFICATE-----” 2_www.domain.com.key 文件包括一段私钥代码 “-----BEGIN RSA PRIVATE KEY-----”和“-----END RSA PRIVATE KEY-----”。 3.2 证书安装将域名 www.domain.com 的证书文件 1_www.domain.com_bundle.crt、私钥文件2_www.domain.com.key保存到同一个目录，例如/usr/local/nginx/conf目录下。更新 Nginx 根目录下 conf/nginx.conf 文件如下： server { listen 443; server_name www.domain.com; #填写绑定证书的域名 ssl on; ssl_certificate 1_www.domain.com_bundle.crt; ssl_certificate_key 2_www.domain.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #按照这个协议配置 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;#按照这个套件配置 ssl_prefer_server_ciphers on; location / { root html; #站点目录 index index.html index.htm; } } 配置完成后，先用bin/nginx –t来测试下配置是否有误，正确无误的话，重启 nginx。就可以使 https://www.domain.com 来访问了。配置文件参数如下 4 使用全站加密，http 自动跳转 https（可选）对于用户不知道网站可以进行 https 访问的情况下，让服务器自动把 http 的请求重定向到 https。在服务器这边的话配置的话，可以在页面里加 js 脚本，也可以在后端程序里写重定向，当然也可以在web服务器来实现跳转。Nginx 是支持 rewrite 的（只要在编译的时候没有去掉pcre）在 http 的 server 里增加 rewrite ^(.*) https://$host$1 permanent;这样就可以实现 80 进来的请求，重定向为 https 了。]]></content>
      <tags>
        <tag>SSL证书</tag>
        <tag>HTTPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下安装Django并验证是否安装成功]]></title>
    <url>%2F2018%2F05%2F03%2Fdjango-install-for-mac%2F</url>
    <content type="text"><![CDATA[登录Django官网或者通过这里下载需要的文件，Mac下面我们可以下载对应的tar.gz文件。注：在下载Django的时候要注意下载的版本和python版本之间的支持。 将下载的文件移动到我们需要安装的目录中，自己可以放在一个单独存放软件的文件夹下（这里不一定是系统默认的安装路径）可以自己新建。在shell命令行中进入下载好的django-2.0.tar.gz文件目录中，执行下面的命令，进行解压。 tar -zxvf django-2.0.tar.gz 解压之后，会在该目录出现django-2.0.5文件夹，然后我们进入该文件夹，执行安装。 python setup.py install 安装完成之后，我们设置下环境变量，因为不设置环境变量的话，在后面执行的话会出现一些小的问题。 编辑.bashrc或者是ect/profile文件，在文件中加入我们的Django安装目录，在查看安装目录的时候，我们可以使用pwd来查看当前的路径，即可显示当前目录，加入下面两行即可。 export DJANGO_HOME=/Users/zhanghanwen/Tools/django-2.0.5/django export PATH=$PATH:$DJANGO_HOME/bin 至此，我们的安装就完成了，接下来我们在python命令行中进行验证是否安装成功。 在命令行中输入python进入python环境下 &gt;&gt;&gt; import django &gt;&gt;&gt; django.get_version() &apos;2.0.5&apos; 返回正确的版本，说明我们安装成功。]]></content>
      <tags>
        <tag>Django</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去除Tensorflow的警告信息]]></title>
    <url>%2F2018%2F01%2F15%2Fremove-waring%2F</url>
    <content type="text"><![CDATA[使用tensorflow运行程序会提示以下警告信息，但是并不影响运行。警告信息如下 2018-01-15 10:56:22.537770: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations. 2018-01-15 10:56:22.537796: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2018-01-15 10:56:22.537802: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations. 2018-01-15 10:56:22.537809: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&apos;t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations. 这个警告是告诉我们可以用自己机器的CPU来进行计算，会得到更好的性能。原因是我直接使用pycharm里面的plugs直接安装的，所以没有去安装对应的模块。去除这个警告我们可以有两种方法，一种是忽略这个警告信息，另一种就是根据提示信息，使用CPU进行计算。这里给出第一种的解决方法。第二种的解决方法大家可以去搜索一下。解决办法： # 在代码的开始加入下面两行代码 # 也就是对应的日志级别，1 代表是 info， 2 代表是 warning # 改为 warning 就可以忽略上面的警告信息 import os os.environ[&apos;TF_CPP_MIN_LOG_LEVEL&apos;]=&apos;2&apos;]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop-spark-hbase集群搭建]]></title>
    <url>%2F2017%2F12%2F27%2Fhadoop-spark-zookeeper-hbase-cluster%2F</url>
    <content type="text"><![CDATA[根据项目需求我们搭建了一套Hadoop + Spark + Hbase + Hive的架构方案。目前服务器总共有六台，每台服务器有五块硬盘，大小分别是300G、300G、4T、4T、4T，各个服务器主机名分别为Cloud、Cloud2、Cloud3、Cloud4、Cloud5、Cloud6。具体使用情况分别如下表： Cloud Cloud2 Cloud3 Cloud4 Cloud5 Cloud6 300G 系统 系统 系统 系统 系统 系统 300G 软件 软件 软件 软件 软件 软件 4T 存储数据 存储数据 存储数据 存储数据 存储数据 存储数据 4T 存储数据 存储数据 存储数据 存储数据 存储数据 存储数据 4T 备份 备份 备份 备份 备份 备份 每台服务器都是centos7的系统，配置完全相同。项目架构所需要安装的软件和各个服务器上所运行的服务具体情况如下表，由于学校不能申请过多的临时IP地址，所以这里先使用内网地址配置，后期会更改为正式IP。 Cloud Cloud2 Cloud3 Cloud4 Cloud5 Cloud6 IP 192.168.1.100 192.168.1.101 192.168.1.102 192.168.1.103 192.168.1.104 192.168.1.105 安装的软件 jdk hadoop hbase spark hive jdk hadoop spark hbase jdk hadoop spark hbase jdk hadoop zookeeper jdk hadoop zookeeper jdk hadoop zookeeper hadoop的服务 Namenode DFSZKFailoverController Namenode DFSZKFailoverController ResourceManager DataNode NodeManager JournalNode DataNode NodeManager JournalNode DataNode NodeManager JournalNode hbase的服务 HMaster HRegionServer HRegionServer spark的服务 Master Worker Worker zookeeper的服务 QuorumPeerMain QuorumPeerMain QuorumPeerMain Tomcat的服务 项目后台 该项目中配置了HA机制，以保证项目的连续性解决方案。首先先说明下什么是HA。 HA意为High Available，高可用性集群，是保证项目连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。通常把正在执行业务的称为活动节点，而作为活动节点的一个备份的则称为备用节点。当活动节点出现问题，导致正在运行的业务（任务）不能正常运行时，备用节点此时就会侦测到，并立即接续活动节点来执行业务。从而实现业务的不中断或短暂中断。 Hadoop的HA机制需要zookeeper来配合使用。架构图如下 说明： 在hadoop2.x中通常由两个NameNode组成，一个处于active状态，另一个处于standby状态。Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步active namenode的状态，以便能够在它失败时快速进行切换。 hadoop2.x官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这里我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode,这里还配置了一个zookeeper集群，用于ZKFC（DFSZKFailoverController）故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为standby状态。 整个架构搭建的具体过程前期准备工作前期准备工作需要在每台服务器都要去执行。 修改服务器主机名为了不重启系统而是修改主机名生效，并且保证系统重启之后，不会丢失所做的修改，我们使用以下的方式来修改主机名。 vi ／etc/sysconfig/network //将 HOSTNAME 修改为 Cloud .... NETWORKING=yes NETWORKING_IPV6=yes HOSTNAME=Cloud 然后在执行执行以下命令 // sysctl 是 centos7 以后才有的，请不要在 centos7 版本以下使用 sysctl kernel.hostname=Cloud 执行上述命令之后，在当前终端会话并不会生效，需要重新打开新的会话才会生效。系统重启也不会失效。将上述的两条命令分别在其它的服务器上都执行一遍。注意：需要将主机名更换 vi ／etc/sysconfig/network HOSTNAME=Cloud2 //分别也在另外四台服务器上设置 //分别为Cloud3、Cloud4、Cloud5、Cloud6 新会话有效 sysctl kernel.hostname=Cloud2 //分别也在另外四台服务器上设置 //分别为Cloud3、Cloud4、Cloud5、Cloud6 修改IP具体的网卡名称依服务器上为准，一般是以ifcfg-开头,ifcfg-lo是LOOPBACK网络不用做任何设置。ip地址和网关依查到的为准，若是虚拟机安装的话，自行查看自己本地的地址和网关。 vi /etc/sysconfig/network-scripts/ifcfg-* // 加入以下内容或者更新对应的内容 BOOTPROTO=static #dhcp改为static（修改） ONBOOT=yes #开机启用本配置，一般在最后一行（修改） IPADDR=192.168.1.100 #静态IP（增加） GATEWAY=192.168.1.1 #默认网关，虚拟机安装的话，通常是2，也就是VMnet8的网关设置（增加） NETMASK=255.255.255.0 #子网掩码（增加） DNS1=192.168.1.2 #DNS 配置，虚拟机安装的话，DNS就网关就行，多个DNS网址的话再增加（增加） 使用同样的方法，分别在Cloud2、Cloud3、Cloud4、Cloud5、Cloud6执行相同的操作。执行完毕后，可以需要重启网络才能生效。使用以下命令重启网络。 service network restart 重启之后，通过下面的命令查看是否配置成功，并进行验证各个服务器之间是否相同，如果要访问外网也同样需要进行验证，在没有设置DNS之前，是不能访问外网的，所以这个时候只需要验证各个服务器之间是否相同即可。 ip addr # 查看ip地址 # 在 Cloud 上验证其它是否能与其它机器互通 ping 192.168.1.102 #在分别与102、103、104、105、106进行验证 修改DNS修改各个服务器的DNS使其能够访问外网，具体修改如下 vi /etc/resolv.conf nameserver 8.8.8.8 nameserver x.x.x.x # 分配的dns直接加上就可以了 修改hostshosts文件是主机名和IP的映射关系，如果设置了hosts，可以使用主机名，而不需要在使用对应的冗长的IP地址了。具体修改如下,需要在每台服务器上都做如下修改。 vi /etc/hosts 192.168.1.100 Cloud 192.168.1.102 Cloud2 192.168.1.103 Cloud3 192.168.1.104 Cloud4 192.168.1.105 Cloud5 192.168.1.106 Cloud6 关闭防火墙在关闭之前，需要先查看一下当前的状态，使用一下命令查看，该命令同样是在centos7下，centos7以下的版本根据对应的版本查看是如何关闭的。 systemctl status firewalld 使用下面命令禁用 systemctl disable firewalld systemctl stop firewalld 关闭SELinux # 临时关闭，不用重启机器 setenforce 0 # 永久关闭，以防止下次重启生效 vi /etc/sysconfig/selinux # 下面为 selinux 文件中需要修改的元素 SELINUX=disabled 设置NTP时间同步设置时间同步之前，需要安装ntp，执行下面的命令安装，若没有yum命令，可以使用apt-get命令，在每台机器上都执行以下命令。 yum -y install ntp or apt-get install ntp ntpdate 0.asia.pool.ntp.org # 手动同步 配置ssh免登录先检查一下系统是否已经安装了ssh，若没有安装执行对应的命令进行安装，使用下面的命令可以检查是否已经安装。 ssh # 回车，若提示找不到该命令，使用下一行的命令进行安装，每台服务器都执行相同的步骤 yum install openssh-server # 或着使用apt-get安装也可以 为了集群安全，我们需要先创建一个用户，在该用户下配置免密钥登录，而不是在root用户下配置免密钥登录，这点请注意，不然的话在hadoop用户去启动集群的时候，同样会提示让你输入密码。下面先创建对应的组和用户，注意：在每台服务上都需要创建。具体如下 useradd -g hadoop hadoop # 创建hadoop用户，并且加入hadoop用户组，会自动创建hadoop组 passwd hadoop # 为hadoop用户创建密码 组和用户添加之后，可以切换到hadoop用户下面，在hadoop用户下执行下面的命令。 su hadoop # 然后输入密码进入 cd ~ # 下面的两行命令需要在每台服务器都执行 ssh-keygen -t rsa # 回车会有提示，都按回车就可以 cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys # 配置服务器本身公钥和密钥 # 同样下面的命令也需要在每台服务器上执行 # 实现每台服务器之间的无密钥登录，可以通过手动，也可以通过脚本实现 # 因为配置过hosts了，所以下面可以直接使用主机名 # 为了方便我们每台服务器之间都做无密钥登录 # 配置的时候，提示输入密码 ssh-copy-id -i ~/.ssh/id_dsa.pub Cloud2(主机名字) # 同样Cloud3及其它机器都要执行 挂载分区和创建应用安装路径先在hadoop用户根目录下创建3个文件夹，分别如下，所有的六台服务器都执行同样的操作。 mkdir software # 300G mkdir -p software/app # 存放安装软件包 mkdir -p software/src # 解压之后的软件 mkdir data # 集群数据存放 4T mkdir data2 # 预留 4T mkdir backup # 备份数据 4T 文件夹创建之后，将分区挂载到对应的文件夹下，在挂载之前先将分区格式化一下，同样的每台服务器都需要执行下面的命令 mkfs -t ext4 /dev/sdb #具体是sdx，取决于系统，可以通过lsblk查看 mkfs -t ext4 /dev/sdc mkfs -t ext4 /dev/sde mkfs -t ext4 /dev/sdf 将对应的分区挂载到刚才创建的四个文夹上，每台服务器都要执行。执行下面的命令 mount /dev/sdb software # sdb分区挂载在software目录下 mount /dev/sdc data # sdc分区挂载在data目录下 mount /dev/sde data2 # sde分区挂载在data2目录下 mount /dev/sdf backup # sdf分区挂载在backup目录下 为了防止启动丢失信息，这里我们在永久写入一下。执行下面的命令 vi /etc/fstab # 在文件后面加入以下内容 /dev/sdb /home/hadoop/software ext4 defaults 0 0 /dev/sdc /home/hadoop/data ext4 defaults 0 0 /dev/sde /home/hadoop/data2 ext4 defaults 0 0 /dev/sdf /home/hadoop/backup ext4 defaults 0 0 说明：这个文件1第列就是具体的分区，第2列是分区挂载的目录，第3列是文件格式，第4列是挂载规则，第5列是备份；0为从不备份，或显示上次至今备份之天数；第7列是启动时fsck检查顺序，0为不检查， “/”永远为1; 环境变量约定所有的环境变量设置均在hadoop用户下面配置，即在根目录下的.bashrc中设置，不需要在etc/profile中设置，所以hadoop用户不需要任何的root权限。 修改环境变量之后，不要忘记重新加载配置文件，不然环境变量不会生效。执行下面的命令生效 source ~/.bashrc 以上就是执行完成以后，我们的准备工作就算完成了，下面可以开始我们的集群的搭建。HADOO集群搭建 安装jdkjdk使用jdk-8u151-linux-x64.tar.gz，下载好之后，将这个文件移动到/home/hadoop/software/app下。在windwos上下载的软件，可以通过工具上传到服务器上，上传之后做下面的配置安装。安装配置如下 cd /software/app tar -zxvf jdk-8u151-linux-x64.tar.gz mv jdk-8u151-linux-x64 ../src cd ~ vi .bashrc # 在文件最后加入jdk的环境变量 export JAVA_HOME=/home/hadoop/software/src/jdk-8u151-linux-x64 export PATH=$PATH:$JAVA_HOME/bin source ~/.bashrc java -version # 验证是否设置成功 将安装好的jdk同步到其它的五台服务器上。命令如下 cd /software/src/jdk-8u151-linux-x64 rm -rf /doc # 为了快速的同步，将不要的帮助文档删掉 cd .. scp -r /jdk-8u151-linux-x64 hadoop@Cloud2:/home/hadoop/software/src scp -r /jdk-8u151-linux-x64 hadoop@Cloud3:/home/hadoop/software/src scp -r /jdk-8u151-linux-x64 hadoop@Cloud4:/home/hadoop/software/src scp -r /jdk-8u151-linux-x64 hadoop@Cloud5:/home/hadoop/software/src scp -r /jdk-8u151-linux-x64 hadoop@Cloud6:/home/hadoop/software/src scp ~/.bashrc hadoop@Cloud2:/home/hadoop/ scp ~/.bashrc hadoop@Cloud3:/home/hadoop/ scp ~/.bashrc hadoop@Cloud4:/home/hadoop/ scp ~/.bashrc hadoop@Cloud5:/home/hadoop/ scp ~/.bashrc hadoop@Cloud6:/home/hadoop/ 到此jdk配置完成。 安装配置zookeeper集群我们使用zookeeper版本如下zookeeper-3.4.8.tar.gz，下载地址如下点击进入下载页面，下载之后都样将其移动到/software/app这个目录下。具体安装配置如下 # 执行下面的操作之前，确保已经将文件上传至服务器上 # 若不是在windows下载的，也可以通过wget命令直接下载到服务器上，但是要确保能访问外网 cd /software/app tar -zxvf zookeeper-3.4.8.tar.gz mv zookeeper-3.4.8 ../src cd ../src cd zookeeper-3.4.8 mkdir data cd conf/ mv zoo_sample.cfg zoo.cfg vi zoo.cfg # 修改并添加如下内容 dataDir=/home/hadoop/software/src/zookeeper-3.4.8/data server.4=Cloud4:2888:3888 # 以下内容是需要添加的 server.5=Cloud5:2888:3888 server.6=Cloud6:2888:3888 ESC键 :wq # 保存并退出 cd ../data echo &quot;4&quot; &gt; myid 将配置好的zookeeper同步Cloud5和Cloud6服务器上。 cd ~ scp -r /software/src/zookeeper-3.4.8 hadoop@Cloud5:/home/hadoop/software/src scp -r /software/src/zookeeper-3.4.8 hadoop@Cloud6:/home/hadoop/software/src 将Cloud5和Cloud6上的zookeeper/data目录下myid文件内容进行更改，这是因为zookeeper集群不能重复。 # 在Cloud5上 cd /software/src/zookeeper-3.4.8/data echo &quot;5&quot; &gt; myid # &gt; 符号表示写入mydi文件并覆盖原来的内容 # 在Cloud6上 cd /software/src/zookeeper-3.4.8/data echo &quot;6&quot; &gt; myid 配置完成之后，分别在三台服务器上启动zookeeper # 在Cloud4上 cd ~ cd /software/src/zookeeper-3.4.8a/ ./zkServer.sh start # 在Cloud5上 cd ~ cd /software/src/zookeeper-3.4.8/ ./zkServer.sh start # 在Cloud6上 cd ~ cd /software/src/zookeeper-3.4.8/ ./zkServer.sh start #分别在Cloud4，Cloud5，Cloud6上执行，下面命令查看是否启动成功。 ./zkServer.sh status #若其中两个是follower，一个是leader，则说明成功。 将zookeeper添加到环境变量中 cd ~ vi .bashrc # 在文件最后加入jdk的环境变量 export ZK_HOME=/home/hadoop/software/src/zookeeper-3.4.8 export PATH=$PATH:$ZK_HOME/bin # 同步其它两台配置文件 scp ~/.bashrc hadoop@Cloud5:/home/hadoop/ scp ~/.bashrc hadoop@Cloud6:/home/hadoop/ 安装配置hadoop集群使用hadoop的这个版本hadoop-2.8.2.tar.gz，下载地址如下点击进入下载页面，下载之后都样将其移动到/software/app这个目录下。具体安装配置如下 #解压 cd /software/app tar -zxvf hadoop-2.8.2.tar.gz mv hadoop-2.8.2 ../src cd ../src cd hadoop-2.8.2/etc/hadoop 3.1 修改hadoop-env.sh export JAVA_HOME=/home/hadoop/software/src/jdk-8u151-linux-x64 3.2 修改core-site.xml &lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ns1 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/software/src/hadoop-2.8.2/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;Cloud4:2181, Cloud5:2181, Cloud6:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3.3 修改hdfs-site.xml &lt;configuration&gt; &lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt; &lt;value&gt;Cloud:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt; &lt;value&gt;Cloud:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt; &lt;value&gt;Cloud2:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt; &lt;value&gt; Cloud2:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://Cloud4:8485; Cloud5:8485; Cloud6:8485/ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/software/src/hadoop-2.8.2/journal&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode失败自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置sshfence隔离机制超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3.4 修改mapred-site.xml &lt;configuration&gt; &lt;!-- 指定mr框架为yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3.5 修改yarn-site.xml &lt;configuration&gt; &lt;!-- 指定resourcemanager地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;Cloud3&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定nodemanager启动时加载server的方式为shuffle server --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 3.6 创建对应的文件夹 cd hadoop-2.8.2/ mkdir tmp mkdir journal 3.7 修改slaves文件slaves是指定子节点的位置，因为要在Cloud上启动HDFS、在Cloud3启动yarn，所以Cloud上的slaves文件指定的是datanode的位置，Cloud3上的slaves文件指定的是nodemanager的位置 vi slaves Cloud4 Cloud5 Cloud6 3.8 将配置好的hadoop拷贝到其它节点上 cd ~ cd /software/src/ scp -r hadoop-2.8.2/ hadoop@Cloud2:/home/hadoop/software/src scp -r hadoop-2.8.2/ hadoop@Cloud3:/home/hadoop/software/src scp -r hadoop-2.8.2/ hadoop@Cloud4:/home/hadoop/software/src scp -r hadoop-2.8.2/ hadoop@Cloud5:/home/hadoop/software/src scp -r hadoop-2.8.2/ hadoop@Cloud6:/home/hadoop/software/src *3.8* 配置环境变量 cd ~ vi .bashrc #添加如下内容 export HADOOP_HOME=/home/hadoop/software/src/hadoop-2.8.2 export PATH=$PATH:$HADOOP_HOME/bin #在其它五台节点上也执行同样的操作，因为前面配置zookeeper环境变量了 #所以这里不能在直接拷贝过去，需要手动在每台节点上添加环境变量 验证hadoop和zookeeper 4.1 启动zookeeper若之前启动的zookeeper进程还活着，并且正常运行，可以直接进行下面的操作，若服务不正常，可以将其关掉并重新启动。 4.2 启动journalnode在Cloud上启动所有journalnode注意：是调用的hadoop-daemons.sh这个脚本，注意是复数s的那个脚本。 cd /software/src/hadoop-2.8.2 sbin/hadoop-daemons.sh start journalnode #在Cloud4，Cloud5，Cloud6上运行jps命令查看是否启动 #若存在JournalNode进程，即成功 4.3 格式化HDFS在Cloud上执行下面命令 hdfs namenode -format 格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件，这里我配置的是/home/hadoop/software/src/hadoop-2.8.2/tmp，然后将/home/hadoop/software/src/hadoop-2.8.2/tmp拷贝到Cloud2的/home/hadoop/software/src/hadoop-2.8.2/下。 scp -r hadoop-2.8.2/tmp/ hadoop@Cloud2:/home/hadoop/software/src/hadoop-2.8.2/ 4.4 格式化zookeeper hdfs zkfc -formatZK 4.5 启动HDFS（在Cloud上） sbin/start-dfs.sh 4.6 启动YARN注意：：是在Cloud3上执行start-yarn.sh，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动。 sbin/start-yarn.sh 到此hadoop集群配置完毕，可以对其测试。 验证hdfs HA机制验证上传文件成功之后，手动杀掉一个namenode，在查看该文件看是否存在。 验证YARN运行一下hadoop提供的demo中的WordCount程序 hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount /xxx /xxx Hbase集群搭建 Hbase使用hbase-1.2.6-bin.tar.gz，下载地址传送门。下载好之后，将这个文件移动到/home/hadoop/software/app下。在windwos上下载的软件，可以通过工具上传到服务器上，上传之后做下面的配置安装。安装配置如下 1.2 修改hbase-env.sh文件 cd ~ cd /software/app/ tar -zxvf hbase-1.2.6-bin.tar.gz mv hbase-1.2.6 ../src cd ../src/hbase-1.2.6 #修改配置文件 vi hbase-env.sh export JAVA_HOME=/home/hadoop/software/src/jdk-8u151-linux-x64 # 如果jdk路径不同需要单独配置 export HBASE_CLASSPATH=/home/hadoop/software/src/hadoop-2.8.2/etc/hadoop/ export HBASE_MANAGES_ZK=false # 不使用hbase自带的zk 1.3 修改hbase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://Cloud:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;Cloud&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;zookeeper.session.timeout&lt;/name&gt; &lt;value&gt;120000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;Cloud4,Cloud5,Cloud6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/software/src/hbase-1.2.6/hbasedata&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 1.4 创建对应的文件夹 cd hbase-1.2.6/ mkdir hbasedata 1.5 修改regionservers文件 Cloud2 Cloud3 1.6 修改环境变量 cd ~ vi .bashrc export HBASE_HOME=/hoem/hadoop/software/src/hbase-1.2.6/ export PATH=$PATH:$HBASE_HOME/bin #将上面的环境变量同样在Cloud2、Cloud3上执行。 1.7 将hbase拷贝到Cloud2，Cloud3节点上 cd /software/src scp -r hbase-1.2.6 hadoop@Cloud2:/home/hadoop/software/src scp -r hbase-1.2.6 hadoop@Cloud3:/home/hadoop/software/src 1.8 启动hbase 在启动hbase之前，请确保zookeeper和hadoop都已经启动，并且正常。 #在Cloud上执行 cd software/src/hbase-1.2.6 bin/start-hbase.sh #启动完成后，分别在Cloud、Cloud2、Cloud3上执行jps命令 #Cloud上会多出一个HMaster进程 #Cloud2上会多出一个HRegionServer进程 #Cloud3上会多出一个HRegionServer进程 Spark集群搭建 Spark使用spark-2.2.0-bin-hadoop2.7.tgz，下载地址传送门。下载好之后，将这个文件移动到/home/hadoop/software/app下。在windwos上下载的软件，可以通过工具上传到服务器上，上传之后做下面的配置安装。 因为Spark需要使用scala，所以我们先安装scala。下载地址传送门，下载之后，移动到software/app/目录下。 cd software/app tar -zxvf scala-2.12.4.tgz tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz mv scala-2.12.4 ../src mv spark-2.2.0-bin-hadoop2.7 ../src 2.1 配置环境变量 cd ~ vi .bashrc export SCALA_HOME=/home/hadoop/software/src/scala-2.12.4 export PATH=$PATH:$SCALA_HOME/bin export SPARK_HOME=/home/hadoop/software/src/spark-2.2.0-bin-hadoop2.7 export PATH=$PATH:$SPARK_HOME/bin # 将上面的环境变量在Cloud2和Cloud3上同样添加 配置Spark 3.1 修改spark-env.sh cd software/src/spark-2.2.0-bin-hadoop2.7/conf cp spark-env.sh.template spark-env.sh # 在末尾加入 vi spark-env.sh export JAVA_HOME=/home/hadoop/software/src/jdk-8u151-linux-x64 export SCALA_HOME=/home/hadoop/bigdata/src/scala-2.12.4 export SPARK_WORKER_MEMORY=2g export SPARK_MASTER_IP=192.168.1.100 #Master对应的ip export HADOOP_CONF_DIR=/home/hadoop/software/src/hadoop-2.8.2 export SPARK_JAR=/home/hadoop/software/src/spark-2.2.0-bin-hadoop2.7/jars 3.2 修改slaves文件 cd software/src/spark-2.2.0-bin-hadoop2.7/conf cp slaves.template slaves # 添加work节点 vi slaves Cloud2 Cloud3 3.3 spark集群复制 cd ~ cd software/src/ scp -r scala-2.12.4/ hadoop@Cloud2:/home/hadoop/software/src/ scp -r scala-2.12.4/ hadoop@Cloud3:/home/hadoop/software/src/ scp -r spark-2.2.0-bin-hadoop2.7/ hadoop@Cloud2:/home/hadoop/software/src/ scp -r spark-2.2.0-bin-hadoop2.7/ hadoop@Cloud2:/home/hadoop/software/src/ 3.4 spark集群启动 cd software/src/spark-2.2.0-bin-hadoop2.7 sbin/start-all.sh #在Cloud、Cloud2、Cloud3上使用jps命令查看是否配置成功 #Cloud上会多出一个Master进程 #Cloud2和Cloud3上会多出一个Worker进程 hive集群搭建 Hive使用apache-hive-2.2.0-bin.tar.gz，下载地址传送门。下载好之后，将这个文件移动到/home/hadoop/software/app下。具体配置如下 cd ~ cd software/app tar -zxvf apache-hive-2.2.0-bin.tar.gz mv apache-hive-2.2.0-bin ../src #配置环境变量 cd ~ vi .bashrc export HIVE_HOME=/home/hadoop/software/src/apache-hive-2.2.0-bin export PATH=$PATH:$HIVE_HOME/bin #最后 source .bashrc 对hive进行配置 cd ~ cd software/src/apache-hive-2.2.0-bin/conf cp hive-env.sh.template hive-env.sh #加入以下内容 vi hive-env.sh export JAVA_HOME=/home/hadoop/software/src/jdk-8u151-linux-x64 export HADOOP_HOME=/home/hadoop/software/src/hadoop-2.8.2 export HIVE_HOME=/home/hadoop/bigdata/src/apache-hive-2.2.0-bin export HIVE_CONF_DIR=/home/hadoop/bigdata/src/apache-hive-2.2.0-bin/conf 修改hbase-site.xml文件 cd software/src/apache-hive-2.2.0-bin/conf touch hive-site.xml vi hive-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hive:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; jdbc驱动下载 wget http://cdn.mysql.com/Downloads/Connector-J/mysql-connector-java-5.1.36.tar.gz cp mysql-connector-java-5.1.33-bin.jar apache-hive-2.2.0-bin/lib/]]></content>
      <tags>
        <tag>Hadoop</tag>
        <tag>Hbase</tag>
        <tag>Spark</tag>
        <tag>Hive</tag>
        <tag>Mysql</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去除Myeclipse中的拼写检查]]></title>
    <url>%2F2017%2F12%2F27%2Fget-rid-of-myeclipse-spell-wrong%2F</url>
    <content type="text"><![CDATA[myeclipse -&gt; proferences -&gt; general -&gt; editor -&gt; text editors -&gt; spelling 然后把enable spell checking 这行对号去掉就行了。 改变字体在，colors and fonts -&gt; basic -&gt; text font 选择edit，然后修改自己喜欢的字体和大小即可。 修改快捷键myeclipse -&gt; proferences -&gt; key 然后搜索content assist ，按住alt + / ctrl + shift + F 是格式化 如何改变jsp里面的编码格式，在项目工程上面右击，proferences-&gt;myeclipse-&gt;Resource，在里面更改编码格式，也可以在winows-&gt;proferences里面进行更改。]]></content>
      <tags>
        <tag>Myeclipse</tag>
        <tag>实用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[omnet++安装和使用]]></title>
    <url>%2F2017%2F12%2F25%2Fomnet-install-and-use%2F</url>
    <content type="text"><![CDATA[登录OMNeT++官网下载对应的windos版本, 官网地址点击这里点击对应的连接进行下在 下载之后，用压缩文件进行解压到本地文件下，进入omnet++根目录，找到下面这个文件 双击上面的文件，进入到shell环境下面 在命令行中输入./configure 第五步完成之后继续输入make,这个过程有点长，需要耐心一段时间。 第六步完成之后会出现下面这样的结果 接下来我们根据提示输入对应的命令，启动我们的编译环境 启动之后，会提示你选择一个工作目录，这里用默认的目录就可以 之后就会进入环境的主页面，如下]]></content>
      <tags>
        <tag>OMNeT++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REST-Webservice和SOAP-Webservice的比较]]></title>
    <url>%2F2017%2F12%2F23%2Frest-webservice-and-soap-webservice%2F</url>
    <content type="text"><![CDATA[在SOA的基础技术实现方式中WebService占据了很重要的地位，通常我们提到WebService第一想法就是SOAP消息在各种传输协议上交互。近几年REST的思想伴随着SOA逐渐被大家接受，同时各大网站不断开放API提供给开发者，也激起了REST风格WebService的热潮。 什么是SOAP，我想不用多说，google一把满眼都是。其实SOAP最早是针对RPC的一种解决方案，简单对象访问协议，很轻量，同时作为应用协议可以基于多种传输协议来传递消息（Http,SMTP等）。但是随着SOAP作为WebService的广泛应用，不断地增加附加的内容，使得现在开发人员觉得SOAP很重，使用门槛很高。在SOAP后续的发展过程中，WS-*系列协议的制定，增加了SOAP的成熟度，也给SOAP增加了负担。 REST REST其实并不是什么协议也不是什么标准，而是将Http协议的设计初衷作了诠释，在Http协议被广泛利用的今天，越来越多的是将其作为传输协议，而非原先设计者所考虑的应用协议。SOAP类型的WebService就是最好的例子，SOAP消息完全就是将Http协议作为消息承载，以至于对于Http协议中的各种参数（例如编码，错误码等）都置之不顾。其实，最轻量级的应用协议就是Http协议。Http协议所抽象的get,post,put,delete就好比数据库中最基本的增删改查，而互联网上的各种资源就好比数据库中的记录（可能这么比喻不是很好），对于各种资源的操作最后总是能抽象成为这四种基本操作，在定义了定位资源的规则以后，对于资源的操作通过标准的Http协议就可以实现，开发者也会受益于这种轻量级的协议。 REST的思想归结以下有如下几个关键点 面向资源的接口设计所有的接口设计都是针对资源来设计的，也就很类似于我们的面向对象和面向过程的设计区别，只不过现在将网络上的操作实体都作为资源来看待，同时URI的设计也是体现了对于资源的定位设计。后面会提到有一些网站的API设计说是REST设计，其实是RPC-REST的混合体，并非是REST的思想。 抽象操作为基础的CRUD这点很简单，Http中的get,put,post,delete分别对应了read,update,create,delete四种操作，如果仅仅是作为对于资源的操作，抽象成为这四种已经足够了，但是对于现在的一些复杂的业务服务接口设计，可能这样的抽象未必能够满足。其实这也在后面的几个网站的API设计中暴露了这样的问题，如果要完全按照REST的思想来设计，那么适用的环境将会有限制，而非放之四海皆准的。 Http是应用协议而非传输协议这点在后面各大网站的API分析中有很明显的体现，其实有些网站已经走到了SOAP的老路上，说是REST的理念设计，其实是作了一套私有的SOAP协议，因此称之为REST风格的自定义SOAP协议。 无状态，自包含这点其实不仅仅是对于REST来说的，作为接口设计都需要能够做到这点，也是作为可扩展和高效性的最基本的保证，就算是使用SOAP的WebService也是一样。 REST vs SOAP成熟度 SOAP虽然发展到现在已经脱离了初衷，但是对于异构环境服务发布和调用，以及厂商的支持都已经达到了较为成熟的情况。不同平台，开发语言之间通过SOAP来交互的web service都能够较好的互通（在部分复杂和特殊的参数和返回对象解析上，协议没有作很细致的规定，导致还是需要作部分修正） REST国外很多大网站都发布了自己的开发API，很多都提供了SOAP和REST两种Web Service，根据调查部分网站的REST风格的使用情况要高于SOAP。但是由于REST只是一种基于Http协议实现资源操作的思想，因此各个网站的REST实现都自有一套，在后面会讲诉各个大网站的REST API的风格。也正是因为这种各自实现的情况，在性能和可用性上会大大高于SOAP发布的web service，但统一通用方面远远不及SOAP。由于这些大网站的SP往往专注于此网站的API开发，因此通用性要求不高。 由于没有类似于SOAP的权威性协议作为规范，REST实现的各种协议仅仅只能算是私有协议，当然需要遵循REST的思想，但是这样细节方面有太多没有约束的地方。REST日后的发展所走向规范也会直接影响到这部分的设计是否能够有很好的生命力。 总的来说SOAP在成熟度上优于REST。 效率和易用性： SOAP协议对于消息体和消息头都有定义，同时消息头的可扩展性为各种互联网的标准提供了扩展的基础，WS-*系列就是较为成功的规范。但是也由于SOAP由于各种需求不断扩充其本身协议的内容，导致在SOAP处理方面的性能有所下降。同时在易用性方面以及学习成本上也有所增加。 REST被人们的重视，其实很大一方面也是因为其高效以及简洁易用的特性。这种高效一方面源于其面向资源接口设计以及操作抽象简化了开发者的不良设计，同时也最大限度的利用了Http最初的应用协议设计理念。同时，在我看来REST还有一个很吸引开发者的就是能够很好的融合当前Web2.0的很多前端技术来提高开发效率。例如很多大型网站开放的REST风格的API都会有多种返回形式，除了传统的xml作为数据承载，还有（JSON,RSS,ATOM）等形式，这对很多网站前端开发人员来说就能够很好的mashup各种资源信息。 因此在效率和易用性上来说，REST更胜一筹。 安全性： 这点其实可以放入到成熟度中，不过在当前的互联网应用和平台开发设计过程中，安全已经被提到了很高的高度，特别是作为外部接口给第三方调用，安全性可能会高过业务逻辑本身。 SOAP在安全方面是通过使用XML-Security和XML-Signature两个规范组成了WS-Security来实现安全控制的，当前已经得到了各个厂商的支持，.net，php ，java 都已经对其有了很好的支持（虽然在一些细节上还是有不兼容的问题，但是互通基本上是可以的）。 REST没有任何规范对于安全方面作说明，同时现在开放REST风格API的网站主要分成两种，一种是自定义了安全信息封装在消息中（其实这和SOAP没有什么区别），另外一种就是靠硬件SSL来保障,但是这只能够保证点到点的安全，如果是需要多点传输的话SSL就无能为力了。 应用设计与改造： 我们的系统要么就是已经有了那些需要被发布出去的服务，要么就是刚刚设计好的服务，但是开发人员的传统设计思想让REST的形式被接受还需要一点时间。同时在资源型数据服务接口设计上来说按照REST的思想来设计相对来说要容易一些，而对于一些复杂的服务接口来说，可能强要去按照REST的风格来设计会有些牵强。这一点其实可以看看各大网站的接口就可以知道，很多网站还要传入function的名称作为参数，这就明显已经违背了REST本身的设计思路。而SOAP本身就是面向RPC来设计的，开发人员十分容易接受，所以不存在什么适应的过程。 总的来说，其实还是一个老观念，适合的才是最好的 技术没有好坏，只有是不是合适，一种好的技术和思想被误用了，那么就会得到反效果。REST和SOAP各自都有自己的优点，同时如果在一些场景下如果去改造REST，其实就会走向SOAP（例如安全）。 REST对于资源型服务接口来说很合适，同时特别适合对于效率要求很高，但是对于安全要求不高的场景。而SOAP的成熟性可以给需要提供给多开发语言的，对于安全性要求较高的接口设计带来便利。所以我觉得纯粹说什么设计模式将会占据主导地位没有什么意义，关键还是看应用场景。 同时很重要一点就是不要扭曲了REST现在很多网站都跟风去开发REST风格的接口，其实都是在学其形，不知其心，最后弄得不伦不类，性能上不去，安全又保证不了，徒有一个看似象摸象样的皮囊。]]></content>
      <tags>
        <tag>Restful</tag>
        <tag>SOAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下安装Tomcat]]></title>
    <url>%2F2017%2F12%2F22%2Finstall-tomcat%2F</url>
    <content type="text"><![CDATA[Tomcat官网 点击这里 cd /usr/local/src/ wget http://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.24/bin/apache-tomcat-8.5.24.tar.gz tar -zxvf apache-tomcat-8.5.24.tar.gz mv apache-tomcat-8.5.24.tar.gz /usr/local/tomcat cp -p /usr/local/tomcat/bin/catalina.sh /etc/init.d/tomcat vim /etc/init.d/tomcat //第二行加入 # chkconfig: 112 63 37 # description: tomcat server init script # Source Function Library . /etc/init.d/functions JAVA_HOME=/usr/local/jdk1.8.0_23/ CATALINA_HOME=/usr/local/tomcat chmod 755 /etc/init.d/tomcat chkconfig --add tomcat chkconfig tomcat on service tomcat start ps aux |grep tomcat 浏览器输入http://[自己的ip]:8080 可以看到tomcat的欢迎页 更改默认启动端口， vim conf/server.xml Connector port=&quot;8080&quot; 改为Connector port=&quot;80&quot; 配置新虚拟主机：找到&lt;/Host&gt;下一行插入新的&lt;Host&gt;内容如下： &lt;Host name=&quot;www.123.cn&quot; appBase=&quot;/data/tomcatweb&quot; unpackWARs=&quot;false&quot; autoDeploy=&quot;true&quot; xmlValidation=&quot;false&quot; xmlNamespaceAware=&quot;false&quot;&gt; &lt;Context path=&quot;&quot; docBase=&quot;./&quot; debug=&quot;0&quot; reloadable=&quot;true&quot; crossContext=&quot;true&quot;/&gt; &lt;/Host&gt; 重启 service tomcat stop; service tomcat start]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pig使用笔记]]></title>
    <url>%2F2017%2F12%2F19%2Fpig-use%2F</url>
    <content type="text"><![CDATA[1. 安装Pig 将pig添加到环境变量当中 2. pig使用 首先将数据库中的数据导入到HDFS上 sqoop import --connect jdbc:mysql://192.168.1.10:3306/yun --username root --password 123 --table trade_detail --target-dir &apos;/sqoop/td&apos; sqoop import --connect jdbc:mysql://192.168.1.10:3306/yun --username root --password 123 --table user_info --target-dir &apos;/sqoop/ui&apos; td = load ‘/sqoop/td’ using PigStorage(‘,’) as (id:long, account:chararray, income:double, expenses:double, time:chararray);ui = load ‘/sqoop/ui’ using PigStorage(‘,’) as (id:long, account:chararray, name:chararray, age:int); td1 = foreach td generate account, income, expenses, income-expenses as surplus; td2 = group td1 by account; td3 = foreach td2 generate group as account, SUM(td1.income) as income, SUM(td1.expenses) as expenses, SUM(td1.surplus) as surplus; tu = join td3 by account, ui by account; result = foreach tu generate td3::account as account, ui::name, td3::income, td3::expenses, td3::surplus; store result into &apos;/result&apos; using PigStorage(&apos;,&apos;);]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Pig</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop-export]]></title>
    <url>%2F2017%2F12%2F16%2Fsqoop-export%2F</url>
    <content type="text"><![CDATA[数据导出工具exportexport工具，是将HDFS平台的数据，导出到外部的结构化存储系统中，可能会为一些应用系统提供数据支持。我们看一下export工具的基本选项及其含义，如下表所示 选项 含义说明 –validate &lt;class-name&gt; 启用数据副本验证功能，仅支持单表拷贝，可以指定验证使用的实现类 –validation-threshold &lt;class-name&gt; 指定验证门限所使用的类 –direct 使用直接导出模式（优化速度） –export-dir &lt;dir&gt; 导出过程中HDFS源路径 -m,–num-mappers &lt;n&gt; 使用n个map任务并行导出 –table &lt;table-name&gt; 导出的目的表名称 –call &lt;stored-proc-name&gt; 导出数据调用的指定存储过程名 –update-key &lt;col-name&gt; 更新参考的列名称，多个列名使用逗号分隔 –update-mode &lt;mode&gt; 指定更新策略，包括：updateonly（默认）、allowinsert –input-null-string &lt;null-string&gt; 使用指定字符串，替换字符串类型值为null的列 –input-null-non-string &lt;null-string&gt; 使用指定字符串，替换非字符串类型值为null的列 –staging-table &lt;staging-table-name&gt; 在数据导出到数据库之前，数据临时存放的表名称 –clear-staging-table 清除工作区中临时存放的数据 –batch 使用批量模式导出 更详细的请点击这里]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sqoop-1.4.4工具import和export使用详解]]></title>
    <url>%2F2017%2F12%2F12%2Fsqoop-import-and-export%2F</url>
    <content type="text"><![CDATA[Sqoop-1.4.4工具import和export使用详解Sqoop可以在HDFS/Hive和关系型数据库之间进行数据的导入导出，其中主要使用了import和export这两个工具。这两个工具非常强大，提供了很多选项帮助我们完成数据的迁移和同步。比如，下面两个潜在的需求： 业务数据存放在关系数据库中，如果数据量达到一定规模后需要对其进行分析或同统计，单纯使用关系数据库可能会成为瓶颈，这时可以将数据从业务数据库数据导入（import）到Hadoop平台进行离线分析。 对大规模的数据在Hadoop平台上进行分析以后，可能需要将结果同步到关系数据库中作为业务的辅助数据，这时候需要将Hadoop平台分析后的数据导出（export）到关系数据库。 这里，我们介绍Sqoop完成上述基本应用场景所使用的import和export工具，通过一些简单的例子来说明这两个工具是如何做到的。工具通用选项,import和export工具有些通用的选项，如下表所示： 选项 含义说明 –connect &lt;jdbc-uri&gt; 指定JDBC连接字符串 –connection-manager &lt;class-name&gt; 指定要使用的连接管理器类 –driver &lt;class-name&gt; 指定要使用的JDBC驱动类 –hadoop-mapred-home &lt;dir&gt; 指定$HADOOP_MAPRED_HOME路径 –help 打印用法帮助信息 –password-file 设置用于存放认证的密码信息文件的路径 -P 从控制台读取输入的密码 –password &lt;password&gt; 设置认证密码 –username &lt;username&gt; 设置认证用户名 –verbose 打印详细的运行信息 –connection-param-file &lt;filename&gt; 可选，指定存储数据库连接参数的属性文件 数据导入工具importimport工具，是将HDFS平台外部的结构化存储系统中的数据导入到Hadoop平台，便于后续分析。我们先看一下import工具的基本选项及其含义，如下表所示： 选项 含义说明 –append 将数据追加到HDFS上一个已存在的数据集上 –as-avrodatafile 将数据导入到Avro数据文件 –as-sequencefile 将数据导入到SequenceFile –as-textfile 将数据导入到普通文本文件（默认） –boundary-query &lt;statement&gt; 边界查询，用于创建分片（InputSplit） –columns &lt;col,col,col…&gt; 从表中导出指定的一组列的数据 –delete-target-dir 如果指定目录存在，则先删除掉 –direct 使用直接导入模式（优化导入速度） –direct-split-size &lt;n&gt; 分割输入stream的字节大小（在直接导入模式下） –fetch-size &lt;n&gt; 从数据库中批量读取记录数 –inline-lob-limit &lt;n&gt; 设置内联的LOB对象的大小 -m,–num-mappers &lt;n&gt; 使用n个map任务并行导入数据 -e,–query &lt;statement&gt; 导入的查询语句 –split-by &lt;column-name&gt; 指定按照哪个列去分割数据 –table &lt;table-name&gt; 导入的源表表名 –target-dir &lt;dir&gt; 导入HDFS的目标路径 –warehouse-dir &lt;dir&gt; HDFS存放表的根路径 –where &lt;where clause&gt; 指定导出时所使用的查询条件 -z,–compress 启用压缩 –compression-codec &lt;c&gt; 指定Hadoop的codec方式（默认gzip） –null-string &lt;null-string&gt; 如果指定列为字符串类型，使用指定字符串替换值为null的该类列的值 –null-non-string &lt;null-string&gt; 如果指定列为非字符串类型，使用指定字符串替换值为null的该类列的值 下面，我们通过实例来说明，在实际中如何使用这些选项。将MySQL数据库中整个表数据导入到Hive表 1bin/sqoop import --connect jdbc:mysql://master1:3306/workflow --table project --username shirdrn -P --hive-import -- --default-character-set=utf-8 将MySQL数据库workflow中project表的数据导入到Hive表中将MySQL数据库中多表JION后的数据导入到HDFS 1bin/sqoop import --connect jdbc:mysql://10.95.3.49:3306/workflow --username shirdrn -P --query &apos;SELECT users.*, tags.tag FROM users JOIN tags ON (users.id = tags.user_id) WHERE $CONDITIONS&apos; --split-byusers.id --target-dir /hive/tag_db/user_tags -- --default-character-set=utf-8 这里，使用了–query选项，不能同时与–table选项使用。而且，变量$CONDITIONS必须在WHERE语句之后，供Sqoop进程运行命令过程中使用。上面的–target-dir指向的其实就是Hive表存储的数据目录。将MySQL数据库中某个表的数据增量同步到Hive表 1bin/sqoop job --create your-sync-job -- import --connect jdbc:mysql://10.95.3.49:3306/workflow --table project --username shirdrn -P --hive-import --incremental append --check-column id --last-value 1 -- --default-character-set=utf-8 这里，每次运行增量导入到Hive表之前，都要修改–last-value的值，否则Hive表中会出现重复记录。将MySQL数据库中某个表的几个字段的数据导入到Hive表 1bin/sqoop import --connect jdbc:mysql://10.95.3.49:3306/workflow --username shirdrn --P --table tags --columns &apos;id,tag&apos; --create-hive-table -target-dir /hive/tag_db/tags -m 1 --hive-table tags --hive-import -- --default-character-set=utf-8 我们这里将MySQL数据库workflow中tags表的id和tag字段的值导入到Hive表tag_db.tags。其中–create-hive-table选项会自动创建Hive表，–hive-import选项会将选择的指定列的数据导入到Hive表。如果在Hive中通过SHOW TABLES无法看到导入的表，可以在conf/hive-site.xml中显式修改如下配置选项： 1234&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;jdbc:derby:;databaseName=hive_metastore_db;create=true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt; 然后再重新运行，就能看到了。使用验证配置选项 1sqoop import --connect jdbc:mysql://db.foo.com/corp --table EMPLOYEES --validate --validator org.apache.sqoop.validation.RowCountValidator --validation-threshold org.apache.sqoop.validation.AbsoluteValidationThreshold --validation-failurehandler org.apache.sqoop.validation.AbortOnFailureHandler 上面这个是官方用户手册上给出的用法，我们在实际中还没用过这个，有感兴趣的可以验证尝试一下。]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下的日期时间]]></title>
    <url>%2F2017%2F12%2F11%2Flinux-data%2F</url>
    <content type="text"><![CDATA[用date更新时间与本机一致,使用ntpdate这个命令 先安装 yum install -y ntp 然后执行就可以了 ntpdate time.windows.com 打印六位的年月日 date +%y%m%d 150904 打印八位的年月日 date +%Y%m%d 20150904 还可以指定格式 date +%Y-%m-%d 指定明天或者是后天，加上-d date -d &quot;+1day&quot; +%Y%m%d 或者一小时 date -d &quot;-1hour&quot; +%H:%M:%S 或者一分钟 date -d &quot;-1min&quot; +%H:%M:%S 一周之前 date -d &quot;-1week&quot; +%Y%m%d 时间 date +%H:%M:%S = date +%T date +%s 时间戳 date +%w, date +%W星期 一月后 date -d &quot;1month&quot;]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqoop安装使用]]></title>
    <url>%2F2017%2F12%2F10%2Fsqoop-install%2F</url>
    <content type="text"><![CDATA[sqoop安装，安装在一台节点上就可以了。 上传sqoop 安装和配置 在添加sqoop到环境变量 将数据库连接驱动拷贝到$SQOOP_HOME/lib里 使用第一类：数据库中的数据导入到HDFS上 sqoop import --connect jdbc:mysql://192.168.1.10:3306/hadoop --username root --password 123 --table trade_detail --columns &apos;id, account, income, expenses&apos; 指定输出路径、指定数据分隔符 sqoop import --connect jdbc:mysql://192.168.1.10:3306/hadoop --username root --password 123 --table trade_detail --target-dir &apos;/sqoop/td&apos; --fields-terminated-by &apos;\t&apos; 指定Map数量 -m sqoop import --connect jdbc:mysql://192.168.1.10:3306/hadoop --username root --password 123 --table trade_detail --target-dir &apos;/sqoop/td1&apos; --fields-terminated-by &apos;\t&apos; -m 2 增加where条件, 注意：条件必须用引号引起来 sqoop import --connect jdbc:mysql://192.168.1.10:3306/hadoop --username root --password 123 --table trade_detail --where &apos;id&gt;3&apos; --target-dir &apos;/sqoop/td2&apos; 增加query语句(使用 \ 将语句换行) sqoop import --connect jdbc:mysql://192.168.1.10:3306/hadoop --username root --password 123 --query &apos;SELECT * FROM trade_detail where id &gt; 2 AND $CONDITIONS&apos; --split-by trade_detail.id --target-dir &apos;/sqoop/td3&apos; 注意：如果使用--query这个命令的时候，需要注意的是where后面的参数，AND $CONDITIONS这个参数必须加上 而且存在单引号与双引号的区别，如果--query后面使用的是双引号，那么需要在$CONDITIONS前加上\即\$CONDITIONS 如果设置map数量为1个时即-m 1，不用加上--split-by ${tablename.column}，否则需要加上 第二类：将HDFS上的数据导出到数据库中(不要忘记指定分隔符) sqoop export --connect jdbc:mysql://192.168.8.120:3306/hadoop --username root --password 123 --export-dir &apos;/td3&apos; --table td_bak -m 1 --fields-terminated-by &apos;,&apos; 配置mysql远程连接 GRANT ALL PRIVILEGES ON hadoop.* TO &apos;root&apos;@&apos;192.168.1.201&apos; IDENTIFIED BY &apos;123&apos; WITH GRANT OPTION; FLUSH PRIVILEGES; GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123&apos; WITH GRANT OPTION; FLUSH PRIVILEGES]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive UDF 的使用]]></title>
    <url>%2F2017%2F12%2F10%2Fhive-udf%2F</url>
    <content type="text"><![CDATA[首先要继承org.apache.hadoop.hive.ql.exec.UDF类实现 evaluate 自定义函数调用过程： 添加jar包（在hive命令行里面执行) hive&gt; add jar /root/NUDF.jar; 创建临时函数 hive&gt; create temporary function getNation as &#39;com.hw.hive.udf.NationUDF&#39;; 调用 hive&gt; select id, name, getNation(nation) from beauty; 将查询结果保存到HDFS中 hive&gt; create table result row format delimited fields terminated by &#39;\t&#39; as select * from beauty order by id desc; hive&gt; select id, getAreaName(id) as name from tel_rec; create table result row format delimited fields terminated by &#39;\t&#39; as select id, getNation(nation) from beauties;]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene笔记]]></title>
    <url>%2F2017%2F12%2F01%2Flucene-note%2F</url>
    <content type="text"><![CDATA[什么是全文检索&emsp;&emsp;全文检索是计算机程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置。当用户查询时根据建立的索引查找，类似于通过字典的检索字表查字的过程。&emsp;&emsp;全文检索（Full-Text Retrieval）是指以文本作为检索对象，找出含有指定词汇的文本。全面、准确和快速是衡量全文检索系统的关键指标。关于全文检索 1、只处理文本 2、不处理语义 3、搜索时英文不区分大小写 4、结果列表有相关度排序 在信息检索工具中，全文检索是最具通用性和实用性的。 全文检索应用场景&emsp;&emsp;我们使用Lucene，主要是做站内搜索，即对一个系统内的资源进行搜索。如BBS、Blog中的文章搜索，网上商店中的商品搜索等。使用Lucene的项目有Eclipse,智联招聘,天猫等。一般不做互联网中资源的搜索，因为不易获取与管理海量资源（专业搜索方向的公司除外）。 全文检索不同于数据库检索&emsp;&emsp;全文检索不同于数据库的SQL查询.(他们所解决的问题不一样，解决的方案也不一样，所以不应进行对比）。在数据库中的搜索就是使用SQL。 SELECT * FROM t WHERE content like ‘%ant%’ 这样会有如下问题： 1、匹配效果：如搜索ant会搜索出planting。这样就会搜出很多无关的信息。 2、相关度排序：查出的结果没有相关度排序，不知道我想要的结果在哪一页。我们在使用百度搜索时，一般不需要翻页，为什么？因为百度做了相关度排序：为每一条结果打一个分数，这条结果越符合搜索条件，得分就越高，叫做相关度得分，结果列表会按照这个分数由高到低排列，所以第1页的结果就是我们最想要的结果。 3、全文检索的速度大大快于SQL的like搜索的速度。这是因为查询方式不同造成的，以查字典举例：数据库的like就是一页一页的翻，一行一行的找，而全文检索是先查目录，得到结果所在的页码，再直接翻到这一页。 关于lucene索引库操作的更新 1、索引文件的检索与维护，更新是先删除后创建。 2、维护倒排索引有三个操作：添加、删除和更新文档。但是更新操作需要较高的代价。因为文档修改后（即使是很小的修改），就可能会造成文档中的很多的关键词的位置都发生了变化，这就需要频繁的读取和修改记录，这种代价是相当高的。因此，一般不进行真正的更新操作，而是使用先删除，再创建的方式代替更新操作。 索引库的优化&emsp;&emsp;优化这个问题是比较纠结的，索引优化也是很费资源和时间的，但是优化索引也是提高检索速度的重要方法，因此需要好好权衡这一点。还有就是在lucene3.6后面的版本中lucene可以自动进行索引的优化，当索引的数目达到一定的量之后会自动进行索引的优化。具体的优化方法有以下几种 1、声明Directory对象 2、声明IndexWriter对象 3、执行优化的方法，参数表示优化称几段索引 4、关闭IndexWriter 分词器分词器的作用&emsp;&emsp;在创建索引时会用到分词器，在使用字符串搜索时也会用到分词器，这两个地方要使用同一个分词器，否则可能会搜索不出结果。&emsp;&emsp;Analyzer（分词器）的作用是把一段文本中的词按规则取出所包含的所有词。对应的是Analyzer类，这是一个抽象类，切分词的具体规则是由子类实现的，所以对于不同的语言（规则），要用不同的分词器。&emsp;&emsp;分词器的工作流程如下 (英文) 分词器： 1、切分关键词 2、去除停用词 3、对于英文单词，把所有字母转为小写（搜索时不区分大小写） 说明：有的分词器还对英文进行形态还原，就是去除单词词尾的形态变化，将其还原为词的原形。这样做可以搜索出更多有意义的结果。如搜索sutdent时，也可以搜索出students，这是很有用的。 关于分词器中的停用词和切分词&emsp;&emsp;有些词在文本中出现的频率非常高，但是对文本所携带的信息基本不产生影响，例如英文的“a、an、the、of”，或中文的“的、了、着、是”，以及各种标点符号等，这样的词称为停用词（stop word）。文本经过分词之后，停用词通常被过滤掉，不会被进行索引。在检索的时候，用户的查询中如果含有停用词，检索系统也会将其过滤掉（因为用户输入的查询字符串也要进行分词处理）。排除停用词可以加快建立索引的速度，减小索引库文件的大小。 常用的中文分词器&emsp;&emsp;中文的分词比较复杂，因为不是一个字就是一个词，而且一个词在另外一个地方就可能不是一个词，如在“帽子和服装”中，“和服”就不是一个词。对于中文分词，通常有三种方式：单字分词、二分法分词、词库分词。 1、单字分词：就是按照中文一个字一个字地进行分词。如：“我们是中国人”， 效果：“我”、“们”、“是”、“中”、“国”、“人”。（StandardAnalyzer、ChineseAnalyzer就是这样）。 2、二分法分词：按两个字进行切分。如：“我们是中国人”，效果：“我们”、“们是”、“是中”、“中国”、“国人”。（CJKAnalyzer就是这样）。 3、词库分词：按某种算法构造词，然后去匹配已建好的词库集合，如果匹配到就切分出来成为词语。通常词库分词被认为是最理想的中文分词算法。如：“我们是中国人”，效果为：“我们”、“中国人”。（使用极易分词的MMAnalyzer。可以使用“极易分词”，或者是“庖丁分词”分词器、IKAnalyzer）。]]></content>
      <tags>
        <tag>Lucene</tag>
        <tag>搜索</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红楼梦语录]]></title>
    <url>%2F2017%2F11%2F30%2Fread-red-dream-note%2F</url>
    <content type="text"><![CDATA[世人都晓神仙好，惟有功名忘不了！古今将相在何方？荒冢一堆草没了。世人都晓神仙好，只有金银忘不了！终朝只恨聚无多，及到多时眼闭了。世人都晓神仙好，只有娇妻忘不了！君生日日说恩情，君死又随人去了。世人都晓神仙好，只有儿孙忘不了！痴心父母古来多，孝顺儿孙谁见了？]]></content>
      <tags>
        <tag>生活</tag>
        <tag>文学</tag>
        <tag>名著</tag>
        <tag>情操</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[忆已失去的兴趣爱好]]></title>
    <url>%2F2017%2F11%2F29%2Fremind-child-hoppy%2F</url>
    <content type="text"><![CDATA[童年 在自己记事之前，懵懵懂懂的孩子，无忧无虑的成长着，兴许那时才是一生之中最快乐的日子。因为对事情不记得，所以没有那么的“烦恼”。而这段时间却是那么的短暂，还没来得及体会就从你身边走过。记事之后，身边多了很多玩伴，下学之后我们都会聚在一起玩耍。那时我们玩捉迷藏、扇地堡、弹琉珠、扇卡片。 印象最深的是弹琉珠，会因为从家人那里得到自己的零花钱，拿着零花钱去买上十几个琉珠，放在口袋里，走在路上会由于碰撞发出清脆的砰砰声音，心情会不自觉得愉快。一路上和小伙伴去到经常玩的地方一起玩斗琉珠。那时自己总是那么的幸运，跟别人玩斗琉珠每次都会赢好多，回家的时候会因为口袋里装着一大把琉珠而有点担心，怕父亲会说又花钱买那么多没用的东西。他们总是不相信我会赢别人，所以每次都会小心翼翼的先把一部分藏在自己的小仓库里，不让他们发现。所庆幸的是那个小仓库一直到现在还存在着，并且里面还有一部分，每次看到的时候，都会回想起那段愉快的童年。现在回想起来虽然觉得很笨但很快乐。 少年 那年十二岁，刚刚升入初中的我。我清楚地记得当时自己是多么的开心，因为父亲给我买了一个风筝，因为这是父亲答应过我的，他说过等你过完十二岁生日的时候，送给你一个风筝。还有陪我一起把风筝放到蓝蓝的天空中去。我的生日是在冬天刚过去，那个季节正是放风筝的好季节，在收到风筝之前，我看着别的小伙伴都拿着自己的风筝在麦田里面欢快的放着自己的风筝，心里面很羡慕他们。 做在家门前，傻傻的望着天空飞翔的风筝，想着父亲什么时候会给我送来风筝。自己在那发呆了一会，母亲就叫吃饭了，一天一晃而过，翌日清晨，我起床之后，在院子里自无所事事的玩耍着，父亲一大早就出去，也不知道干什么去了。快到吃早饭的时候，父亲回来了，看着父亲手里拿着东西，起初还以为是别的什么呢。等我接过来一看，是一个很大很大的老鹰风筝，自己当时开心坏了。父亲说吃过早饭我们一起去放风筝，早饭吃的很匆忙，因为我的心思一直都在那个风筝上，根本没有在吃饭上，不一会父亲也忙好了。我们就一起去麦田里放风筝，我和父亲一起将风筝撑好，父亲拿着线，我抬着风筝往逆风方向跑去，差不多的时候，我将风筝举起放入空中，父亲在另外一头拽着线，风筝慢慢的升入高空。我跑过去，接过线，由自己操作着风筝，在麦田里挥舞着自己手中的线。回首前尘，我意识到在过去父亲是多么渴望自己以后能够像老鹰一样在天空翱翔。自己当时始终没有意识到。那个冬天，造就了今天一部分的我。 后记 如今的我已走入社会，回首过往的童年，是多么幸福的事情。目前的兴趣爱好甚是广泛，但却缺少了一种味道，多了一种味道。 喜欢书、喜欢运动、喜欢文字、抬头看路、低头做人，这就是我————阿文]]></content>
      <tags>
        <tag>生活</tag>
        <tag>杂谈</tag>
        <tag>兴趣</tag>
        <tag>记忆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux运维安全]]></title>
    <url>%2F2017%2F11%2F27%2Flinux-system-safe%2F</url>
    <content type="text"><![CDATA[账号以及密码一定要复杂，密码需要符合这些规范：字符大于10个；至少包含大小写以及数字；密码中不能包含账号，不能包含自己的姓名全拼，不能有自己的生日数字，不能有自己的电话号码；密码要定期更换；不能把密码保存在记事本等文档中要用专业的存密码的软件保存； 可以拿一台机器作为跳板机来登陆其他服务器，其他服务器做登陆ip限制 /etc/shos.allow, /etc/hosts.deny 能使用密钥尽量避免使用密码登陆 vim /etc/ssh/sshd_config //PermitRootLogin without-password 改为 PermitRootLogin no 可以禁止root直接登陆服务器，只允许普通用户登录，普通用户su到root（PermitRootLogin no） vim /etc/ssh/sshd_config chkconfig --list chkconfig nginx off 服务器上用不到的端口关闭，用不到的服务停掉（ntsysv） 应用环境程序软件（apache，nginx，php，mysql）避免使用太老版本 不可逆操作在操作前一定要备份相关的数据或配置文件 重要数据一定要备份，尽量本地和远程存两份 敲命令切勿太快，避免误操作。 web禁止目录浏览 （apache：Option -Indexes；nginx编译时加上 --without-http_autoindex_module） web可写目录下禁止解析php （apache：php_admin_flag engine off；nginx：location ～.*abc/.*\.php?${deny all;} web默认虚拟机主机禁止访问，apache第一个虚拟主机，nginx如果不单独分离虚拟主机配置文件也为第一个，否则就是有“listen 80 default”那个。 设定php禁用函数：php.ini中增加 disable_functions = popen,passthru,escapeshellarg,escapeshellcmd,escapeshellarg,shell_exec,proc_get_status,ini_alter,ini_restore,dl,pfsockopen,openlog,syslog,readlink,symlink,leak,popepassthru,stream_socket_server,popen,proc_open,proc_close,phpinfo 设定 openbase_dir(apache: php_admin_value open_basedir /data/123:/tmp/; php-fpm: php_admin_value[open_basedir]=/data/123:/tmp/) 网站根目录下禁止有test命令的文件如test.php php.ini中关闭display_error php禁止访问远程文件 （php.ini中allow_url_fopen = off; allow_url_include = off） 站点后台访问需要限定ip （apache http://www.lishiming.net/thread-5365-1-1.html nginx: http://www.lishiming.net/thread-6458-1-1.html ) 建议每个站点都配置访问日志，并且做日志切割压缩归档，磁盘空间允许的话，尽量存放比较久的时间 尽量避免开放ftp服务，如果要开放要满足两个原则：1.限定ip访问（iptables实现）；2.密码设置一定要复杂]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016年总结]]></title>
    <url>%2F2017%2F11%2F26%2F2016-summarize%2F</url>
    <content type="text"><![CDATA[此时写下这篇总结已是2016年的最后的时光，夜黑高空，天上有着不少的星星，想必大家都在热闹的过着跨年，而自己独自一人坐在桌前用冰冷的双手敲打着键盘，写下自己这一年的经历与感悟。 这也是我在2016年干的最后一件事情吧，或许在写完本篇的时候在看个电影，度过自己的跨年夜。回顾2016的点点滴滴，经历了很多事情和很多人，太多的事情不是用眼睛看到的那么简单，只有亲身经历过了，才会有更深的感触。从春季开学，在学校里每天面对着少之又少的课程然而你也要必须去，强行过完这段时间。虽说自己可以不去，可以找时间自己充实自己，但是自己的心始终不再学校，早已向逃离学校走上工作岗位去从事工作或着加入别人。虽大多数人都告诫说外面的世界不是那么的好混，还是学校好。我虽然体会到，但我还是一心想出去实践。毕竟自己也有所准备，有利用业余时间学习自己喜欢的语言和其它技术来补充自己，另一方面也来自家乡同龄人的压力，记得每次回家的时候，都可以听到，那谁谁工作一年能挣好多钱。自己听见嘴上虽说不在乎，其实心里面早已有影响，或多或少的会对自己说自己以后一定要比他们挣的多，现在想想感觉自己掉钱眼去了，看不到自己所学的东西有什么收获的时候，有时会感觉到上大学到底有什么用，还不如不上跟社会人一样，在高中毕业之后，选择去外面打工，那样的话想必现在的我也会跟他们拿着差不多的工资吧，也就没有了这大学四年的经历了。但是现在想想其实也没有什么，只不过是路子不同而已，在大学里能够结识到更多的老师和同学，拓展自己的眼界，可以参加自己感兴趣的社团，在怎么说大学也算是个小社会。大学里面也存在着各种各样的关系，有时想想大学可能比社会上更“黑”。有些事情不知道比知道好很多，最起码自己会保持之前对待生活和未来的乐观，有些事情知道了，或多或少的会影响到自己。但是现实是很残酷的，有些东西你不想知道，它也会跑到你耳边。 时间是不等人的，在不知不觉中就很快把大四的上学期结束了，在这期间自己也参加了一些比赛并获得了奖励，而在此时自己也面临着两个选择，一是去找工作，二是选择考研。自己在这两个选择之中纠缠了很久都没有确定，感觉这是自己做选择以来最费心的，期间也有回家过，问下家人的意见，但是我知道最终的决定还是在于自己。家人永远都尊重我自己的选择，在几经波折之后，自己最终选择去读研。这时已经是快6月份了，自己心里也有点着急，想人家考研的从二月份就已经准备了，自己先在都落后了好几个月，瞬间就有压力了，但是自己相信自己，时间还是够用的。在这自己开始着手自己的考研之路的时候，其它的事情又插进来，就是关于分散实习和集中实习。在这期间的选择自己没有太多的犹豫，想必自己也受到自己内心的驱使和其它的一些另外因素。集中实习是去无锡培训一个月相关技术。这个决定在最初自己还抱有希望，希望自己能够两者兼得，但是经历过后悔心的感觉都有，或许自始自终这就是自己错误的选择，自己要为自己的错误买单。在此也告诉自己以后不要想那么美好的事情，这个世界上没有什么会天上掉馅饼的，当自己有机会，有优势的时候，就要充分利用，不能为了别人而牺牲自己，显得自己很高大尚，其实在别人心里这什么都不算。这是社会的规则，没人会为你同情而认为你这么做是为他们好。自己是大错特错了，这次也给自己一个深刻的教训，以后不能太单纯的看问题，去满足别人需求，而不顾自己的需求。只有你自己够强大了，你才能更有发言权。去无锡这一趟，自己更是深痛万分，没想到这是一个特大的谎言，不知学校为何要安排这样的形式，签这样的合同，而去让很多学生白白浪费这一个月的时间。学生完全可以利用暑假的时间加上这一个月可以自己找工作和充实自己。经历这之后，这也是自己给自己上了一堂课，让自己体会到所谓的什么。这个社会不是你想的那样，你不能去改变社会，但你可以提高自己，让自己更强大。在此期间的更多的感受实属这辈子都不想在经历，自己都有点怀疑自己，自己坚强的内心都有那么一时刻被动摇。 从无锡回来的之后，自己就直接回到学校，继续考研之路。回来之后明显感觉自己像换个人一样，真的是环境能够影响到一个人或者是改变一个人。明显感觉到自己的状态变了，不再像以前那样，因为自己心中经历了这一个月，让自己更加懂得能够坐在教室里看书，专心去做自己的想要做的事情时，是多么的来之不易。虽然每天都重复的一样的生活，但每天都是在进步，那种感觉不再是没有去无锡之前的自己，在此之前就是所谓的看书，看别人每天积极的去看书，自己也加入这个过程，但是想必这明显不是自己主动去做事情，效率也不算太高，大家每天都重复着这种生活。在此之后，自己每天像是感觉不到累一样，使劲的学，都学不够，这或许是去无锡这趟的教训吧。让自己知道时间来之不易，更应该为自己的未来做打算。走好自己的每一步，相信只有努力过，才可能会有收获，不然实属扯淡。回来之后，完成自己的最后两个比赛，离自己考试的时间也剩的寥寥无几了，在此给自己的教训是：做事一定要全身心投入，竭尽全力的去完成。不要分散自己的时间，否则会付出比别人太多太多的努力，就是聪明也不要分散时间，因为有些东西是用时间积累起来的，不是短时间内就可以完成。 两场比赛之后，警钟早已在自己心里敲起，因为还有三十多天就要考试了。必须全力以赴，尽自己最大的努力去尽可能的完成任务，去弥补之前没有完成的任务。此时回想在这期间真是如流水般的过来了，这时间比骑车快多了，每天除了吃饭睡觉，所有的时间都投入学习，自己内心一直在告诉自己要不断的去努力。一股力量一直在支撑着自己，每天往死干。就是感觉不到累，也没有出现什么情况，这是自己之前锻炼的结果吧，虽在备考过程之中没有坚持锻炼，但身体还是很棒的，感谢当初的自己。现在虽然考研已经结束了，但这个过程给自己留下了很多的感触，这是很丰富的一年，虽这一年很曲折，但很充实，成长不少。 现在考研已过了一个星期了，自己也放松了快一个星期，明天新的一年要开始奋斗了，要开始着手准备一些东西了。希望自己在2017年的能够有更多的见识，更多的提升自己，增强自己的能力，走好自己的新长征，做最好的自己。 Follow my heart ! see you have I love you ! 喜欢书、喜欢运动、喜欢文字、抬头看路、低头做人，这就是我————阿文 2016年12月31日晚]]></content>
      <tags>
        <tag>生活</tag>
        <tag>总结</tag>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学会使用curl命令]]></title>
    <url>%2F2017%2F11%2F24%2Fstudy-curl-command%2F</url>
    <content type="text"><![CDATA[curl是linux系统命令行下用来简单测试web访问的工具，几个常用的选项你要掌握 curl -xip:port www.baidu.com # -x可以指定ip和端口，省略写hosts，方便实用 curl -Iv http://www.qq.com #-I可以把访问的内容略掉，只显示状态码，-v可以显示详细过程 curl -u user：password http://123.com #-u可以指定用户名和密码 curl http://study.lishiming.net/index.html -O #可以下载，还可以使用-o自定义名字 curl -o index2.html http://study.lishiming.net/index.html]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rsync应用实例]]></title>
    <url>%2F2017%2F11%2F24%2Frsync-application-practice%2F</url>
    <content type="text"><![CDATA[ssh方式 1、rsync -avL test1/ www@192.168.238.0.101:/tmp/test2/ 2、rsync -avL 192.168..0.101:/tmp/test2/ ./test3/ 3、由于需要输入密码所以不合适写到脚本中，但可以通过创建密钥对，让两台机器产生信任关系从而不用输入密码 ssh-keygen -t rsa 连续回车 ssh-copy-id id_rsa.pub hostip 4、如果ssh端口不是22，那么需要写成下面这样的形式： rsync -av &quot;--rsh=ssh -p [port] &quot; /dir1/192.168.0.101:/tmp/dir2/ 后台服务方式, 配置文件/etc/rsyncd.conf，内容如下： #port=873 #监听端口默认为873，也可以是别的端口 log file=/var/log/rsync.log #指定日志 pid file=/var/run/rsyncd.pid #指定pid #address=192.168.238.2 #可以定义绑定的ip 以上部分为全局配置部分，以下为模块内的设备 [test] #为模块名，自定义 path=/root/rsync #指定该模块对应在哪个目录下 use chroot=true #是否限定在该目录下，默认为true，当有软链接时，需要改为false max connections=4 #指定最大可以连接的客户端数 read only=no #是否为只读 list=true #是否可以列出模块名 uid=root #以哪个用户的身份来传输 gid=root #以哪个组的身份来传输 auth users=test #指定验证用户名，可以不设置 secrets file=/etc/rsyncd.passwd #指定密码文件，如果设定验证用户，这一项必须设置 hosts allow=192.168.238.22 #设置可以允许访问的主机，可以是网段 密码文件/etc/rsyncd.passwd的内容格式为 username：password 启动服务的命令是 rsync --daemon 默认去使用/etc/rsyncd.conf 这个配置文件，也可以指定配置文件 rsync --daemon--config=/etc/rsync2.conf 可使用的选项有 rsync --daemon -help 几个测试点：port，use，chroot，log file，secrets file，hosts，allow，list。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下数据备份工具rsync]]></title>
    <url>%2F2017%2F11%2F23%2Flinux-rsync-note%2F</url>
    <content type="text"><![CDATA[1、scp 1.txt /tmp #拷贝本地的数据，这和cp一样 2、scp [-r] /dir/ user@ip:/dir/ #拷贝到远程，也类似于cp 3、rsync的优点在于，可以增量备份，就是说目标机器上已经存在某个文件，且和源文件一样，那它不会覆盖该文件。 4、rsync 不仅可以像cp一样在本地同步数据，还可以同步到远程 5、没有rsync命令，使用yum -install -y rsync安装 6、rsync -av 123.txt /tmp/ 7、rsync -av 123.txt root@192.168.238.22：/tmp/ #其中root@可以省略掉，回车后会提示让输入目标机器的root密码 rsync命令格式 rsync [option]... SRC DEST rsync [option]... SRC [USER@]HOST:DEST rsync [option]... [USER@]HOST:SRC DEST rsync [option]... [USER@]HOST::SRC DEST rsync [option]... SRC [USER@]HOST::DEST rsync常用选项 -a：归档模式，表示以递归方式传输文件，并保持所有属性，等同于-riptgoD，-a选项后面可以跟一个--no-OPTION这个表示关闭-riptgoD中的某一个例如-a--no-l等同于-rptgoD -r 对子目录以递归模式处理，主要是针对目录来说的，如果单独传一个文件不需要加-r，但是传输的是目录必须加-r选项 -v 打印一些信息出来，比如速率，文件数量等 -l 保留软链接 -L 向对待常规文件一样处理软链接，如果是SRC中有软链接文件，则加上该选项后将会把软链接指向的目标文件拷贝到DST -p 保持文件权限 -o 保持文件爱你属住信息 -g 保持文件属组信息 -D 保持设备文件信息 -t 保持文件时间信息 --delete 删除那些DST中SRC没有的文件 --exclude=PATTERN 指定排除不需要传输的文件，等号后面跟文件名，可以是万用字符模式（如*.txt） --progress 在同步的过程中可以看到同步的过程状态，比如统计要同步的文件数量、同步的文件传输速度等等 -u显示同步过程的详细信息，--exclude后面也可以使用通配符 加上这个选项后将会把DST中比SRC还新的文件排除掉，不会覆盖 最常用的 -a，-v，--delete， --exclud，-L，-u，--progress 选项讲解 1、rsync -av dir/ dir2/ #其中dir2/目录可以不存在，记得同步目录时一定要在末尾加上/2、-a会把软链接原原本本的拷贝过去，那有时候我们想拷贝源文件怎么办？就是用到一个-L 3、rsync -avL test1/ test2/ 4、-u 选项的作用是，如果目标文件比源文件新，那么会忽略掉该文件 touch test2/1.txt； rsync -avu test1/ test2/ 5、rsync -av –delete test1/ test2/ #这样会把test2目录比test1/ 目录多出来的文件删除掉 6、rsync -a –exclude=”2.txt” test1/ test2/ #在同步的过程中，会忽略掉2.txt这个文件 7、rsync -a –progress –exclude=”.txt” test1/ test2/ #–progress 显示同步过程的详细信息，–exclude后面也可以使用通配符]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux系统日志]]></title>
    <url>%2F2017%2F11%2F23%2Flinux-log-study%2F</url>
    <content type="text"><![CDATA[Linux系统日志学习笔记 1、/var/log/messages核心系统日志文件2、每周归档一个日志messages-201509093、/etc/logrotate.conf4、messages由syslogd这个守护进程产生的，如果停掉这个服务则系统不会产生/var/log/messages5、/var/log/wtmp 查看用户登录历史last6、/var/log/btmp lastb 查看无效登录历史7、/var/log/maillog8、/var/log/secure9、dmesg10、/var/log/dmesg exec与xargs1、exec和find同时使用2、查找当前目录创建时间大于10天的文件并删除 find . -mtime +10 -exec rm -rf {} \; 3、批量更改文件名 find ./* -exec mv {} {}_bak \; 4、xargs用在管道符号后面 5、 find . -mtime +10 |xargs rm -rf 6、 ls -d ./* |xargs -n1 -i{} mv {} {}_bak 7、xargs可以把多行变成一行 cat 1.txt|xargs screen工具介绍 1、screen相当于一个虚拟终端，它不会因为网络中断而退出，每次登录都可以进入那个screen使用方法：直接输入screen2、screen -ls 查看已经开启的screen3、ctrl+a 再按d退出该screen会话，知识退出，并没有结束，结束的话输入ctrl+d或者输入exit4、退出后还想再次登录某个screen会话，使用screen -r screenid若只有一个screen直接screen -r5、screen -S hanwen ；登录的话screen -r hanwen]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux系统服务管理和系统任务计划]]></title>
    <url>%2F2017%2F11%2F21%2Flinux-system-service-task%2F</url>
    <content type="text"><![CDATA[Linux系统服务管理1、工具ntsysv类似图形界面管理工具，如果没有该命令使用下面命令安装 yum install -y ntsysv 2、常用的服务有 crond，iptables，network，sshd，rsyslog，sysstat，irqbalance，sendmail，microcode_ctl 3、列表 chkconfig --list 4、添加或者删除一个 chkconfig --add/del servicename 5、状态开关 chkconfig --level[345] servicename on/off 1、通过 crontab 这个命令来完成的常用的选项有 分 时 日 月 周 -u：指定某个用户，不加-u选项则为当前用户； -e：制定计划任务 -l：列出计划任务 -r：删除计划任务 用户的cron存到了这里/var/spool/cron/username]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux抓包工具]]></title>
    <url>%2F2017%2F11%2F21%2Flinux-dump-tool%2F</url>
    <content type="text"><![CDATA[linux 抓包工具 1、tcpdump系统自带抓包工具 //-nn表示让第三列和第四列显示成ip+端口号的实行。 //-i 后跟设备名 2、tcpdump -nn -i eth0 tcp and host 192.168.0.1 and port 80 vs0不限定包的长度 -c 只抓多少包 -w更文件，抓的包写到的文件 3、tcpdump -nn -vs0 tcp and port not 22 -c 100 -w 1.cap 4、wireshark 在linux下也可以安装 yum install -y wireshark 5、抓包分析http请求： tshark -n -t a -R http.request -T fields -e &quot;frame.time&quot; -e &quot;ip.src&quot; -e &quot;http.host&quot; -e &quot;http.request.method&quot; -e &quot;http.request.ri&quot; 抓取之后，会有一个文件，在 /tmp/etherXXXXevWMie 每次抓取之后最后最好删除， /tmp/etherXXXXevWMie |xargs rm]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用工具ps、free、netstat]]></title>
    <url>%2F2017%2F11%2F20%2Flinux-common-tools%2F</url>
    <content type="text"><![CDATA[PS 命令 1、ps aux /ps -elf2、PID：进程的id，这个id很有用，在linux中内核管理进程就得靠pid来识别和管理某一个进程，比如我想终止某一个进程，则用‘kill 进程的pid’有时并不能杀掉，则需要加一个-9选项了，kill -9 进程pid3、STAT：表示进程的状态，进程状态分为以下几种4、D不能中断的进程（通常为IO）5、R正在运行中的进程6、S已经中断的进程，系统大部分进程都是这个状态7、T已经停止或者暂停的进程，如果我们正在运行一个命令，比如说sleep 10 如果我们按一下ctrl -z 让他暂停，那么我们用ps查看就会显示T这个状态8、X已经死掉的进程（这个从来不会出现）9、Z表示僵尸进程，即杀不掉、打不死的垃圾进程，占系统一点资源，不过没有关系。如果占用太多（一般不会出现），就需要重视了。10、&lt; 高优先级进程11、N 低优先级进程12、L在内存中被锁了内存分页13、s主进程14、l多线程进程15、+在前台运行的进程 重新加载进程 kill -HUP pid free 查看系统内存使用情况 1、free以k为单位显示-m以M为单位 -g以G为单位2、mem（total）：内存总数；mem（used）：已经分配的内存mem（free)：未分配的内存；mem（buffers）：系统分配但未被使用的buffers；mem（cached）系统分配但未被使用的cache3、buffers/cache（used）：实际使用的buffers与cache总量，也是实际使用的内存；buffers/cache（free）：未被使用的buffers与cache和未被分配的内存值和，这就是系统当前实际可用内存4、buffers是即将要被写入磁盘的，cache是被从磁盘中读出来的 netstat 查看网络状况 1、netstat -lnp 查看当前系统开启的端口以及socket2、netstat -an 查看当前系统所有的连接3、关于tcp三次握手和四次挥手的文章]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux系统监控]]></title>
    <url>%2F2017%2F11%2F20%2Flinux-monitor%2F</url>
    <content type="text"><![CDATA[在使用监控之前可能会没有安装该命令，如果没有安装可以使用下面的命令进行安装 yum install -y sysstat 网卡流量sar -n DEV， sar -n DEV 1 10 查看历史负载 sar -q 查看磁盘读写 sar -b 1、命令w和uptime的第一行，upload 2、system load averages 单位时间短内活动的进程数 3、查看cpu的个数和核数 processor physical id 4、vmstat 5、vmstat 1 10 1分钟 5分钟 15分钟 查看cpu信息 cat /proc/cpuinfo proc显示进程的相关信息 r：表示允许和等待cpu时间片的进程数，如果长期大于服务器cpu的个数，则说明cpu不够用了。b：表示等待资源的进程数，比如等待I/O、内存等。该数值如果长时间大于11，则需要关注一下 swap： si：表示由交换区写入到内存的数据量so：表示由内存写入到交换区的数据量 io： bi：表示从块设备读取数据的量（读磁盘）bo：表示从块设备写入数据的量（写磁盘） wa：表示I/O等待所占用的CPU的时间百分比,查看io还有iotoo这个工具,没有可以安装 yum install -y iotop top top用于动态监控进程所占系统资源，每隔3秒变一次。RES这一项为进程所占内存大小，而%MEM为使用内存百分比。在top状态下，按“shift + m”，可以按照内存使用大小排序。按数字‘1’可以列出各颗cpu的使用状态top -bn1 它表示非动态打印系统资源使用情况，可以用在shell脚本中。]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下iptables的三张表]]></title>
    <url>%2F2017%2F11%2F19%2Flinux-three-iptables%2F</url>
    <content type="text"><![CDATA[1、iptables -t 指定表名，默认不加-t则是filter表2、filter 这个表主要用于过滤包的，是系统预设的表，内建三个链INPUT、OUTPUT以及FORWARD。INPUT作用于进入本机的包：OUTPUT作用于本机送出的包；FORWARD作用于那些跟本机无关的包。3、nat主要用处是网络地址转换，也有三个链。PREROUTING链的作用是在包刚刚到达防火墙时改变它的目的地址，如果需要的话。OUTPUT链改变本地产生的包的目的地址。POSTROUTING链在包就要离开防火墙之前改变其源地址。4、mangle这个表主要是用于给数据包打标记，然后根据标记去操作哪些包。这个表几乎不怎么用。iptables规则： 1、查看规则iptables -t nat -nvL2、清楚规则iptables -t nat -F3、增加/删除规则iptables -A/-D INPUT -s 10.72.11.12 -p tcp –sport 1234 -d 10.72.137.159 –dport 80 -j DROP4、插入规则iptables -I INPUT -s 1.1.1.1 -j DROP/ACCEPT/REJECT5、iptables -nvL –line-numbers 查看规则带有id号6、iptables -D INPUT 1 根据规则的id号删除对应规则7、iptables -P INPUT DROP 用来设定默认规则，默认是ACCEPT，一旦设定为DROP后，只能使用iptables -P ACCEPT 才能恢复成原始状态，而不能使用-F参数 iptables实例 1、针对filter表，预设策略INPUT链DROP，其他两个链ACCEPT，然后针对192.168.137.0/24开通22端口，对所有网段开发80端口，对所有网段开放21端口。脚本如下：123456#!/bin/bashipt=&quot;/sbin/iptables&quot;$ipt -F;$ipt -P INPUT DROP;$ipt -P OUPUT ACCEPT; $ipt -P FORWARD ACCEPT;$ipt -A INPUT -s 192.18.137.0/24 -p tcp --dport 22 -j ACCEPT$ipt -A INPUT -p tcp --dport 80 -j ACCEPT $ipt -A INPUT -p tcp --dport 21 -j ACCEPT 2、icmp的包有常见的应用，本机ping通外网，外网ping不通本机 iptables -I INPUT -p icmp --icmp-type 8 -j DROP iptables nat表应用： 1、路由器就是使用iptables的nat原理实现2、假设您的机器上有两块网卡eth0和eth1，其中eth0的IP为192.168.10.11,eth1的IP为172.16.10.11。eth0连接了intnet但eth1没有连接，现在有另一台机器（172.16.10.12）和eth1是互通的，那么如何设置也能够让连接eth1的这台机器能够连接intnet？3、echo “1” &gt;／proc/sys/net/ipv4/ip_forward4、iptables -t nat -A POSTROUTING -s 172.16.10.0/24 -o eth0 -j MASQUERADE iptables规则备份与恢复 1、service iptables save 这样会保存到/etc/sysconfig/iptables2、iptables-save &gt;ｍyipt.rule 可以把防火墙规则保存到指定文件中3、iptables-restore &lt; myipt.rule 这样可以恢复指定的规则]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux防火墙selinux]]></title>
    <url>%2F2017%2F11%2F19%2Flinux-selinux%2F</url>
    <content type="text"><![CDATA[1、是Redhat/CentOS系统特有的安全机制。不过因为这个东西限制太多，我们可以不启用。 2、查看selinux的状态的命令： （1）getenforce 设置为setenforce （2）sestatus 3、配置文件/etc/selinux/config 三种形式：enforcing，permissive，disabled SELINUX=disabled 4、没有这两个命令setenforce 0/1 getenforce 用一下命令安装 yum install -y libselinux-utils iptables 1、iptables -nvL 查看规则 2、iptables -F 清除当前的规则 3、iptables -Z 计数器清零 4、service iptables save 保存规则 保存的规则文件为：/etc/sysconfig/iptables 5、service iptables stop 可以暂停防火墙，但重启之后它会读取/etc/sysconfig/iptables从而启动防火墙，另外即使我们停止防火墙，但一旦我们添加任何一条规则，它也会开启]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本综合练习]]></title>
    <url>%2F2017%2F11%2F18%2Flinux-shell%2F</url>
    <content type="text"><![CDATA[什么是Shell 1、它是一种脚本语言，并非编程语言2、可以使用一些逻辑判断、循坏等语法3、可以自定义子函数4、是系统命令的集合5、shell脚本可以实现自动化运维，大大增加我们的工作效率 shell脚本的结构及执行 6、开头行指定bash路径：#！/bin/bash7、脚本的名字以.sh结尾，用于区分这是一个shell脚本8、执行方式有两种：chmod +x 1.sh; ./1.sh如果没有执行权限可以bash 1.sh9、bash -x 1.sh 可以查看脚本执行过程 脚本默认放在这个目录：/usr/local/sbin/ shell脚本中的变量 1、当脚本中使用某个字符串频繁并且字符串长度很长时就应该使用变量替代2、使用条件语句时，变量是必不可少的3、引用某个命令的结果时，用变量替代4、写和用户交互的脚步时，变量也是必不可少的5、内置变量$0,$1,$2……6、数学运算a=1；b=2；c=$(($a+$b))或者$[$a+$b] shell中的逻辑判断用（（））这个就要加；[]这个不需要加分号 格式1： if条件；then语句；fi 格式2： if条件；then语句；else语句；fi 格式3： if...；then...；elif...；then...；else...； fi 逻辑判断表达式： if[$a -gt $b ]; if [$a -lt 5 ]; if[ $b -eq 10 ]等 -gt(&gt;); -lt(&lt;); -ge(&gt;=); -le(&lt;=); eq(==); -ne(!=) //注意到处都是空格 可以使用&amp;&amp; || 结合多个条件 if 判断文件、目录属性 1、[ -f file ]判断是否是普通文件，且存在2、[ -d fiel ]判断是否是目录，且存在3、[ -e file ]判断文件或目录是否存在4、[ -r file ]判断文件是否刻度5、[ -w file ]判断文件是否可写6、[ -x file ]判断文件是否可执行 netstat -lnp打印当前系统开放端口的 if判读一些特殊用法 1、if [ -z $a ] 这个表示当变量a 的值为空时会怎么样，表示为空2、if grep -q ‘123’ 1.txt; then 表示如果1.txt中含有‘123’的行时会怎么样，q相当quite3、if [ ! -e file ]; then 表示文件不存在时会怎么样4、if (($a]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux下的变量以及系统和个人环境变量的配置文件]]></title>
    <url>%2F2017%2F11%2F18%2Flinux-variable-study%2F</url>
    <content type="text"><![CDATA[1、系统变量名都是大写，echo可以查看变量名2、env可以列出当前用户的所有环境变量以及用户自定义全局变量3、set命令可以把所有变量列出来包括系统的和自定义的全局变量以及当前shell自定义变量 linux下设置自定义变量规则：执行#bash进入当前用户的子shell，可以用pstree查看 (1) 格式为“a=b”，其中a为变量名，b为变量的内容，等号两边不能有空格(2) 变量名只能有英、数字以及下划线组成，而且不能以数字开头(3) 当变量内容带有特殊字符（如空格）时，需要加上单引号(4) 如果变量内容中需要用到其他命令运行结果则可以使用反引号(5) 变量内容可以累加到其他变量的内容，需要加双引号 5、系统所有用户使用变量 export myname=hanwen 全局变量，加入/etc/profile并 source /etc/profile //永久生效 6、系统某个用户使用变量 export myname=hanwen 加入当前用户家目录下的.bashrc中source .bashrc vim ~/.bashrc source .bashrc 7、export myname=hanwne 全局变量，export不加任何选项表示，声明所有的环境变量以及用户自定义变量 8、用户自定义变量，可以使用unset变量名进行解除变量设置 系统和个人环境变量的配置文件 1、/etc/profile PATH,USER,LOGNAME,MAIL,INPUTRC,HOSTNAME,HISTSIZE,umask等2、/etc/bashrc $PS1 umask 以后如果设置umask修改/etc/profile不要改这个文件3、.bash_profile用户自己的环境变量4、.bashrc 当用户登录时以及每次打开新的shell时，执行该文件5、.bash_history 记录命令历史用的6、.bash_logout :当退出shell时，会执行该文件]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些shell中常用的命令cut]]></title>
    <url>%2F2017%2F11%2F17%2Fshell-usually-command-cut%2F</url>
    <content type="text"><![CDATA[语法 cut -d ‘分割字符’【-cf】 n 这里的n是正整数 -d后面指定分隔符，用单引号引起来，-f指定第几段 cut -d ‘：‘ -f 1 /etc/passwd | head -n 5 -c后面只有一个数字表示截取第几个字符 head -n2 /etc/passwd | cut -c2 -f 后面跟一个数字区域，表示截取从几到几 head -n2 /etc/passwd | cut -c2-5 SORT语法 sort【-t分隔符】【-kn1，n2】【-nru】 （n1&lt;n2) 不加选项，从首字符向后，依次按ASCII码值进行升序排序 sort /etc/passwd -t后指定分隔符，-kn1，n2表示在指定的区间中排序，-k后面只跟一个数字表示对第n个字符排序，-n表示使用纯数字排序 sort -t： -k3 -n /etc/passwd -r 表示以降序的形式排序 sort -t: -k3,5 -r /etc/passwd -u 去重 cut -d: -f4 /etc/passwd |sort -n u WC用于统计文档的行数、字符数、词数 不加任何选项，会显示行数，词数以及字符数 -l统计行数 -m统计字符数 -w统计字数 -uniq，tee，tr uniq去重复，最常用就一个-c用来统计重复的行数，去重前要先排序 sort tsetb.txt |uniq -c tee后跟文件名，类似于&gt;，比重定向多了一个功能，在把文件写入后面所跟的文件中的同时，还显示在屏幕上 tr替换字符，最常用的就是大小写转换： head -n2 /etc/passwd |tr &apos;[a-z]&apos; &apos;[A-Z]&apos; tr替换一个字符也是可以的 grep &apos;root&apos; /etc/passwd | tr &apos;r&apos; &apos;R&apos; Split切割大文件用 -b：按大小来分割单位为byte split -b50 1.txt 默认会以xaa，xab，。。。这样的形式定义分割后的文件名，也可以指定文件名 split -b50 1.txt 123 -l：按行数分割，split -l10 file]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell的一些快捷键]]></title>
    <url>%2F2017%2F11%2F17%2Fshell-command%2F</url>
    <content type="text"><![CDATA[命令历史：history 1234!! 执行上一条命令 !$ 以空格为分割的最后一个!n 执行history中的第几个命令! 运行history中的匹配 比如！c，可能会执行cd 1234567891011121314151617181920212223242526272829303132history -c 清楚命令历史-w 内存命令写到文件中，关机的时候执行tab键可以补全文件路径或者命令alias a=&quot;b&quot; unalias a通配符*匹配零个或者多个字符？匹配一个字符输入输出重定向 \&gt;, \&gt;&gt;, ,2&gt;,错误重定向 ,2&gt;&gt;。管道符 |作业控制ctrl + z，暂停命令 jobs，后台有哪些命令 fg，调用到前台 bg。把暂停的变成开始 在后台执行会有&amp;这个符号]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下安装一个源码包]]></title>
    <url>%2F2017%2F11%2F16%2Finstall-source-package%2F</url>
    <content type="text"><![CDATA[源码包是开源的可自行更改的程序包，大多用用C语言开发，不能直接使用，需要编译成二进制的可执行文件，编译源码包的必须有gcc支持，如果没有需要安装 yun install -y gcc 通常情况编译三步曲 ./configure //配置各种编译参数 make //根据指定的编译参数进行编译 make install //安装到指定目录。 源码安装实例-httpd的源码安装下载源码包 cd /usr/local/src/ #约定目录 wget http://apache.etoak.com/httpd/httpd-2.2.24.tar.bz2 解压 tar -jxvf httpdd-2.2.24.tar.bz2 //查看README或者INSTALL说明文件 指定编译参数 ./configure --help 安装约定 ./configure --prefix=/usr/local/apache2 echo $? //验证是否成功,如果输出结果是0，则上述的步骤执行成功 make make install 或者以上两步可以一起执行 make &amp; make install]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rpm工具的使用]]></title>
    <url>%2F2017%2F11%2F16%2Frpm-study-note%2F</url>
    <content type="text"><![CDATA[先介绍一下rpm包名字的构成，是由-和.分成了若干部分分，如： abrt-cli-2.0.8.15.el6.centos.i686.rpm rpm包并没有写具体的平台而是noarch这代表这个rpm包没有硬件平台限制。 安装一个包 -i表示安装 -v可视化 -h显示安装进度 -force：强制安装，即使覆盖属于其它包的文件也要安装 -nodeps：当要安装的rpm包依赖其他包时，即使其他包没有安装，也要安装这个包 rpm -ivh /mnt/Package/libjpeg-turbo-devel-1.2.1-1.el6.i686.rpm 升级 -U：就是升级的意思(update) rpm -Uvh filename.rpm rpm的卸载 rpm -e filename 查询一个包是否安装 rpm -q 包名（不带有平台信息以及后缀名） 查询当前系统所有安装过的rpm包 rpm -qa 查看是否安装过相关的,grep是过滤的作用 rpm -qa |grep vim 查询rpm包的相关信息 rpm -qi 包名 列出一个rpm所安装的文件列表： rpm -ql 包名 某个文件属于哪个rpm包，文件的绝对路径 rpm -qf filename 这里可以结合反引号一起使用，比如 rpm -qf `which ls` 反引号是esc键下面的那个 查看该文件由哪个包所安装]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yum工具的使用]]></title>
    <url>%2F2017%2F11%2F16%2Fyum-tools-study%2F</url>
    <content type="text"><![CDATA[yum常用操作//wc -l 统计行数 yum list |wc -l yum list 列出所有可用rpm包资源 搜索某个包 yum search ‘keywords’ or yum list |grep ‘keywords’ yum安装包 yum install -y filename（包名） yum卸载包 yum remove -y filename（包名） yum升级包 yum update -y filename（包名） 创建本地的yum源 1. mount /dev/cdrom /mnt 2. cp -f /etc/yum.repos.d /etc/yum.repos.d.bak 3. rm -rf /etc/yum.repos.d/* 4. vim /etc/yum.repos.d/dvd.repo #加入如下内容 [dvd] name=install dvd baseurl=file:///mnt enabled=1 gpgcheck=0 5. yum makecache 利用yum下载一个rpm包 yum install -y yum-plugin-downloadonly.noarch 首先需要安装一个插件来支持只下载不安装 yum install 包名 -y --downloadonly #这样就已经下载了 yum install 包名 -y --downloadonly --downloaddir=/usr/local/src #指定一个下载目录]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本编辑工具Vim]]></title>
    <url>%2F2017%2F11%2F15%2Fvim-tool-method%2F</url>
    <content type="text"><![CDATA[若是使用下面的命令提示没有该命令，可以先进行安装，安装Vim命令如下，Vim官网vim.wendal.net yum install -y vim-enhanced 一般模式！$ 使用上面的命令 ：set nu 显示行号。 h向左 j向下 k向上 l向右 ctrl +f 下翻 ctrl +b 上翻 shift +4本行行尾 shift +6本行行首 或者是数字0 gg首行 G尾行 nG移动到第几行，（n为任意数字） dd 删除/剪切光标所在的那一行 p 粘贴，在下一行粘贴，P在上一行粘贴 u 返回 ctrl + r 反着还原 yy 复制光标所在行 ndd 剪切n行 nyy 复制n行 x表示向后删除/剪切一个字符，X表示向前删除/剪切一个字符 nx向后删除n个字符 加上+号这个选项，可以光标到最后一行 加上+5 表示在第几行 比如：vim +4 /etc/init.d/iptables 命令模式/xxxx查找某个xxx 按n是向下走 ？xxx查找某个xxx 按n是向上走 ：n1，n2s /word1/word2/g 在n1-n2行之间查找word1并替换word2，不加g则只替换每行的第一个word1 s代表替换 ：1,$s/word1/word2/g 将文档所有的word1替换为word2，不加g则只替换每行的第一个word1 ：加向上箭头，翻以前使用的命令 编辑模式O在上一行插入 o在下一行插入 写完之后按esc a 光标后插入 i 在光标前插入 I 在行首插入 A在行尾插入]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文档的压缩与打包]]></title>
    <url>%2F2017%2F11%2F15%2Fdocument-zip-unzip%2F</url>
    <content type="text"><![CDATA[可以打包目录也可以打包文件，语法 tar [-zjxcvfpP] filename //其中test是文件或目录 tar -cvf test.tar test -c表示建立包，-v可视化，压缩时跟“-f文件名”，意思是压缩后的文件名为filename，解压时跟“-f文件名”，意思是解filename。请注意，如果是多个参数组合的情况下带有“-f”，请把“-f”写到最后面 查看包内容 //-t：查看tar包里面的文件 tar -tf test.tar 解包 //-x：解包或者解压缩 tar -xvf test.tar 不管是打包还是解包，原来的文件是不会删除的，但它会覆盖当前已经存在的文件或者目录。 打包的同时使用gzip压缩 //-z表示打包同时使用gzip压缩 tar -czvf 1.tar.gz 1 //其中1可以是文件也可以是目录 解压.tar.gz的压缩包 tar -xzvf 1.tar.gz 使用bzip2压缩 -j表示打包同时使用bzip2压缩 tar -jzvf 1.tar.bz2 1 解压.tar.bz2 tar -xjvf 1.tar.bz2 同样使用tar -tf 查看压缩的包 tar -tf 1.tar.gz 或者 tar -tf 1.tar.bz2 –exclude可以在打包的时候，排除某些文件或者目录 tar -cvzf 1.tar.gz --exclude 1.txt dir/ 排除多个文件或者目录 tar -czvf 1.tar.gz --exclude 1.txt --exclude 123/ dir/ zip和unzipzip是压缩工具，unzip是解压缩工具压缩文件 zip filename.zip filename 压缩目录 zip -r dir.zip dir/ 解压缩zip压缩包 unzip filename.zip gzip压缩，语法gzip [-d#] filename 其中#为1-9的数字，默认压缩级别为6 只能压缩文件，不能压缩目录 gzip filename 生成filename.gz 源文件消失 解压 gzip -d filename.gz 解压后，压缩文件也会消失 bzip2压缩工具语法 bzip2 [-dz] filename //“-z”表示压缩，也可以不加 压缩时，可以加“-z”也可以不加，都可以压缩文件 bzip2 filename 生成filename.bz2。源文件消失 不支持压缩目录，只支持压缩文件 bzip2 -d filename.bz2 //解压后压缩文件消失 可以使用bzcat查看bz2的压缩前的文件内容]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式学习]]></title>
    <url>%2F2017%2F11%2F14%2Fregular-note%2F</url>
    <content type="text"><![CDATA[正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个“规则字符串”，这个“规则字符串”用来表达对字符串的一种过滤逻辑。 正则之grep语法：grep [-cinvABC] &apos;word&apos; filename -c:打印符合要求的行数 -i:忽略大小写 -n:在输出符合要求的行的同时连同行号一起输出 -v:打印不符合要求的行 -A:后跟一个数字（有无空格都可以）例如-A2则表示打印符合要求的行以及下面两行 -B：后跟一个数字，例如-B2则表示打印符号要求的行以及上面两行 -C： 后跟一个数字，例如-C2则表示打印符合要求的行以及上下个两行 -r：会把目录下所有的文件全部遍历 -w：w为word的意思 grep工具实例：//1、过滤出带有某个关键词的行并输出行号 grep -n &apos;root&apos; 1.txt //2、过滤出不带有某个关键词的行并输出行号 grep -n -v &apos;root&apos; 1.txt //3、过滤出所有包含数字的行 grep &apos;[0-9]&apos; 1.txt //4、去除所有以‘#’开头的行 grep -v &apos;^#&apos; 1.txt //5、去除所有空行和以&apos;#&apos;开头的行 grep -v &apos;^$&apos; 1.txt | grep -v&apos;^#&apos; //或者是： grep -v &apos;^$&apos; | grep -v&apos;^#&apos; 1.txt //6、过滤出以英文字母开头的行 grep &apos;^[a-zA-Z]&apos; 1.txt //7、过滤出以非数字开头的行 grep &apos;^[^0-9]&apos; 1.txt //8、过滤出任意一个或多个字符 grep &apos;r.o&apos; 1.txt; r,t之间的任意字符，r，t必须有 grep &apos;r*t&apos; 1.txt; 零个或多个r，但是必须有t grep &apos;r.*t&apos; 1.txt; 只要有r和t就行了 //9、.表示任意一个字符；*表示零个或多个前面的字符； //.*表示零个或多个任意字符，空行也包括在内 //10、指定过滤字符次数 grep &apos;o\{2\}&apos; 1.txt 正则之egrep1、egrep工具是grep工具的一个扩展 2、egrep ‘o+&apos; 1.txt 表示1个或1个以上前面字符 3、egrep &apos;o?&apos; 1.txt 表示0个或者1个前面字符 4、egrep &apos;roo|body&apos; 1.txt 匹配roo或者匹配body相当于grep -E &apos;root|body&apos; 1.txt 5、egrep &apos;r(oo)|(at)o&apos; 1.txt用括号表示一个整体，表示roo，或者ato 6、egrep &apos;(oo)+&apos; 1.txt 表示1个或者多个’oo&apos; awk工具1、截取文档中的某段 awk -F &apos;:&apos; &apos;{print $1}&apos; 1.txt &apos;{print $0}[打印整个文件 2、也可以使用自定义字符连接每个段 awk -F &apos;:&apos; &apos;{print $1 &quot;#&quot;$2&quot;#&quot;$3&quot;#&quot;$4}&apos; 1.txt 3、匹配字符或字符串 awk &apos;/oo/&apos; 1.txt 4、针对某个段匹配 awk -F &apos;:&apos; &apos;$1~/oo/&apos; 1.txt 5、多次匹配 awk -F &apos;:&apos; &apos;/root/ {print $1,$3}; $1~/test/; $3~/20/&apos; 1.txt 6、条件操作符==，&gt;,&lt;,!=,&gt;=,&lt;= 7、awk -F &apos;:&apos; &apos;$3==&quot;0&quot;&apos; 1.txt 8、awk -F &apos;:&apos; &apos;$3&gt;=&quot;500&quot;&apos; 1.txt 9、awk -F &apos;:&apos; &apos;$7!=&quot;/sbin/nologin&quot;&apos; 1.txt 10、awk -F &apos;:&apos; &apos;$3&lt;$4&apos; 1.txt 11、awk -F &apos;:&apos; &apos;$3&gt;&quot;5&quot; &amp;&amp; $3&lt;&quot;7&quot;&apos; 1.txt 12、awk -F &apos;:&apos; &apos;$3&gt;&quot;5&quot; || $7==&quot;/bin/bash&quot;&apos; 1.txt 13、awk内置变量NF（段数） NR（行数） 14、head -n3 1.txt |awk -F &apos;:&apos; &apos;{print NF}&apos; 15、head -n3 1.txt |awk -F &apos;:&apos; &apos;{print $NF}&apos; 16、head -n3 1.txt |awk -F &apos;:&apos; &apos;{print NR}&apos; 17、打印20行以后的行awk ’NR&gt;20&apos; 1.txt 18、awk -F &apos;:&apos; &apos;NR&gt;20 &amp;&amp; $1 ~ /ssh/&apos; 1.txt 19、更改某个段的值awk -F &apos;:&apos; &apos;$1=&quot;root&quot; 1.txt 20、数学计算，把第三段和第四段值相加，并赋予第七段 awk -F &apos;:&apos; &apos;{$7=$3+$4; print $0}&apos; 1.txt 21、计算第三段的总和 awk -F &apos;:&apos; &apos;{(tot=tot+$3)}; END {print tot}&apos; 1.txt 22、awk中也可以使用if关键词 awk -F &apos;:&apos; &apos;{if($1==&quot;root&quot;) print$0}&apos; 1.txt sed工具1、打印指定行 sed &apos;10&apos;p -n 1.txt; sed &apos;1,4&apos;p -n 1.txt; sed &apos;5,$&apos;p -n 1.txt; 2、打印包含某个字符串的行 sed -n &apos;/root/&apos;p 1.txt可以使用^.*$等特殊符号 3、-e可以实现同时进行多个任务 sed -e &apos;/root/p&apos; -e &apos;/body/p&apos; -n 1.txt也可以用； 实现sed &apos;/root/p; /body/p&apos; -n 1.txt 4、删除行 sed &apos;/root/d&apos; 1.txt ; sed &apos;1d&apos; 1.txt; sed &apos;1,10d&apos; 1.txt 5、替换 sed &apos;1,2s/ot/to/g&apos; 1.txt 其中s就是替换的意思，g为全局替换，否则只替换第一次的，/也可以为#，@等 6、删除所有数字 sed &apos;s/[0-9]//g&apos; 1.txt 7、删除所有非数字 sed &apos;s/[^0-9]//g&apos; 1.txt 8、调换两个字符串位置 head -n2 1.txt |sed &apos;s/\(root\)\(.*\)\(bash\)/\3\2\1/&apos; 3,2,1代表前面的root，.*，和bash，也就是把root 和bash 调换位置 9、直接修改文件内容 sed -i &apos;s/ot/to/g&apos; 1.txt]]></content>
      <tags>
        <tag>正则表达式</tag>
        <tag>Regular</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb分布式存储]]></title>
    <url>%2F2017%2F11%2F13%2Fmongodb-sharding-note%2F</url>
    <content type="text"><![CDATA[分片(sharding)是指将数据拆分，将其分散存在不同的机器上的过程。有时也用分区(partitioning)来表示这个概念。将数据分散到不同的机器上，不需要功能强大的大型计算机就可以储存更多的数据，处理更多的负载。 MongoDB分片的基本思想就是将集合切分成小块。这些块分散到若干片里面，每个片只负责总数据的一部分。应用程序不必知道哪片对应哪些数据，甚至不需要知道数据已经被拆分了，所以在分片之前要运行一个路由进程，该进程名为mongos。这个路由器知道所有数据的存放位置，所以应用可以连接它来正常发送请求。对应用来说，它仅知道连接了一个普通的mongod。路由器知道数据和片的对应关系，能够转发请求到正确的片上。如果请求有了回应，路由器将其收集起来回送给应用。 设置分片时，需要从集合里面选一个键，用该键的值作为数据拆分的依据。这个键称为片键(shard key)。 {name:&quot;zhangsan&quot;,age:1} 用个例子来说明这个过程：假设有个文档集合表示的是人员。如果选择名字(“name”)作为片键，第一片可能会存放名字以A~F开头的文档，第二片存的G~P的名字，第三片存的Q~Z的名字。随着添加或者删除片，MongoDB会重新平衡数据，使每片的流量都比较均衡，数据量也在合理范围内。分片原理 实战操作 #1、创建三个目录，分别存放两个mongod服务的数据文件和config服务的数据文件 mongod_node1 mongod_node2 config_node #2、开启config服务器 。mongos要把mongod之间的配置放到config服务器里面，所以首先开启它，这里就使用2222端口。 命令为： mongod --dbpath \config_node --port 2222 #3、开启mongos服务器 。这里要注意的是我们开启的是mongos，端口3333，同时指定下config服务器。命令为： mongos --port 3333 --configdb=127.0.0.1:2222 #4、启动mongod服务器 。对分片来说，也就是要添加片了，这里开启两个mongod服务，端口分别为：4444，5555。命令为： mongod --dbpath \mongod_node1 --port 4444 mongod --dbpath \mongod_node2 --port 5555 #5、服务配置 。client直接跟mongos打交道，也就说明我们要连接mongos服务器，然后将4444，5555的mongod交给mongos,添加分片也就是addshard()。 db.runCommand({&quot;addshard&quot;:127.0.0.1:4444&quot;, allowLocal:true}) db.runCommand({&quot;addshard&quot;:127.0.0.1:5555&quot;, allowLocal:true}) #6、开启数据库分片功能，命令很简单 enablesharding(), 这里就开启test数据库。 db.runCommand({&quot;enablesharding&quot;:&quot;test&quot;}) //test为数据库 #7、指定集合中分片的片键，这里就指定为person.name键。 db.runCommand({shardcollection&quot;:&quot;test.person&quot;, &quot;key&quot;:{&quot;name&quot;:1}}) #8、通过mongos插入100w记录，然后通过printShardingStatus命令查看mongodb的数据分片情况。 1234for(var i = 0; i &lt; 1000000; i++) &#123; db.person.insert(&#123;&quot;name&quot;:&quot;zhang&quot;+i, &quot;age&quot;:i&#125;)&#125;db.printShardingStatus()]]></content>
      <tags>
        <tag>MongoDB</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb副本集]]></title>
    <url>%2F2017%2F11%2F12%2Fmongodb-replication%2F</url>
    <content type="text"><![CDATA[副本集就是有自动故障恢复功能的主从集群。主从集群和副本集最大的区别就是副本集没有固定的“主节点”；整个集群会选出一个“主节点”，当其挂掉后，又在剩下的从节点中选中其他节点为“主节点”，副本集总有一个活跃点(primary)和一个或多个备份节点(secondary)。下面以三个节点为例 //节点1： HOST：localhost:10001 Log File：D:\mongodb\logs\node1\logs.txt Data File：D:\mongodb\dbs\node1 //节点2： HOST：localhost:10002 Log File：D:\mongodb\logs\node2\logs.txt Data File：D:\mongodb\dbs\node2 //节点3： HOST：localhost:10003 Log File：D:\mongodb\logs\node3\logs.txt Data File：D:\mongodb\dbs\node3 //启动节点1： mongod --dbpath D:\mongodb\dbs\node1 --logpath D:\mongodb\logs\node1\logs.txt --logappend --port 10001 --replSet hw/localhost:10002 --master //启动节点2： mongod --dbpath D:\mongodb\dbs\node2 --logpath D:\mongodb\logs\node2\logs.txt --logappend --port 10002 --replSet hw/localhost:10001 //启动节点3： mongod --dbpath D:\mongodb\dbs\node3 --logpath D:\mongodb\logs\node3\logs.txt --logappend --port 10003 --replSet hw/localhost:10001,localhost:10002 //初始化节点(只能初始化一次)：随便登录一个节点,以10001为例 mongo localhost:10001/admin db.runCommand({ &quot;replSetInitiate&quot;:{ &quot;_id&quot;:hw&quot;, &quot;members&quot;:[ { &quot;_id&quot;:1, &quot;host&quot;:&quot;localhost:10001&quot;, &quot;priority&quot;:3 }, { &quot;_id&quot;:2, &quot;host&quot;:&quot;localhost:10002&quot;, &quot;priority&quot;:2 }, { &quot;_id&quot;:3, &quot;host&quot;:&quot;localhost:10003&quot;, &quot;priority&quot;:1 } ] }}); 查询当前主库，登录10002 mongo localhost:10002 db.$cmd.findOne ( {ismaster: 1 } ); 关闭10001服务Dos命令窗口, 登录10002查询当前主库 mongo localhost:10002 db.$cmd.findOne ( {ismaster: 1 } );]]></content>
      <tags>
        <tag>MongoDB</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb主从复制集群]]></title>
    <url>%2F2017%2F11%2F12%2Fmongodb-master-slave%2F</url>
    <content type="text"><![CDATA[主从复制是MongoDB最常用的复制方式。这种方式非常灵活，可用于备份、故障恢复、读扩展等。最基本的设置方式就是建立一个主节点和一个或者多个从节点，每个从节点要知道主节点的地址。 //启动主服务器 mongod --master //启动从服务器.其中master_address就是上面主节点的地址。 mongod --slave --source master_address 主从复制主要是用来分流的，读写分离。比如写、改、更新操作在master上，读操作只在slave上。]]></content>
      <tags>
        <tag>MongoDB</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mongodb数据库学习]]></title>
    <url>%2F2017%2F11%2F11%2Fmongodb-note%2F</url>
    <content type="text"><![CDATA[关系型数据库 Mysql、SqlServer、Oracle 非关系型数据库(Nosql not only sql) Redis、MongoDB、HBase、BigTable、CouchDB、Neo4j 结构化的数据：固定的长度、固定的类型、固定的格式。非结构化的数据：avi，ppt，doc 非关系型数据库的优点： 1. 简单的扩展 2. 快速的读写 3. 低廉的成本 4. 灵活的数据模型 非关系型数据库的不足之处： 1. 不提供对SQL的支持 2. 支持的特性不够丰富 3. 现有的产品不够成熟 MongoDB是用C++语言编写的非关系型数据库。特点是高性能、易部署、易使用，存储数据十分方便，主要特性有：1234567面向集合存储，易于存储对象类型的数据模式自由支持动态查询支持完全索引，包含内部对象支持复制和故障恢复使用高效的二进制数据存储，包括大型对象文件存储格式为BSON(一种JSON的扩展) MongoDB几本概念介绍12345文档(document)是MongoDB中数据的基本单元，非常类似于关系型数据库系统中的行(但是比行要复杂的多)集合(collection)就是一组文档，如果说MongoDB中的文档类似于关系型数据库中的行，那么集合就如同表MongoDB的单个计算机可以容纳多个独立的数据库，每一个数据库都有自己的集合和权限MongoDB自带简洁但功能强大的JavaScript shell，这个工具对于管理MongoDB实例和操作数据作用非常大每一个文档都有一个特殊的键&quot;_id&quot;,它在文档所处的集合中是唯一的，相当于关系数据库中的表的主键 MongoDB数据类型 数据类型 描述 举例 null 表示空值或者未定义的对象 {“x”:null} 布尔值 真或者假：true或者false {“x”:true} 32位整数 32位整数。 shell是不支持该类型的，shell中默认会转换成64位浮点数 64位整数 64位整数。 shell是不支持该类型的，shell中默认会转换成64位浮点数 64位浮点数 64位浮点数。 shell中的数字就是这一种类型 {“x”：3.14，”y”：3} 字符串 UTF-8字符串 {“foo”:”bar”} 符号 shell不支持，shell会将数据库中的符号类型的数据自动转换成字符串 对象id 文档的12字节的唯一id {“id”: ObjectId()} 日期 从标准纪元开始的毫秒数 {“date”:new Date()} 正则表达式 文档中可以包含正则表达式，遵循JavaScript的语法 {“foo”:/foobar/i} 代码 文档中可以包含JavaScript代码 {“x”：function() {}} 未定义 undefined {“x”：undefined} 数组 值的集合或者列表 {“arr”: [“a”,”b”]} 内嵌文档 文档可以作为文档中某个key的value {“x”:{“foo”:”bar”}} 为何要用非关系型数据库&emsp;&emsp;在关系型数据库当中为某一个记录扩展一个字段方便吗？不方便，必须要改表结构.在非关系型数据库中，它的结构不固定，每条记录可以有不一样的键，每条记录可以根据需要增加一些自己的键值对 mongodb的安装 将mongodb根目录下bin的路径加到环境变量中即可。 启动mongodb 1. 新建一个mongodb的数据存放路径 2. mongod --dbpath=存放路径 (=号两边不要有空格) 默认占用一个端口对外提供服务（默认端口27017） 客户端连接，默认情况连接是不需要认证的，直接在命令行或者终端中输入下面命令，就可以连接到mongodb的服务器端 mongo ip:port 新建数据库和删除数据库 use xxx //新建数据库 db.dropDatabase() //删除当前数据库 查看所有数据库和查看当前数据库 show dbs //查看所有数据库 db //当前数据库 查看当前数据库中所有的集合 show collections or show tables 新建集合的两种方式，第一种隐式创建 //在创建集合的同时往集合里面添加数据 db.c1.insert({name:&quot;zhangsan&quot;, age:1}); 第二种显示创建 db.createCollection(); 向集合中插入数据 db.collectionName.insert({key:value, key:value}) 删除集合中的文档 db.collectionName.remove({删除条件}) //若不加删除条件为删除集合中的所有文档 查询集合中的文档 db.collectionName.find({查询条件}) db.collectionName.findOne() //查询集合中的文档，返回某些特定的键值，0表示返回不包含该key的所有列，若是1表示返回该key字段 db.collectionName.find({查询条件}, {key:0}) //注：_id字段始终都会被返回，哪怕没有明确指定 查询集合中的文档，使用条件表达式(&lt;, &lt;=, &gt;, &gt;=, !=) //大于： field &gt; value db.collection.find({field:{$gt:value}}); //小于： field &lt; value db.collection.find({field:{$lt:value}}); //大于等于： field &gt;= value db.collection.find({field:{$gte:value}}); //小于等于： field &lt;= value db.collection.find({field:{$lte:value}}); //不等于： field != value db.collection.find({field:{$ne:value}}); 查询集合中的文档，统计(count)、排序(sort)、分页(skip, limit) db.collectionName.count(); db.collectionName.find().count(); db.collectionName.find({age:{$lt:5}}).count(); db.collectionName.find().sort({age:1}); 降序-1 db.collectionName.find().skip(2).limit(3); db.collectionName.find().sort({age:-1}).skip(2).limit(3); db.collectionName.find().sort({age:-1}).skip(2).limit(3).count(); db.collectionName.find().sort({age:-1}).skip(2).limit(3).count(0); db.collectionName.find().sort({age:-1}).skip(2).limit(3).count(1); 查询集合中的文档 ,$all主要用来查询数组中的包含关系，查询条件中只要有一个不包含就不返回 查询集合中的文档 ,$in，类似于关系型数据库中的IN 查询集合中的文档 ,$nin，与$in相反 查询集合中的文档 ,$or，相当于关系型数据库中的OR，表示或者的关系，例如查询name为user2或者age为3的文档，命令为： db.collectionName.find({$or:[{name:”user2”},{age:3}]}) 查询集合中的文档 ,$nor，表示根据条件过滤掉某些数据，例如查询name不是user2，age不是3的文档，命令为： db.collectionName.find({$nor:[{name:”user2”},{age:3}]}) 查询集合中的文档 ,$exists，用于查询集合中存在某个键的文档或不存在某个键的文档，例如查询customer集合中存在name键的所有文档，可以使用 //$exists:1表示真，指存在 //$exists:0表示假，指不存在 db.customer.find({name:{$exists:1}})， 更新集合中的文档 //criteria:用于设置查询条件的对象 //objNew:用于设置更新内容的对象 //upsert:如果记录已经存在，更新它，否则新增一个记录，取值为0或1 //multi：如果有多个符合条件的记录，是否全部更新，取值为0或1 db.collection.update(criteria, objNew, upsert, multi) 注意：默认情况下，只会更新第一个符合条件的记录一般情况下后两个参数分别为0,1 ，即： db.collection.update(criteria,objNew,0,1) 更新集合中的文档, $set 用来指定一个键的值，如果这个键不存在，则创建它。例如：给name为user1的文档添加address，可以使用命令： db.c1.update({name:”user1”},{$set:{address:”bj”}},0,1) 将name为user1的文档修改address为tj，其它键值对不变,命令为： db.c1.update({name:”user1”},{$set:{address:”tj”}},0,1) 更新集合中的文档,使用 $inc 将集合中name为user1的age加1，其它键不变, $inc表示使某个键值加减指定的数值 db.c1.update({name:”user1”},{$inc:{age:1}}) 更新集合中的文档, $unset 用来删除某个键，例如删除name为user1的文档中的address键，可以使用命令： db.collectionName.update({name:”user1”},{$unset:{address:1}},0,1) 索引就是用来加速查询的。数据库索引与书籍的索引类似：有了索引就不需要翻遍整本书，数据库则可以直接在索引中查找，使得查找速度能提高几个数量级。在索引中找到条目以后，就可以直接跳转到目标文档的位置。 //创建普通索引 db.collection.ensureIndex({key:1}) //查看关于索引的相关信息 db.collection.stats() //查看查询使用索引的情况 db.collection.find({key:value}).explain() //删除索引 db.collection.dropIndex({key:1}) //删除集合，也会将集合中的索引全部删除 //创建唯一索引 db.collection.ensureIndex({key:1}，{unique:true}) //查看关于索引的相关信息 db.collection.stats() //查看查询使用索引的情况 db.collection.find({key:value}).explain() //删除索引 db.collection.dropIndex({key:1}) //删除集合，也会将集合中的索引全部删除 固定集合指的是事先创建而且大小固定的集合。固定集合特性：固定集合很像环形队列，如果空间不足，最早的文档就会被删除，为新的文档腾出空间。一般来说，固定集合适用于任何想要自动淘汰过期属性的场景，没有太多的操作限制。创建固定集合使用命令 //size指定集合大小，单位为KB，max指定文档的数量 db.createCollection(“collectionName”,{capped:true,size:100000,max:100}); 当指定文档数量上限时，必须同时指定大小。淘汰机制只有在容量还没有满时才会依据文档数量来工作。要是容量满了，淘汰机制会依据容量来工作。 备份(mongodump)和恢复(mongorestore)MongoDB提供了备份和恢复的功能，分别是MongoDB下载目录下的mongodump.exe和mongorestore.exe文件,备份数据使用下面的命令： //-h：MongDB所在服务器地址，例如：127.0.0.1，当然也可以指定端口号：127.0.0.1:27017 //-d：需要备份的数据库实例，例如：test //-o：备份的数据存放位置，例如：c:\data\dump，当然该目录需要提前建立，在备份完成后，系统自动在dump目录下建立一个test目录，这个目录里面存放该数据库实例的备份数据。 mongodump -h dbhost -d dbname -o dbdirectory 恢复数据使用下面的命令： //-h：MongoDB所在服务器地址 //-d：需要恢复的数据库实例，例如：test，当然这个名称也可以和备份时候的不一样，比如test2 //-directoryperdb：备份数据所在位置，例如：c:\data\dump\test mongorestore -h dbhost -d dbname -directoryperdb dbdirectory 导入(mongoimport)和导出(mongoexport)导出数据可以使用命令 //-h 数据库地址 //-d 指明使用的库 //-c 指明要导出的集合 //-o 指明要导出的文件名 mongoexport -h dbhost -d dbname -c collectionName -o output 导入数据可以使用命令： //-h 数据库地址 //-d 指明使用的库 //-c 指明要导入的集合 mongoimport -h dbhost -d dbname -c collectionname 文件的地址... 安全和认证每个MongoDB实例中的数据库都可以有许多用户。如果开启了安全性检查，则只有数据库认证用户才能执行读或者写操作。在认证的上下文中，MongoDB会将普通的数据作为admin数据库处理。admin数据库中的用户被视为超级用户(即管理员)。在认证之后，管理员可以读写所有数据库，执行特定的管理命令，如listDatabases和shutdown。在开启安全检查之前，一定要至少有一个管理员账号。 在admin数据库中创建管理员账号： use admin; db.addUser(“root”,”root”); 在test数据库中创建普通账号： use test; db.addUser(“zhangsan”,”123”); db.addUser(“lisi”,”123”,true); 注意：用户zhangsan，密码为123，对test数据库拥有读写权限，用户lisi，密码为123，对test数据库拥有只读权限 重新启动数据库服务，并开启安全检查 mongod --dbpath d:\mongo_data --auth]]></content>
      <tags>
        <tag>MongoDB</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle存储函数、触发器、闪回]]></title>
    <url>%2F2017%2F11%2F10%2Foracle-other-note%2F</url>
    <content type="text"><![CDATA[存储函数&emsp;&emsp;函数为一命名的存储程序，可带参数，并返回一计算值。函数和过程的结构类似，但必须有一个return子句，用于返回函数值。函数说明要制定函数名、结果值的类型，以及参数类型等。 触发器总结: 触发器可用于以下情况1234数据确认实施复杂的安全性检查做审计，跟踪表上所做的数据操作等数据的备份和同步 闪回（FlashBack） 1. 错误的删除了数据，并且commit 2. 错误的删除了表drop table 3. 如何获取表上的历史记录 4. 如何撤销一个已经提交了的事务 闪回的类型 1. 闪回表: 将表回退到过去的一个时间上 （9i） 2. 闪回删除：操作Oracle的回收站 3. 闪回版本查询： 表上的历史记录 4. 闪回事务查询： 获取一个 undo_sql 5. 返回数据库： 将数据库回退到过去的一个时间上 6. 闪回归档日志 设置回退的时间间隔 alter system set undo_retention=1200 scope=both; scope的取值：memoery(只改当前数据库，退出之后恢复)，spfile(数据库重启之后也保持不变)，both(上面两者的结合) 总结： 闪回表show parameters undo --&gt; 900秒 SCN 系统改变号 执行闪回表 行移动 alter table xxx enable row movement 系统表（数据字典）不能闪回 不能跨越DDL 问题：如何获取离该操作最近的一个时间 闪回删除 Oracle的回收站：管理员没有回收站 执行闪回删除 （1）通过表明闪回删除 （2）通过回收站中的名字闪回删除 （3）闪回重名的表]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql备份和恢复]]></title>
    <url>%2F2017%2F11%2F09%2Fmysql-backup-and-recovery%2F</url>
    <content type="text"><![CDATA[备份 mysqldump -uroot -p db &gt; db.sql 恢复 mysql -uroot -p db &lt; db.sql 只备份一个表 mysqldumpp -uroot -p db tb1 &gt; table.sql 备份时指定字符集 mysqldump -uroot -p --default-character-set=utf8 db &gt; db.sql 恢复也指定字符集 mysql -uroot -p --default-character=utf8 db &lt; db.sql 这是myisam和innodb都可以用上述的方式备份，mysqldump。myisam引擎特点：可以直接拷贝源文件，比较小。innodb引擎不可以，因为里面是个很大的文件，虚拟的内存，比较大。]]></content>
      <tags>
        <tag>Mysql</tag>
        <tag>数据库=</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[更改MySQL数据库密码以及忘记密码修改]]></title>
    <url>%2F2017%2F11%2F08%2Fmysql-fixed-password%2F</url>
    <content type="text"><![CDATA[修改Mysql的root账户密码，默认root密码是空的，可以直接登录 PATH=$PATH:/usr/local/mysql/bin 加入到/etc/profile然后执行以下命令，使命令生效 source /etc/profile mysqladmin修改密码使用 mysqladmin -uroot password ‘yourpass’ //设置密码 更改root密码 mysqladmin -uroot -p password ‘newpass’ 连接mysql mysql -uroot -p -h ip -Pport 如果忘记了mysql密码，可以使用下面的方式来重新设置密码，编辑mysql主配置文件my.cnf，在[mysqld]字段下添加参数skip-grant，重启数据库服务 service mysqld restart 修改相应用户密码 use mysql； update user set password=password(&apos;yourpassword&apos;) where user=&apos;root&apos;; 修改/etc/my.cnf 去掉skip-grant，重启mysql]]></content>
      <tags>
        <tag>Mysql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql数据库常用操作]]></title>
    <url>%2F2017%2F11%2F08%2Fmysql-common-operator%2F</url>
    <content type="text"><![CDATA[查看都有哪些库 show databases; 查看某个库的表 use db； show tables； 查看表的字段 desc tb； 查看建表语句 show create table tb； 在结尾加上\G格式化一下，比较容易观察。 当前是哪个用户 select user()； 当前库 select database()； 创建库 create datebase db1； 创建表 create table t1 (`id` int(4), `name` char(40)); 查看数据库版本 select version(); 查看mysql状态 show status; 修改mysql参数 show variables like &apos;max_connect%&apos;; 可以在/etc/my.cnf里面修改 set global max_connect_errors = 1000; mysql -uroot -p654321 -hlocalhost -P3306 -e ‘show processlist’ 查看mysql队列 show processlist; 创建普通用户并授权 grant all on *.* to user1 identified by &apos;123456&apos;; grant all on db1.* to &apos;user2&apos;@&apos;10.0.2.100&apos; identified by &apos;111222&apos;; grant all on db1.* &apos;user3&apos;@&apos;identified by &apos;231222&apos;; 更改密码 UPDATE mysql.user SET password=PASSWORD(&quot;newpwd&quot;)WHERE user=&apos;username&apos;; 刷新缓存 flush privileges; 查询 select count(*) from mysql.user; select * from mysql.db; select * from mysql.db where host like &apos;10.0.%&apos;; 插入 update db1.t1 set name=&apos;aaa&apos; where id=1; 清空表 truncate table db1.t1； 删除表 drop table db1.t1; 删除数据库 drop database db1; 修复表tb1 repair table tb1 [use_frm]; repair table 表名；]]></content>
      <tags>
        <tag>Mysql</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle索引和视图]]></title>
    <url>%2F2017%2F11%2F07%2Foracle-index-view%2F</url>
    <content type="text"><![CDATA[视图的优点 1. 限制数据访问 2. 简化复杂查询 3. 提供数据的相互独立 4. 同样的数据，可以有不同的显示方式 但视图不能提高性能。 索引 1. 一种独立于表的模式对象，可以存储在与表不同的磁盘或表空间中 2. 索引被删除或损坏，不会对表产生影响，其影响的只是查询的速度 3. 索引一旦建立，Oracle管理系统会对其进行自动维护，而且由Oracle管理系统决定何时使用索引，用户不用再查询语句中指定使用哪个索引 4. 在删除一个表时，所有基于该表的索引会自动被删除 5. 通过指针加速Oracle服务器的查询速度 6. 通过快速定位数据的方法，减少磁盘I/O 哪些情况可以创建索引 1. 列中数据值分布范围很广 2. 列经常在WHERE子句或连接条件中出现 3. 表经常被访问而且数据量很大，访问的数据大概占数据总量的2%到4% 哪些情况不要创建索引 1. 表很小 2. 列不经常作为连接条件或出现在WHERE子句中 3. 查询的数据大于2%到4% 4. 表经常更新 同义词相当于别名]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle子查询]]></title>
    <url>%2F2017%2F11%2F06%2Foracle-subquery%2F</url>
    <content type="text"><![CDATA[子查询所要解决的问题：不能一步求解注意的问题 1. 括号 2. 合理的书写风格 3. 可以在主查询的where select having from后面放置子查询 4. 不可以在group by后面放置子查询 5. 强调from后面的子查询 6. 主查询和子查询可以不是同一张表；只要子查询返回的结果主查询可以使用即可 7. 一般不在子查询中排序；但在Top-N分析问题中，必须对子查询排序 8. 一般先执行子查询，再执行主查询；但相关子查询例外 9. 单行子查询只能使用单行操作符；多行子查询只能使用多行操作符 10. 子查询中的null 集合运算，注意的问题: 1. 参与运算的各个集合必须列数相同 且类型一致 2. 采用第一个集合表头作为最后表头 3. order by 永远在最后 4. 括号]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[oracle数据类型]]></title>
    <url>%2F2017%2F11%2F06%2Foracle-type%2F</url>
    <content type="text"><![CDATA[SQL的类型 1. DML(Data Manipulation Language 数据操作语言): insert update delete select 2. DDL(Data Definition Language 数据定义语言): create/alter/drop/truncate tabl create/drop view,sequence,index,synonym(同义词) 3. DCL(Data Control Language 数据控制语言）： grant(授权) revoke（撤销权限） delete 和truncate区别 1. delete 逐条删除，truncate先删除表，再重建 2. delete是DML(可以回滚) truncate是DDL（不可以回滚） 3. delete 不会释放空间；truncate会 //delete 并不是真正删除；把数据换个地方(undo 表空间)存 4. delete会产生碎片 truncate不会 5. delete可以闪回(flashback) truncate不可以 set feedback off 回显信息关闭 海量插入数据 1. 数据泵 2. SQL*Loader 3. 外部表 去除碎片 1、alter table 表名 move 2、导出导入 Oracle中的事务 1. 起始标志：事务中第一条DML语句 2. 结束标志： 提交： 显式 commit 隐式 正常退出exit，DDL，DCL 回滚 显式 rollback 隐式 非正常退出，掉电，宕机]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle单行函数]]></title>
    <url>%2F2017%2F11%2F05%2Foracle-simple-funcation%2F</url>
    <content type="text"><![CDATA[字符函数 lower(&apos;Hello World&apos;) 小写 upper(&apos;Hello World&apos;) 大写 initcap(&apos;hello world&apos;) 首字母大写 substr(a,b) 从a中，第b位开始取子串 substr(a,b,c) 从a中，第b位开始取,取c位 length 字符数 lengthb 字节数 instr(a,b) 在a中，查找b lpad 左填充 rpad 右填充。 lpad(&apos;abcd&apos;,10,&apos;@&apos;) //@@@@@@@@@@abcd rpad(&apos;abcd&apos;,10,&apos;@&apos;) //abcd@@@@@@@@@@ trim 去掉前后指定的字符 trim(&apos;H&apos; from &apos;Hello World&apos;) replace 替换 replace(&apos;Hello World&apos;,&apos;l&apos;,&apos;*&apos;) 数值函数 round 四舍五入 round(56.926,2) 56.93 round(56.926,1) 56.9 round(56.926,0) 57 round(56.926,-1) 60 round(56.926,-2) 100 trunc 截断 MOD 求余 日期函数 当前时间 select sysdate from dual; 格式化显式一个时间 select to_char(sysdate,&apos;yyyy-mm-dd hh24:mi:ss&apos;) from dual; 昨天、今天、明天 sysdate-1、sysdate、sysdate+1 日期段 months_between months_between(sysdate,employtime) //精确值 日期相加，不支持两个日期相加(+) add_months add_months(sysdate,22) to_char(sysdate,&apos;yyyy-mm-dd hh24:mi:ss&quot;今天是&quot;day&apos;) 本月的最后一天 last_day last_day(sysdate) 下一个星期五 next_day next_day(sysdate,&apos;星期五&apos;) 两位小数，千位符，货币代码 to_char(salary,&apos;L9,999.99&apos;) 日期四舍五入 round round(sysdate,&apos;month&apos;)、round(sysdate,&apos;year&apos;) //month看day，year看month nvl2(a,b,c) 当a=null时候，返回c；否则返回b，相当于三元运算符 nullif(a,b) 当a=b时候，返回null，否则返回a coalesce 从左到右找到第一个不为null的值并返回 判断 select name,job,salary 涨前, case job when &apos;PRESIDENT&apos; then salary+1000 when &apos;MANAGER&apos; then salary+800 else salary+400 end 涨后 from emp; select name,job,salary 涨前, decode job, &apos;PRESIDENT&apos;, sal+1000, &apos;MANAGER&apos;, sal+800, sal+400) 涨后 from emp; next_day的应用 每个星期一自动备份数据 1、分布式数据库 2、快照]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle数据库操作]]></title>
    <url>%2F2017%2F11%2F05%2Foracle-note%2F</url>
    <content type="text"><![CDATA[查看所有的表 select * from tab; 对于出错的sql语句，可以通过数字进行定位，然后使用–c（change）来改变对应出错的地方。 c /form/from 使用/命令是执行上一条命令 ／ null值的注意事项： 1、包含null的表达式都为null 2、null永远不等于null 3. 如果集合中，含有null，不能使用not in；但是可以使用in 4. null的排序 5. 组函数自动滤空;可以嵌套滤空函数来屏蔽他的滤空功能 1、对于要处理null的表达式，可以使用nvl(a,b),若a不为null，则返回a的值 2、对于要选择null的列，如果使用column=null则一条也不能查询出来，而要选择column is null才可以选择null的列。 3、in, not in 本质上是逻辑or运算，in(id=1 or id=2 or id=null), not(id=1 and id=2 and id=null), 其中 False or Null == Null, True or Null == True 4. null的升序是没有问题的，降序的时候null最大，会排在最前面，可以使用nulls last命令将其排在后面。 edit命令，执行edit（ed）命令之后，在windows上会将上一条sql通过记事本打开，在linux上则会通过vi打开。 对于使用别名当中，若有特殊字符或者关键字，需要加上双引号。 distinct作用于后面的所有列。 连接符 –concat select concat(&apos;a&apos;,&apos;b&apos;) from dual; dual表跟任何表都没有关系。 select concat(1+3) from dual; dual(伪表)是为了满足语法的要求。 select &apos;a&apos;||&apos;b&apos; 字符串 from dual; //||相当于加号 结果如下： 字符串 a b 字符串 日期和字符只能在单引号中出现 sql和sqlplus sql（select, insert, update, delete）不能缩写 sqlplus(ed(edit), c(change), format(for), col(column), desc) 可以缩写 iSQL*Plus其实就是sqlplus的网页版，oracle 9i,10g支持（不安全，协议http）。11g,12c不支持 表述表结构、 编辑SQL语句、 执行SQL语句、 将SQL保存在文件并将SQL语句执行结果保存在文件中、 在保存的文件中执行语句、 将文本文件装入SQL*Plus编辑窗口 查看状态 lsnctl status 清屏 host cls 录制操作，可以记录操作的内容 spool xxx.txt，spool of 追加，后面的空格必须是两个空格 apppend 注释 -- 查询注意 字符串大小写敏感 日期格式敏感 修改日期格式 select * from v$nls_parameters; alter session set NLS_DATE_FORMAT=&apos;yyyy-mm-dd&apos;; //session只在当前会话有效 between……and 1. 含有边界 2. 小值在前,大值在后 in 在集合中 like 模糊查询 like &apos;S%&apos;; &apos;_&apos;是占位符，比如姓名为三个字，like &apos;___&apos;，对于有特殊字符的需要进行转义。比如 like &apos;%\_%&apos; escape &apos;\&apos; oracle 自动开启事务 order by 后面可以加，列，表达式，别名，序号(select的第几列) 作用于后面所有的列，先按照第一个列排序，如果相同，再按照第二列排序；以此类推。 desc 只作用于离他最近的一列 SQL执行时间的开关 set timing on set timing off 和集合中任意一个值比较 any 和集合中的所有值比较 all 行号,伪列 rownum 关于行号 1. rownum 永远按照默认的顺序生成 2. rownum只能使用&lt; &lt;=; 不能使用&gt; &gt;= 临时表：create global temporary table 特点：当事务或者会话结束的时候，表中的数据自动删除 sql优化 1、*和字段名 2、where 从右往左，先判断右边的在判断左边的 3、尽量使用where 4. 理论上，尽量使用多表查询 5. 尽量不要使用集合运算]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle多行函数]]></title>
    <url>%2F2017%2F11%2F05%2Foracle-multiply-funcation%2F</url>
    <content type="text"><![CDATA[多行函数 sum count(*) 平均值 1、avg(column) //组函数自动滤空 2、sum(column)/count(*) 3、sum(column)/count(column) //count(column)不为null的记录 按分组求平均值的时候，先求出平均值，在分组即可。 where后面不能使用组函数而having可以，where也要在group by前面，having是在group by之后。 where先过滤在分组 having先分组在过滤 group by rollup(a,b) group by a,b + group by a + group by null 多表查询 等值连接 不等值连接 外连接 对于某些不成立的记录，仍然希望包含在最后的结果中 左外连接：当where e.deptno=d.deptno不成立的时候，等号左边的表仍然被包含 where e.deptno=d.deptno(+) 右外连接：当where e.deptno=d.deptno不成立的时候，等号右边的表仍然被包含 where e.deptno(+)=d.deptno 自连接：通过表的别名，将同一张表视为多张表，自连接不适合操作大表]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式之grep]]></title>
    <url>%2F2017%2F11%2F04%2FRegular-Expression%2F</url>
    <content type="text"><![CDATA[语法：grep [-cinvABC] ‘word’ filename123456789-c:打印符合要求的行数-i:忽略大小写-n:在输出符合要求的行的同时连同行号一起输出-v:打印不符合要求的行-A:后跟一个数字（有无空格都可以）例如-A2则表示打印符合要求的行以及下面两行-B：后跟一个数字，例如-B2则表示打印符号要求的行以及上面两行-C： 后跟一个数字，例如-C2则表示打印符合要求的行以及上下个两行-r：会把目录下所有的文件全部遍历-w：w为word的意思 grep工具实例：1、过滤出带有某个关键词的行并输出行号 grep -n &apos;root&apos; 1.txt 2、过滤出不带有某个关键词的行并输出行号 grep -n -v &apos;root&apos; 1.txt 3、过滤出所有包含数字的行 grep &apos;[0-9]&apos; 1.txt 4、去除所有以‘#’开头的行 grep -v &apos;^#&apos; 1.txt 5、去除所有空行和以’#’开头的行 grep -v &apos;^$&apos; 1.txt|grep -v&apos;^#&apos; 或者是： grep -v &apos;^$&apos; |grep -v&apos;^#&apos; 1.txt 6、过滤出以英文字母开头的行 grep &apos;^[a-zA-Z]&apos; 1.txt 7、过滤出以非数字开头的行 grep &apos;^[^0-9]&apos; 1.txt 8、过滤出任意一个或多个字符 grep &apos;r.o&apos; 1.txt; r,t之间的任意字符，r，t必须有 grep &apos;r*t&apos; 1.txt; 零个或多个r，但是必须有t grep &apos;r.*t&apos; 1.txt; 只要有r和t就行了 9、.表示任意一个字符；*表示零个或多个前面的字符 .*表示零个或多个任意字符，空行也包括在内 10、指定过滤字符次数 grep &apos;o\{2\}&apos; 1.txt]]></content>
      <tags>
        <tag>正则表达式</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis学习笔记续二]]></title>
    <url>%2F2017%2F11%2F03%2Fmybatis-advance-note%2F</url>
    <content type="text"><![CDATA[Mybatis的应用场景&emsp;&emsp;对需求变更大的项目，关系数据模型不固定，比如互联网项目，建议使用mybatis，灵活去编写sql语句，对sql语句进行修改、优化方便。 Mybatis开发过程 12345配置SqlMapConfig.xml（全局配置文件，名称不固定）配置XXXMapper.xml映射文件根据配置创建SqlSessionFactory,SqlSessionFactory在实际使用时，建议使用单例模式根据SqlSessionFactory创建Sqlsession,Sqlsession是一个面向用户的接口，是线程不安全的，最佳应用场合是方法体内,SqlSession提供操作数据库的常用方法，内部使用Exceutor（基本执行器、缓存执行器）底层封装类操作数据库。通过Sqlsession操作数据库,对于插入、删除、更新进行事务提交,SqlSesssion使用完毕进行关闭。 Mybatis延迟加载&emsp;&emsp;延迟加载意义：在需求允许的情况下，先查询单表，当需要关联其它表查询时，进行延迟加载，去关联查询，达到目标：不需要关联信息时不查询，需要时再查询。好处：提高数据库的性能。如果不使用mybatis的延迟加载，可以使用以下方法 1234567定义两个mapper接口。查询订单信息列表根据用户id查询用户信息查询过程：首次查询只调用第一个mapper接口（查询订单信息列表）当需要查询某个订单的用户信息时，从订单信息中获取用户id，调用第二个mapper（根据用户id查询用户信息）。 使用Mybatis的延迟加载，过程如下打开延迟加载开关，在SqlMapConfig.xml中配置setting全局参数：123456lazyLoadingEnabled：延迟加载的总开关，设置为trueaggressiveLazyLoading：设置为false，实现按需加载（将积极变为消极）&lt;settings&gt; &lt;setting name=&apos;lazyLoadingEnabled&apos; value=&apos;true&apos;&gt;&lt;/setting&gt; &lt;setting name=&apos;aggressiveLazyLoading&apos; value=&apos;false&apos;&gt;&lt;/setting&gt;&lt;/settings&gt; 配置延迟加载: 要根据订单信息中的用户id(外键)查询用户信息，修改订单信息查询的statement。将resultType改为resultMap，定义resultMap，配置延迟加载：然后在resultMap中添加需要延迟加载的配置，通过&lt;associattion property=&apos;&apos; javaType=&apos;&apos; select=&apos;xxxmethod&apos; column=&apos;要传入的参数值&apos;/&gt; 使用association关联查询单个对象和使用collection关联查询集合对象都支持延迟加载。 Mybatis缓存&emsp;&emsp;将从数据库中查询出来的数据缓存起来，缓存介质：内存、磁盘，从缓存中取数据，而不从数据库查询，减少了数据库的操作，提高了数据处理性能。 一级缓存，Mybatis默认提供一级缓存，缓存范围是一个sqlSession。在同一个SqlSession中，两次执行相同的sql查询，第二次不再从数据库查询。但是如果第一次查询后，执行commit提交，mybatis会清除缓存，第二次查询从数据库查询。 一级缓存的原理：一级缓存采用Hashmap存储，mybatis执行查询时，从缓存中查询，如果缓存中没有从数据库查询。如果该SqlSession执行commit()提交，清除缓存 Map的key：（code+。。。statement的id+sql+输入参。。） 二级缓存，缓存范围是跨SqlSession的，范围是mapper的namespace，相同的namespace使用一个二级缓存结构。需要进行参数配置让mybatis支持二级缓存。在核心配置文件SqlMapConfig.xml中加入，表示打开二级缓存开关 &lt;setting name=&quot;cacheEnabled&quot; value=&quot;true&quot;/&gt; 还需要在mapper.xml中配置是否打开该mapper的二级缓存。 &lt;cache/&gt; //配置二级缓存 二级缓存注意事项： 1、实现序列化 2、如何清除缓存。执行相同的statement，如果执行提交操作需要清除二级缓存。如果想让statement执行后刷新缓存（清除缓存），在statement中设置flushCache=&quot;true&quot; （默认值 是true） 3、设置statement是否开启二级缓存。如果让某个statement启用二级缓存，设置useCache=true（默认值为true 二级缓存原理：如果二缓存开启，首先从二级缓存查询数据，如果二级缓存有则从二级缓存中获取数据，如果二级缓存没有，从一级缓存找是否有缓存数据，如果一级缓存没有，查询数据库。 二级缓存应用场景1、针对复杂的查询或统计的功能，用户不要求每次都查询到最新信息，使用二级缓存，通过刷新间隔flushInterval设置刷新间隔时间，由mybatis自动刷新。比如：实现用户分类统计sql，该查询非常耗费时间。将用户分类统计sql查询结果使用二级缓存，同时设置刷新间隔时间：flushInterval（一般设置时间较长，比如30分钟，60分钟，24小时，根据需求而定。 2、针对信息变化频率高，需要显示最新的信息，使用二级缓存。将信息查询的statement与信息的增、删、改定义在一个mapper.xml中，此mapper实现二级缓存，当执行增、删、修改时，由mybatis及时刷新缓存，满足用户从缓存查询到最新的数据。比如：新闻列表显示前10条，该查询非常快，但并发大对数据也有压力。将新闻列表查询前10条的sql进行二级缓存，这里不用刷新间隔时间，当执行新闻添加、最佳的方案使用页面缓存。]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis学习笔记续]]></title>
    <url>%2F2017%2F11%2F02%2Fmybatis-note-second%2F</url>
    <content type="text"><![CDATA[Mapper动态代理方法只需要写dao接口(Mapper)，而不需要写dao实现类，由mybatis根据dao接口和映射文件中statement的定义生成接口实现代理对象。可以调用代理对象方法。Mybatis官方建议：将dao接口叫做mapper。 目标：通过一些规则让mybatis根据dao接口和映射文件中statement的定义生成接口实现代理对象，mybatis将以下代码自动在代理对象实现： User user = sqlSession.selectUser(&quot;findUserById&quot;, id); 为了让XXXmapper.xml和XXXmapper.java对应起来，将XXXmapper.xml中的namespace设置为XXXmapper.java的全限定名（包括包名）。 Mybatis生成代理对象时，根据statement的标签决定调用SqlSession的方法(select、insert、update..)，根据上边接口方法返回值,类型来决定是调用selectOne还是selectList，如果返回的是单个对象，动态代理调用selectOne()，如果返回的是集合对象，动态代理调用selectList()。 Mapper开发规则 1234在XXXmapper.xml中将namespace设置为mapper.java的全限定名将XXXmapper.java接口的方法名和XXXmapper.xml中statement的id保持一致。将XXXmapper.java接口的方法输入参数类型和XXXapper.xml中statement的parameterType保持一致将XXXmapper.java接口的方法输出结果类型和XXXmapper.xml中statement的resultType保持一致 Mybatis配置文件SqlMapConfig.xml作为mybatis的全局配置文件，配置内容包括：数据库环境、mapper定义、全局参数设置。 properties（属性） settings（全局配置参数） typeAliases（类型别名） typeHandlers（类型处理器） objectFactory（对象工厂） plugins（插件） environments（环境集合属性对象） environment（环境子属性对象） transactionManager（事务管理） dataSource（数据源） mappers（映射器） typeAliases（类型别名） 在parameterType和resultType设置时，为了方便编码，定义别名代替pojo的全路径。 typeHandlers（类型处理器） 类型处理器用于java类型和jdbc类型映射. parameterType(输入类型) parameterType：用于设置输入参数的类型 resultType(结果类型) 将sql查询结果集映射为java对象。要求sql查询的字段名和resultType指定pojo的属性名一致，才能映射成功。如果全部字段和pojo的属性名不一致，映射生成的java对象为空，只要有一个字段和pojo属性名一致，映射生成的java对象不为空。 结论：sql查询字段名和pojo的属性名一致才能映射成功。当然也可以返回简单类型，比如int，string。 resultType和resultMap区别 resultType：sql查询结果集使用resultType映射，要求sql查询字段名和pojo的属性名一致，才能映射成功。 resultMap： 如果sql查询结果集的字段名和pojo的属性名不一致，使用resultMap将sql查询字段名和pojo的属性作一个对应关系 ，首先需要定义resultMap，最终要使用pojo作为结果集映射对象。 不管select返回的是单个 对象还是集合对象，resultType要指定单条记录映射的java对象。 #{}与${}的区别&emsp;&emsp;#{}：表示占位符，如果获取简单类型，#{}中可以使用value或其它名称。有效防止sql注入使用#{}设置参数无需考虑参数的类型。 比如使用#{}比较日期字段， select * from tablename where birthday &gt;=#{birthday} &emsp;&emsp;${}：表示sql拼接，如果获取简单类型，#{}中只能使用value。${}无法防止sql注入。使用${}设置参数必须考虑参数的类型，比如：使用oracle查询条件是日期类型，如果使用${}，必须人为将${}两边加单引号通过to_date转日期。 Select * from table where birthday &gt;=to_date(‘${birthday}’,’yyyy-MM-dd’) 在没有特殊要求的情况下，建议使用#{}占位符,有些情况必须使用${}，比如：需要动态拼接表名 Select * from ${tablename} 动态拼接排序字段： select * from tablename order by ${username} desc 包装对象的使用为了方便查询，我们通常不只是查询一个pojo中的属性，还有可能会查询其它的条件，比如数组或者集合，这时我们可以通过包装对象来实现。将我们要查询的条件封装到包装对象中，然后在我们调用XXXMap.java的时候直接传入我们的参数即可。 当sql语句比较多的时候，会发现有很多重复的where条件，这时我们可以通过抽取sql，转换为动态sql(比如if)或者sql片段。]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis学习笔记]]></title>
    <url>%2F2017%2F11%2F01%2Fmybatis-note%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;MyBatis本是apache的一个开源项目iBatis, 2010年这个项目由apache software foundation 迁移到了google code，并且改名为MyBatis。MyBatis是一个优秀的持久层框架，它对jdbc的操作数据库的过程进行封装，使开发者只需要关注SQL本身，而不需要花费精力去处理例如注册驱动、创建connection、创建statement、手动设置参数、结果集检索等jdbc繁杂的过程代码。Mybatis通过xml或注解的方式将要执行的statement配置起来，并通过java对象和statement中的sql进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射成java对象并返回。 引入Mybatis之前，我们先看下传统的JDBC的不雅之处 1、将sql语句硬编码到java代码中，如果修改sql语句，需要修改java代码，重新编译。系统可维护性不高。设想如何解决？能否将sql单独 配置在配置文件中。 2、数据库连接频繁开启和释放，对数据库的资源是一种浪费。设想如何解决？使用数据库连接池管理数据库连接。 3、向preparedStatement中占位符的位置设置参数时，存在硬编码（占位符的位置，设置的变量值）设想如何解决？能否也通过配置的方式，配置设置的参数，自动进行设置参数。 4、解析结果集时存在硬编码（表的字段名、字段的类型）设想如何解决？能否将查询结果集映射成java对象。 可以看到上面这几条对于我们来说还是不太友好了，引入mybatis之后，这些问题就会迎刃而解。先看下mybatis配置文件 1、mybatis.xml，此文件作为mybatis的全局配置文件，配置了mybatis的运行环境等信息。mapper.xml文件即sql映射文件，文件中配置了操作数据库的sql语句。此文件需要在mybatis.xml中加载。 2、通过mybatis环境等配置信息构造SqlSessionFactory即会话工厂 3、由会话工厂创建sqlSession即会话，操作数据库需要通过sqlSession进行。 4、mybatis底层自定义了Executor执行器接口操作数据库，Executor接口有两个实现，一个是基本执行器、一个是缓存执行器。 5、Mapped Statement也是mybatis一个底层封装对象，它包装了mybatis配置信息及sql映射信息等。mapper.xml文件中一个sql对应一个Mapped Statement对象，sql的id即是Mapped statement的id。 6、Mapped Statement对sql执行输入参数进行定义,包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql前将输入的java对象映射至sql中，输入参数映射就是jdbc编程中对preparedStatement设置参数。 7、Mapped Statement对sql执行输出结果进行定义，包括HashMap、基本类型、pojo，Executor通过Mapped Statement在执行sql后将输出结果映射至java对象中，输出结果映射过程相当于jdbc编程中对结果的解析处理过程。]]></content>
      <tags>
        <tag>数据库</tag>
        <tag>Mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive-hql详解]]></title>
    <url>%2F2017%2F10%2F29%2Fhive-hql%2F</url>
    <content type="text"><![CDATA[set hive.cli.print.header=true; CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT &apos;IP Address of the User&apos;) COMMENT &apos;This is the page view table&apos; PARTITIONED BY(dt STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;\001&apos; STORED AS SEQUENCEFILE; TEXTFILE sequencefile create table tab_ip_seq(id int,name string,ip string,country string) row format delimited fields terminated by &apos;,&apos; stored as sequencefile; insert overwrite table tab_ip_seq select * from tab_ext; create &amp; load create table tab_ip(id int,name string,ip string,country string) row format delimited fields terminated by &apos;,&apos; stored as textfile; load data local inpath &apos;/home/hadoop/ip.txt&apos; into table tab_ext; external,外部表被drop时，只清除元数据，表数据并不会被删除 CREATE EXTERNAL TABLE tab_ip_ext(id int, name string, ip STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE LOCATION &apos;/external/hive&apos;; CTAS 用于创建一些临时表存储中间结果 CREATE TABLE tab_ip_ctas AS SELECT id new_id, name new_name, ip new_ip,country new_country FROM tab_ip_ext SORT BY new_id; insert from select 用于向临时表中追加中间结果数据 create table tab_ip_like like tab_ip; insert overwrite table tab_ip_like select * from tab_ip; CLUSTER &lt;–相对高级一点，你可以放在有精力的时候才去学习&gt; create table tab_ip_cluster(id int,name string,ip string,country string) clustered by(id) into 3 buckets; load data local inpath &apos;/home/hadoop/ip.txt&apos; overwrite into table tab_ip_cluster; set hive.enforce.bucketing=true; insert into table tab_ip_cluster select * from tab_ip; select * from tab_ip_cluster tablesample(bucket 2 out of 3 on id); PARTITION create table tab_ip_part(id int,name string,ip string,country string) partitioned by (part_flag string) row format delimited fields terminated by &apos;,&apos;; load data local inpath &apos;/home/hadoop/ip.txt&apos; overwrite into table tab_ip_part partition(part_flag=&apos;part1&apos;); load data local inpath &apos;/home/hadoop/ip_part2.txt&apos; overwrite into table tab_ip_part partition(part_flag=&apos;part2&apos;); select * from tab_ip_part; select * from tab_ip_part where part_flag=&apos;part2&apos;; select count(*) from tab_ip_part where part_flag=&apos;part2&apos;; alter table tab_ip change id id_alter string; ALTER TABLE tab_cts ADD PARTITION (partCol = &apos;dt&apos;) location &apos;/external/hive/dt&apos;; show partitions tab_ip_part; write to hdfs insert overwrite local directory &apos;/home/hadoop/hivetemp/test.txt&apos; select * from tab_ip_part where part_flag=&apos;part1&apos;; insert overwrite directory &apos;/hiveout.txt&apos; select * from tab_ip_part where part_flag=&apos;part1&apos;; (不支持into，也就是不支持文件的追加) array create table tab_array(a array&lt;int&gt;,b array&lt;string&gt;) row format delimited fields terminated by &apos;\t&apos; collection items terminated by &apos;,&apos;; 示例数据 tobenbrone,laihama,woshishui 13866987898,13287654321 abc,iloveyou,itcast 13866987898,13287654321 select a[0] from tab_array; select * from tab_array where array_contains(b,&apos;word&apos;); insert into table tab_array select array(0),array(name,ip) from tab_ext t; map create table tab_map(name string,info map&lt;string,string&gt;) row format delimited fields terminated by &apos;\t&apos; collection items terminated by &apos;;&apos; map keys terminated by &apos;:&apos;; 示例数据： fengjie age:18;size:36A;addr:usa furong age:28;size:39C;addr:beijing;weight:90KG load data local inpath &apos;/home/hadoop/hivetemp/tab_map.txt&apos; overwrite into table tab_map; insert into table tab_map select name,map(&apos;name&apos;,name,&apos;ip&apos;,ip) from tab_ext; struct create table tab_struct(name string,info struct&lt;age:int,tel:string,addr:string&gt;) row format delimited fields terminated by &apos;\t&apos; collection items terminated by &apos;,&apos; load data local inpath &apos;/home/hadoop/hivetemp/tab_st.txt&apos; overwrite into table tab_struct; insert into table tab_struct select name,named_struct(&apos;age&apos;,id,&apos;tel&apos;,name,&apos;addr&apos;,country) from tab_ext; cli shell [hadoop@yun~]hive -S -e &apos;select country,count(*) from tab_ext&apos; &gt; /home/hadoop/hivetemp/e.txt 有了这种执行机制，就使得我们可以利用脚本语言（bash shell,python）进行hql语句的批量执行 select * from tab_ext sort by id desc limit 5; select a.ip,b.book from tab_ext a join tab_ip_book b on(a.name=b.name); UDF select if(id=1,first,no-first),name from tab_ext; hive&gt;add jar /home/hadoop/myudf.jar; hive&gt;CREATE TEMPORARY FUNCTION my_lower AS &apos;org.dht.Lower&apos;; select my_upper(name) from tab_ext;]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive学习笔记]]></title>
    <url>%2F2017%2F10%2F28%2Fhive-study-note%2F</url>
    <content type="text"><![CDATA[上传hive安装包 解压 配置 安装mysql 查询以前安装的mysql相关包 rpm -qa | grep mysql 暴力删除这个包 rpm -e mysql-libs-5.1.66-2.el6_3.i686 --nodeps rpm -ivh MySQL-server-5.1.73-1.glibc23.i386.rpm rpm -ivh MySQL-client-5.1.73-1.glibc23.i386.rpm 执行命令设置mysql /usr/bin/mysql_secure_installation 将hive添加到环境变量当中 GRANT ALL PRIVILEGES ON hive.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;123&apos; WITH GRANT OPTION; FLUSH PRIVILEGES create table trade_detail (id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by &apos;\t&apos;; create table user_info (id bigint, account string, name string, age int) row format delimited fields terminated by &apos;\t&apos;; 将mysq当中的数据直接导入到hive当中 sqoop import --connect jdbc:mysql://192.168.1.10:3306/itcast --username root --password 123 --table trade_detail --hive-import --hive-overwrite --hive-table trade_detail --fields-terminated-by &apos;\t&apos; sqoop import --connect jdbc:mysql://192.168.1.10:3306/itcast --username root --password 123 --table user_info --hive-import --hive-overwrite --hive-table user_info --fields-terminated-by &apos;\t&apos; 创建一个result表保存前一个sql执行的结果 create table result row format delimited fields terminated by &apos;\t&apos; as select t2.account, t2.name, t1.income, t1.expenses, t1.surplus from user_info t2 join (select account, sum(income) as income, sum(expenses) as expenses, sum(income-expenses) as surplus from trade_detail group by account) t1 on (t1.account = t2.account); create table user (id int, name string) row format delimited fields terminated by &apos;\t&apos; 将本地文件系统上的数据导入到HIVE当中 load data local inpath &apos;/root/user.txt&apos; into table user; 创建外部表 create external table stubak (id int, name string) row format delimited fields terminated by &apos;\t&apos; location &apos;/stubak&apos;; 普通表和分区表区别：有大量数据增加的需要建分区表 create table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by &apos;\t&apos;; 分区表加载数据 load data local inpath &apos;./book.txt&apos; overwrite into table book partition (pubdate=&apos;2010-08-22&apos;);]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase配置文件hbase-site.xml]]></title>
    <url>%2F2017%2F10%2F28%2Fhbase-site-xml%2F</url>
    <content type="text"><![CDATA[&lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;yun4:2181,yun5:2181,yun6:2181&lt;/value&gt; &lt;/property&gt;]]></content>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活杂味]]></title>
    <url>%2F2017%2F10%2F25%2Fstudents-life-think%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;每天驱动我们的不是梦想，而是现实。现实让我们一天一天的进步。你不前进则后退。 &emsp;&emsp;每一次的开始都有不一样的感觉，这次再次踏上合肥之路，心里满满的努力。星期五，中午吃过饭就匆匆忙忙的去赶车。赶车确实有趣，本以为是礼拜五，想到没有那么多人车上，心里还暗自欣喜，结果则是满满的人。让我不由的欣赏这些回家的，是幸福，异是幸福。在车上还是蛮挤人的。都可以相互踩到脚了。这次出来是带着任务出来的，心理还是很开心的。可以多见识一些。在我的思维散发的时刻，不知不觉就到车站了。下车之后，当然是先取票啦，几个人一起去取票，七八个人走在一起，感觉还是蛮壮大的嘛。取过票，就直接进车站了，稍后就踏上了火车。有一次坐火车，心里还是蛮喜欢的。喜欢火车上的平静，比在汽车上舒服多了。只是火车上没有那样好的氛围，没人看杂志，书籍。倒是有人玩手机，打扑克。这也是我国的一大特色吧。其实，在火车上还挺享受的，可能是因为自己没有做过几次火车吧，对火车有一种亲切感，每次做都有开心的事情。或多或少都可以见识见识，看到各地的人，感觉人与人之间的差别还是很大的。坐火车上看外面的风景也是不错，但是不足的是火车给人体现出来的速度不是那么让人喜欢，总感觉它是没有走，以后有机会就去做做飞机，动车体现一下。对于火车晚点，这件事大家还是很反感的，对于大家与其晚点那么长时间，还不如在路途中走慢一点呢，那样大家的心理就不会有太多的抱怨啦。人总是那么的关注眼前。往往被其它的事务所迷住。跟老师在一起，他们倒是挺嗨的，把我们放在一边，他们在那里打牌，打的是嗨，一直打到火车下站，要不是我们提醒，还能坐过头呢～4，5个小时，在不知不觉度过，之后便是火车到站，到站时间已是9点半了，也没有公交了，坐滴走了，出来怎么都是好的。可是自己肚子早已饿的不行，但是还得忍着，要到宾馆才行。没想到宾馆这次比上次还好，心理又是32赞。把东西放下就立刻出来去吃饭了，还没有想好要吃什么呢，老师带着我们吃大排档，哈哈这个我喜欢，比较有氛围，大家围在一起，还可以说说话。这次让我惊讶的是老师竟然要了酒，想不到，老师说明天上午又没有重要的事情，喝点酒，消消乏。感觉跟老师坐在一起，大家都比较生疏，都没有太多的话跟老师聊，只能借助酒了，老师先认识认识学生，一人来一杯。每个人都有机会。一圈下来，没想到他们却更内向了。还不如不喝呢，大家都不怎么喝酒。让我不能理解，很奇怪也，按道理我们应该把老师陪好的呀。接下来也是随便说说，跟老师说说学校里的事，生活中的事，老师还是可以聊的来嘛，还是有很多话题的。越说越觉得像是一家人。就这样不知不觉的吃过饭，已是十二点啦，这也算夜宵啦。然后都回宾馆了，也不能出去玩玩啦，回到宾馆洗洗就睡觉啦。一天的旅途结束，满满的幸福。 喜欢书、喜欢运动、喜欢文字、抬头看路、低头做人，这就是我————阿文]]></content>
      <tags>
        <tag>生活</tag>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase深入学习]]></title>
    <url>%2F2017%2F10%2F25%2Fhbase-high-study%2F</url>
    <content type="text"><![CDATA[1、表的设计（行键）2、模糊查询 3、索引建立（二级索引：协同处理器coorprocessor，solr+hbase）4、运维：主备集群之间的复制]]></content>
      <tags>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase shell]]></title>
    <url>%2F2017%2F10%2F24%2Fhbase-shell%2F</url>
    <content type="text"><![CDATA[进入hbase命令行 ./hbase shell 显示hbase中的表 list 创建user表，包含info、data两个列族 create &apos;user&apos;, &apos;info1&apos;, &apos;data1&apos; create &apos;user&apos;, {NAME =&gt; &apos;info&apos;, VERSIONS =&gt; &apos;3&apos;} 向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsan put &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;zhangsan&apos; 向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为female put &apos;user&apos;, &apos;rk0001&apos;, &apos;info:gender&apos;, &apos;female&apos; 向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20 put &apos;user&apos;, &apos;rk0001&apos;, &apos;info:age&apos;, 20 向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为picture put &apos;user&apos;, &apos;rk0001&apos;, &apos;data:pic&apos;, &apos;picture&apos; 获取user表中row key为rk0001的所有信息 get &apos;user&apos;, &apos;rk0001&apos; 获取user表中row key为rk0001，info列族的所有信息 get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos; 获取user表中row key为rk0001，info列族的name、age列标示符的信息 get &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;info:age&apos; 获取user表中row key为rk0001，info、data列族的信息 get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;, &apos;data&apos; get &apos;user&apos;, &apos;rk0001&apos;, {COLUMN =&gt; [&apos;info&apos;, &apos;data&apos;]} get &apos;user&apos;, &apos;rk0001&apos;, {COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]} 获取user表中row key为rk0001，列族为info，版本号最新5个的信息 get &apos;user&apos;, &apos;rk0001&apos;, {COLUMN =&gt; &apos;info&apos;, VERSIONS =&gt; 2} get &apos;user&apos;, &apos;rk0001&apos;, {COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5} get &apos;user&apos;, &apos;rk0001&apos;, {COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]} 获取user表中row key为rk0001，cell的值为zhangsan的信息 get &apos;people&apos;, &apos;rk0001&apos;, {FILTER =&gt; &quot;ValueFilter(=, &apos;binary:图片&apos;)&quot;} 获取user表中row key为rk0001，列标示符中含有a的信息 get &apos;people&apos;, &apos;rk0001&apos;, {FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;} put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:name&apos;, &apos;fanbingbing&apos; put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:gender&apos;, &apos;female&apos; put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:nationality&apos;, &apos;中国&apos; get &apos;user&apos;, &apos;rk0002&apos;, {FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;} 查询user表中的所有信息 scan &apos;user&apos; 查询user表中列族为info的信息 scan &apos;user&apos;, {COLUMNS =&gt; &apos;info&apos;} scan &apos;user&apos;, {COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 5} scan &apos;persion&apos;, {COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 3} 查询user表中列族为info和data的信息 scan &apos;user&apos;, {COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;]} scan &apos;user&apos;, {COLUMNS =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]} 查询user表中列族为info、列标示符为name的信息 scan &apos;user&apos;, {COLUMNS =&gt; &apos;info:name&apos;} 查询user表中列族为info、列标示符为name的信息,并且版本最新的5个 scan &apos;user&apos;, {COLUMNS =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5} 查询user表中列族为info和data且列标示符中含有a字符的信息 scan &apos;user&apos;, {COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;} 查询user表中列族为info，rk范围是[rk0001, rk0003)的数据 scan &apos;people&apos;, {COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;} 查询user表中row key以rk字符开头的 scan &apos;user&apos;,{FILTER=&gt;&quot;PrefixFilter(&apos;rk&apos;)&quot;} 查询user表中指定范围的数据 scan &apos;user&apos;, {TIMERANGE =&gt; [1392368783980, 1392380169184]} 删除数据, 删除user表row key为rk0001，列标示符为info:name的数据 delete &apos;people&apos;, &apos;rk0001&apos;, &apos;info:name&apos; 删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据 delete &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, 1392383705316 清空user表中的数据 truncate &apos;people&apos; 修改表结构,首先停用user表（新版本不用） disable &apos;user&apos; 添加两个列族f1和f2 alter &apos;people&apos;, NAME =&gt; &apos;f1&apos; alter &apos;user&apos;, NAME =&gt; &apos;f2&apos; 启用表 enable &apos;user&apos; 禁用表 disable &apos;user&apos;(新版本不用) 删除一个列族： alter &apos;user&apos;, NAME =&gt; &apos;f1&apos;, METHOD =&gt; &apos;delete&apos; 或 alter &apos;user&apos;, &apos;delete&apos; =&gt; &apos;f1&apos; 添加列族f1同时删除列族f2 alter &apos;user&apos;, {NAME =&gt; &apos;f1&apos;}, {NAME =&gt; &apos;f2&apos;, METHOD =&gt; &apos;delete&apos;} 将user表的f1列族版本号改为5 alter &apos;people&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 5 启用表 enable &apos;user&apos; 删除表 disable &apos;user&apos; drop &apos;user&apos; get &apos;person&apos;, &apos;rk0001&apos;, {FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;} get &apos;person&apos;, &apos;rk0001&apos;, {FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;} scan &apos;person&apos;, {COLUMNS =&gt; &apos;info:name&apos;} scan &apos;person&apos;, {COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;} scan &apos;person&apos;, {COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;} scan &apos;person&apos;, {COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;20140201&apos;, ENDROW =&gt; &apos;20140301&apos;} scan &apos;person&apos;, {COLUMNS =&gt; &apos;info:name&apos;, TIMERANGE =&gt; [1395978233636, 1395987769587]} delete &apos;person&apos;, &apos;rk0001&apos;, &apos;info:name&apos; alter &apos;person&apos;, NAME =&gt; &apos;ffff&apos; alter &apos;person&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 10 get &apos;user&apos;, &apos;rk0002&apos;, {COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]}]]></content>
      <tags>
        <tag>Hbase</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase-集群]]></title>
    <url>%2F2017%2F10%2F23%2Fhbase-colony%2F</url>
    <content type="text"><![CDATA[1.上传hbase安装包 2.解压 tar -zxvf hbase.xx.tar.gz 3.配置hbase集群，要修改3个文件（首先zk集群已经安装好了） 注意：要把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf下 3.1 修改hbase-env.sh export JAVA_HOME=/usr/java/jdk1.7.0_55 //告诉hbase使用外部的zk export HBASE_MANAGES_ZK=false vim hbase-site.xml &lt;configuration&gt; &lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;weekend05:2181,weekend06:2181,weekend07:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; vim regionservers yun3 yun4 yun5 yun6 3.2拷贝hbase到其他节点 scp -r /yun/hbase-0.96.2-hadoop2/ yun2:/app/ scp -r /yun/hbase-0.96.2-hadoop2/ yun3:/app/ scp -r /yun/hbase-0.96.2-hadoop2/ yun4:/app/ scp -r /yun/hbase-0.96.2-hadoop2/ yun5:/app/ scp -r /yun/hbase-0.96.2-hadoop2/ yun6:/app/ 4.将配置好的HBase拷贝到每一个节点并同步时间。 5.启动所有的hbase 分别启动zk ./zkServer.sh start 启动hdfs集群 start-dfs.sh 启动hbase，在主节点上运行： start-hbase.sh 6.通过浏览器访问hbase管理页面 192.168.31.123:60010 7.为保证集群的可靠性，要启动多个HMaster hbase-daemon.sh start master]]></content>
      <tags>
        <tag>大数据</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper应用场景]]></title>
    <url>%2F2017%2F10%2F22%2Fzookeeper-apply-scene%2F</url>
    <content type="text"><![CDATA[ZooKeeper是一个高可用的分布式数据管理与系统协调框架。基于对Paxos算法的实现，使该框架保证了分布式环境中数据的强一致性，也正是基于这样的特性，使得ZooKeeper解决很多分布式问题。 数据发布与订阅（配置中心）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;发布与订阅模型，即所谓的配置中心，顾名思义就是发布者将数据发布到ZK节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，服务式服务框架的服务地址列表等就非常适合使用。 1、应用中用到的一些配置信息放到ZK上进行集中管理。这类场景通常是这样：应用在启动的时候会主动来获取一次配置，同时，在节点上注册一个Watcher，这样一来，以后每次配置有更新的时候，都会实时通知到订阅的客户端，从而达到获取最新配置信息的目的。 2、分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在ZK的一些指定节点，供各个客户端订阅使用。 3、分布式日志收集系统。这个系统的核心工作是收集分布在不同机器的日志。收集器通常是按照应用来分配收集任务单元，因此需要在ZK上创建一个以应用名作为path的节点P，并将这个应用的所有机器ip，以子节点的形式注册到节点P上，这样一来就能够实现机器变动的时候，能够实时通知到收集器调整任务分配。 4、系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息的发问。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入ZK之后，就不用自己实现一套方案了，只要将这些信息存放到指定的ZK节点上即可。 注意：在上面提到的应用场景中，有个默认前提是：数据量很小，但是数据更新可能会比较快的场景。 负载均衡&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;负载均衡是指软负载均衡。在分布式环境中，为了保证高可用性，通常同一个应用或同一个服务的提供方都会部署多份，达到对等服务。而消费者就须要在这些对等的服务器中选择一个来执行相关的业务逻辑，其中比较典型的是消息中间件中的生产者，消费者负载均衡。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;消息中间件中发布者和订阅者的负载均衡，linkedin开源的KafkaMQ和阿里开源的metaq都是通过zookeeper来做到生产者、消费者的负载均衡。 这里以metaq为例讲下： 生产者负载均衡:metaq发送消息的时候，生产者在发送消息的时候必须选择一台broker上的一个分区来发送消息，因此metaq在运行过程中，会把所有broker和对应的分区信息全部注册到ZK指定节点上，默认的策略是一个依次轮询的过程，生产者在通过ZK获取分区列表之后，会按照brokerId和partition的顺序排列组织成一个有序的分区列表，发送的时候按照从头到尾循环往复的方式选择一个分区来发送消息。 消费负载均衡：在消费过程中，一个消费者会消费一个或多个分区中的消息，但是一个分区只会由一个消费者来消费。MetaQ的消费策略是 每个分区针对同一个group只挂载一个消费者。 如果同一个group的消费者数目大于分区数目，则多出来的消费者将不参与消费。 如果同一个group的消费者数目小于分区数目，则有部分消费者需要额外承担消费任务。在某个消费者故障或者重启等情况下，其他消费者会感知到这一变化（通过 zookeeper watch消费者列表），然后重新进行负载均衡，保证所有的分区都有消费者进行消费. 命名服务(Naming Service)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，远程对象等等——–这些我们都可以统称他们为名字（Name）。其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用ZK提供的创建节点的 API，能够很容易创建一个全局唯一的path，这个path就可以作为一个名称。 阿里巴巴集团开源的分布式服务框架Dubbo中使用ZooKeeper来作为其命名服务，维护全局的服务地址列表，[点击这里][1]查看Dubbo开源项目。在Dubbo实现中：服务提供者在启动的时候，向ZK上的指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。服务消费者启动的时候，订阅/dubbo/${serviceName}/providers目录下的提供者URL地址， 并向/dubbo/${serviceName}／consumers目录下写入自己的URL地址。 注意，所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。另外，Dubbo还有针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者的信息。 分布式通知/协调&emsp;&emsp;ZooKeeper中特有watcher注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对ZK上同一个znode进行注册，监听znode的变化（包括znode本身内容及子节点的），其中一个系统update了znode，那么另一个系统能够收到通知，并作出相应处理。 另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过zk上某个节点关联，大大减少系统耦合。 另一种系统调度模式：某系统有控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了ZK上某些节点的状态，而ZK就把这些变化通知给他们注册Watcher的客户端，即推送系统，于是，作出相应的推送任务。 另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到zk来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。 总之，使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合 集群管理与Master选举&emsp;&emsp;集群机器监控：这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报“我还活着”。 这种做法可行，但是存在两个比较明显的问题： 1、集群中机器有变动的时候，牵连修改的东西比较多。 2、有一定的延时。 利用ZooKeeper有两个特性，就可以实时另一种集群机器存活性监控系统： 1、客户端在节点 x 上注册一个Watcher，那么如果 x?的子节点变化了，会通知该客户端。 2、创建EPHEMERAL类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。例如，监控系统在 /clusterServers 节点上注册一个Watcher，以后每动态加机器，那么就往 /clusterServers 下创建一个 EPHEMERAL类型的节点：/clusterServers/{hostname}. 这样，监控系统就能够实时知道机器的增减情况，至于后续处理就是监控系统的业务了。 &emsp;&emsp;Master选举则是zookeeper中最为经典的应用场景了。在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑（例如一些耗时的计算，网络I/O处理），往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个master选举便是这种场景下的碰到的主要问题。利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /currentMaster 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选取了。另外，这种场景演化一下，就是动态Master选举。这就要用到?EPHEMERAL_SEQUENTIAL类型节点的特性了。上文中提到，所有客户端创建请求，最终只有一个能够创建成功。在这里稍微变化下，就是允许所有请求都能够创建成功，但是得有个创建顺序，于是所有的请求最终在ZK上创建结果的一种可能情况是这样： /currentMaster/{sessionId}-1 ,?/currentMaster/{sessionId}-2 ,?/currentMaster/{sessionId}-3等等，每次选取序列号最小的那个机器作为Master，如果这个机器挂了，由于他创建的节点会马上消失，那么之后最小的那个机器就是Master了。 &emsp;&emsp;在搜索系统中，如果集群中每个机器都生成一份全量索引，不仅耗时，而且不能保证彼此之间索引数据一致。因此让集群中的Master来进行全量索引的生成，然后同步到集群中其它机器。另外，Master选举的容灾措施是，可以随时进行手动指定master，就是说应用在zk在无法获取master信息时，可以通过比如http方式，向一个地方获取master。 &emsp;&emsp;在Hbase中，也是使用ZooKeeper来实现动态HMaster的选举。在Hbase实现中，会在ZK上存储一些ROOT表的地址和 HMaster的地址，HRegionServer也会把自己以临时节点（Ephemeral）的方式注册到Zookeeper中，使得HMaster可以随时感知到各个HRegionServer的存活状态，同时，一旦HMaster出现问题，会重新选举出一个HMaster来运行，从而避免了 HMaster的单点问题 分布式锁分布式锁，这个主要得益于ZooKeeper为我们保证了数据的强一致性。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 所谓保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把zk上的一个znode看作是一把锁，通过 create znode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。 控制时序，就是所有视图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock已经预先存在，客户端在它下面创建临时有序节点（这个可以通过节点的属性控制：CreateMode.EPHEMERAL_SEQUENTIAL来指定）。Zk的父节点（/distribute_lock）维持一份sequence,保证子节点创建的时序性，从而也形成了每个客户端的全局时序。 分布式队列队列方面，简单地讲有两种，一种是常规的先进先出队列，另一种是要等到队列成员聚齐之后的才统一按序执行。对于第一种先进先出队列，和分布式锁服务中的控制时序场景基本原理一致，这里不再赘述。第二种队列其实是在FIFO队列的基础上作了一个增强。通常可以在 /queue这个znode下预先建立一个/queue/num 节点，并且赋值为n（或者直接给/queue赋值n），表示队列大小，之后每次有队列成员加入后，就判断下是否已经到达队列大小，决定是否可以开始执行了。这种用法的典型场景是，分布式环境中，一个大任务Task A，需要在很多子任务完成（或条件就绪）情况下才能进行。这个时候，凡是其中一个子任务完成（就绪），那么就去 /taskList 下建立自己的临时时序节点（CreateMode.EPHEMERAL_SEQUENTIAL），当/taskList 发现自己下面的子节点满足指定个数，就可以进行下一步按序进行处理了。]]></content>
      <tags>
        <tag>Zookeeper</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper环境搭建]]></title>
    <url>%2F2017%2F10%2F21%2Fzookeeper-build%2F</url>
    <content type="text"><![CDATA[1.下载zookeeper压缩包，若是在虚拟机上跑的机器，需要将zk上传，也可以通过yum命令直接在虚拟机里面下载。 2.解压 tar -zxvf zookeeper压缩包 3.配置（先在一台节点上配置）&nbsp;&nbsp;&nbsp;&nbsp;3.1 添加一个zoo.cfg配置文件,该配置文件在Zookeeper根目录/conf下面 mv zoo_sample.cfg zoo.cfg &nbsp;&nbsp;&nbsp;&nbsp;3.2 修改配置文件（zoo.cfg） dataDir=/home/tools/hadoop/zookeeper-3.4.5/data server.1=ip:2888:3888 server.2=ip:2888:3888 server.3=ip:2888:3888 &nbsp;&nbsp;&nbsp;&nbsp;3.3 在（dataDir=/home/tools/hadoop/zookeeper-3.4.5/data）创建一个myid文件，里面内容是server.N中的N（server.2里面内容为2） echo &quot;1&quot; &gt; myid &nbsp;&nbsp;&nbsp;&nbsp;3.4将配置好的zk拷贝到其他节点 scp -r /yun/zookeeper-3.4.5/ yun2:/tools/ scp -r /yun/zookeeper-3.4.5/ yun3:/tools/ &nbsp;&nbsp;&nbsp;&nbsp;3.5注意：在其他节点上一定要修改myid的内容 在yun2应该讲myid的内容改为2 （echo &quot;2&quot; &gt; myid） 在yun3应该讲myid的内容改为3 （echo &quot;3&quot; &gt; myid） 4.启动集群&nbsp;&nbsp;&nbsp;&nbsp;分别启动zk ./zkServer.sh start zookeeper的最主要功能：1、保管客户端提交的数据（极其少量的数据),每一份数据在zookeeper叫做一个znode,znode之间形成一种树状结构（类似于文件树）比如： /lock(znode名) host001(znode中存的数据) /lock/last_acc(znode名) host008(znode中存的数据)]]></content>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper配置文件详解]]></title>
    <url>%2F2017%2F10%2F21%2Fzookeeper-setting-explain%2F</url>
    <content type="text"><![CDATA[zookeeper的默认配置文件为zookeeper/conf/zoo_sample.cfg，需要将其修改为zoo.cfg。在修改之前，最好保留一份。 cp zoo_sample.cfg zoo.cfg 其中各配置项的含义，解释如下： 1.tickTime：CS通信心跳时间Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳。tickTime以毫秒为单位。 tickTime=2000 2.initLimit：LF初始通信时限集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。 initLimit=5 3.syncLimit：LF同步通信时限集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。 syncLimit=2 4.dataDir：数据文件目录Zookeeper保存数据的目录，默认情况下，Zookeeper将写数据的日志文件也保存在这个目录里。 dataDir=/home/tools/opt/zookeeper/data 5.clientPort：客户端连接端口客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。 clientPort=2181 6.服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口）这个配置项的书写格式比较特殊，规则如下：2888端口号是zookeeper服务之间通信的端口，而3888是zookeeper与其他应用程序通信的端口 server.N=ip:A:B server.1=ip:2888:3888 server.2=ip:2888:3888 server.3=ip:2888:3888]]></content>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac上安装mysql笔记]]></title>
    <url>%2F2017%2F10%2F20%2Fmysql-install-for-mac%2F</url>
    <content type="text"><![CDATA[1、从mysql官网上找到download，然后选择下面的Mysql Community Server，找到对应的mac版本，即后缀名为.dmg的文件，点击下载,然后点击下图中的按钮即可下载。 2、下载之后打开对应的dmg文件，会弹出安装提示框，按照提示框提示一直点击下一步，等待安装完成。安装完成之后会出现一个对话框，提示你这是mysql的root账户的初始密码，在退出该对话框之前，请先保存！！！3、打开终端，输入下面命令 mysql -uroot -p 会提示错误，找不到mysql命令，这是我们需要配置一下我们的环境变量，编辑/etc/profile文件 vi /etc/profile 发现没有权限，是因为我们是用自己的账户进入的，也就是你当前的账户不是root账户，需要我们切换到root账户下，然后在去编辑该文件。对于没有使用过root账户的，这是我们执行下面的命令 su root 会提示我们输入密码，然后输入root账户的密码，但是我们之前一直没有使用过root密码，所以这时候我们需要重新设置一下root密码，然后在重新进入root账户下即可。执行以下命令 su passwd root 提示输入新的密码，这时我们就可以重新设置密码了。设置密码之后，再切换到root账户下。 su root 输入我们刚才设置的密码，就可以切换到root账户下。这时我们编辑/etc/profile文件，将mysql添加到path目录下。 PATH=$PATH:/usr/local/mysql/bin //保存退出的时候，如果发现没有权限对该文件写操作，可以先使用 `q!` 命令强制退出。 //然后使用chmod重新授权 chmod -R 755 /etc/profile 保存退出之后，刷新一下该文件 source /etc/profile 这时我们在进入mysql命令行界面。输入以下命令 mysql -uroot -p 输入我们一开始安装完成mysql时弹出的初始密码，就可以进入mysql命令行界面。进入命令行之后，我们执行下面的命令。 show databases; 却发现提示错误，提示我们在执行命令之前要先更新密码，然后在执行命令。这时我们要更新密码。更新密码之前，我们先打开系统偏好设置，在最下面找到mysql，将mysql停止掉。然后我们在命令行下面输入下面的命令 sudo /usr/local/mysql/bin/mysqld_safe --skip-grant-tables &amp; ‘&amp;’ 符号的作用时在后台执行。然后使用updata语句更新我们的密码。 UPDATE mysql.user SET authentication_string=PASSWORD(&apos;new password&apos;) WHERE User=&apos;root&apos;; 刷新 FLUSH PRIVILEGES; 最后重启我们的mysql服务，再次进入mysql命令行界面，重新执行语句就OK了。但是为了安全起见，我们要建立一个当前mysql帐号，不要使用mysql的root账户进入。我们先进入mysql命令行下面，然后执行下面的命令： create user hanwen identified by &apos;123456&apos;; 这样我们就建立一个新的mysql账户，也可以授权给该用户。 create database test; //创建test数据库 //该用户拥有test数据库的所有权限 grant all privileges on test.* to hanwen@localhost identified by &apos;123456&apos;; //或者授予root权限 GRANT ALL PRIVILEGES ON *.* TO &apos;hanwen&apos;@&apos;%&apos;; flush privileges;//刷新系统权限表 我们重新退出，在使用新用户登录即可。]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS冗余数据块的自动删除]]></title>
    <url>%2F2017%2F10%2F19%2Fhdfs-redundancy-auto-delete%2F</url>
    <content type="text"><![CDATA[在日常维护Hadoop集群的过程中发现这样一种情况:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;某个节点由于网络故障或者DataNode进程死亡，被NameNode判定为死亡，HDFS马上自动开始数据块的容错拷贝；当该节点重新添加到集群中时，由于该节点上的数据其实并没有损坏，所以造成了HDFS上某些block的备份数超过了设定的备份数。通过观察发现，这些多余的数据块经过很长的一段时间才会被完全删除掉，那么这个时间取决于什么呢？ 该时间的长短跟数据块报告的间隔时间有关。Datanode会定期将当前该结点上所有的block信息报告给Namenode，参数dfs.blockreport.intervalMsec就是控制这个报告间隔的参数。 hdfs-site.xml文件中有一个参数： &lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;3600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt; &lt;/property&gt; 其中3600000为默认设置，3600000毫秒，即1个小时，也就是说，块报告的时间间隔为1个小时，所以经过了很长时间这些多余的块才被删除掉。通过实际测试发现，当把该参数调整的稍小一点的时候（60秒），多余的数据块确实很快就被删除了。]]></content>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DataNode节点超时时间设置]]></title>
    <url>%2F2017%2F10%2F18%2Fdatanode-timeout-setting%2F</url>
    <content type="text"><![CDATA[DataNode 进程死亡或者网络故障造成 DataNode 无法与 NameNode 通信，NameNode 不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS 默认的超时时长为 10 分钟 + 30 秒。如果定义超时时间为 timeout，则超时时长的计算公式为： timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。 而默认的 heartbeat.recheck.interval 大小为 5 分钟，dfs.heartbeat.interval 默认为 3 秒。需要注意的是 hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒， dfs.heartbeat.interval 的单位为秒。所以，举个例子，如果 heartbeat.recheck.interval 设置为 5000（毫秒），dfs.heartbeat.interval 设置为 3（秒，默认），则总的超时时间为 40 秒。 hdfs-site.xml 中的参数设置格式： &lt;property&gt; &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;]]></content>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DNS 了解下]]></title>
    <url>%2F2017%2F10%2F18%2Fdns-note%2F</url>
    <content type="text"><![CDATA[DNS为Domain Name System（域名系统）的缩写，它是一种将ip地址转换成对应的主机名或将主机名转换成与之相对应ip地址的一种服务机制。 其中通过域名解析出ip地址的叫做正向解析，通过ip地址解析出域名的叫做反向解析。 DNS使用TCP和UDP协议，端口号都是53，但它主要使用UDP协议，服务器之间备份使用TCP协议。 全世界只有13台“根”服务器，1个主根服务器放在美国，其它12台为辅根服务器 DNS服务器根据角色可以分为：主DNS服务器，从DNS服务器，缓存DNS服务器，DNS转发服务器。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记之Combiner]]></title>
    <url>%2F2017%2F10%2F17%2Fcombiner%2F</url>
    <content type="text"><![CDATA[1、是在每一个map task的本地运行，能收到map输出的每一个key的valuelist，所以可以做局部汇总处理 2、因为在map task的本地进行了局部汇总，就会让map端的输出数据量大幅精简，减小shuffle过程的网络IO 3、combiner其实就是一个reducer组件，跟真实的reducer的区别就在于，combiner运行maptask的本地 4、combiner在使用时需要注意，输入输出KV数据类型要跟map和reduce的相应数据类型匹配 5、要注意业务逻辑不能因为combiner的加入而受影响]]></content>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop副本存放策略]]></title>
    <url>%2F2017%2F10%2F15%2Fhadoop-replication-save%2F</url>
    <content type="text"><![CDATA[1、先在客户端所连接的datanode上存放一个副本2、再在另一个机架上选择一个datanode存放第二个副本3、最后在本机架上根据负载情况随机挑选一个datanode存放第三个副本 副本数量的配置优先级1、服务端hdfs-site.xml中可以配置2、在客户端指定dfs.replication的值 客户端所指定的值优先级更高!!!]]></content>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop环境搭建]]></title>
    <url>%2F2017%2F10%2F14%2Fhadoop-environment-build%2F</url>
    <content type="text"><![CDATA[1. 准备Linux环境1.1 修改主机名 vim /etc/sysconfig/network NETWORKING=yes HOSTNAME=yun 1.2 修改IP 修改配置文件方式 vim /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=&quot;eth0&quot; BOOTPROTO=&quot;static&quot; ONBOOT=&quot;yes&quot; IPADDR=&quot;192.168.80.190&quot; NETMASK=&quot;255.255.255.0&quot; GATEWAY=&quot;192.168.80.88&quot; 1.3 修改主机名和IP的映射关系 vim /etc/hosts 192.168.80.190 yun 1.4 关闭防火墙 #查看防火墙状态 service iptables status #关闭防火墙 service iptables stop #查看防火墙开机启动状态 chkconfig iptables --list #关闭防火墙开机启动 chkconfig iptables off 1.5重启Linux reboot 或者 init 6 2. 安装JDK2.1 上传alt+p 后出现sftp窗口，然后put jdk路径 2.2 解压jdk #创建文件夹 mkdir /home/hadoop/app #解压 tar -zxvf jdk-xxxx.tar.gz -C /root/xxx 2.3 将java添加到环境变量中 vim /etc/profile #在文件最后添加 export JAVA_HOME=xxxxx export PATH=$PATH:$JAVA_HOME/bin #刷新配置 source /etc/profile 3. 安装hadoop2.4.1先上传hadoop的安装包到服务器上去/root/hadoop/ 注意：hadoop2.x的配置文件$HADOOP_HOME/etc/hadoop 伪分布式需要修改5个配置文件 3.1 配置hadoop 第一个：hadoop-env.sh vim hadoop-env.sh #第27行 export JAVA_HOME=/xxx/xxx/xxx 第二个：core-site.xml &lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://xxxx:8000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/xxx/xxx/hadoop-2.4.1/tmp&lt;/value&gt; &lt;/property&gt; 第三个：hdfs-site.xml hdfs-default.xml (3) &lt;!-- 指定HDFS副本的数量 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 第四个：mapred-site.xml (mv mapred-site.xml.template mapred-site.xml) mv mapred-site.xml.template mapred-site.xml vim mapred-site.xml &lt;!-- 指定mr运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 第五个：yarn-site.xml &lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hostname&lt;/value&gt; &lt;/property&gt; &lt;!-- reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; 3.2 将hadoop添加到环境变量 vim /etc/proflie export JAVA_HOME=/xxxx/xxx/jdk export HADOOP_HOME=/xxxx/hadoop-2.4.1 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin source /etc/profile 3.3 格式化namenode（是对namenode进行初始化） hdfs namenode -format (hadoop namenode -format) 3.4 启动hadoop 先启动HDFS sbin/start-dfs.sh 再启动YARN sbin/start-yarn.sh 3.5 验证是否启动成功 使用jps命令验证 54212 NameNode 12345 Jps 32142 SecondaryNameNode 23672 NodeManager 34712 ResourceManager 26341 DataNode http://xxxx:50070 （HDFS管理界面） http://xxxx:8088 （MR管理界面） 4.配置ssh免登陆#生成ssh免登陆密钥 #进入到我的home目录 cd ~/.ssh ssh-keygen -t rsa （四个回车） 执行完这个命令后，会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） 将公钥拷贝到要免登陆的机器上 ssh-copy-id localhost 5. 安装过程中出现错误解决错误：localhost: /home/hadoop/hadoop/sbin/slaves.sh: line 60: ssh: command not found 解决：安装openssh-client或者openssh-clients]]></content>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssh配置无密码登陆]]></title>
    <url>%2F2017%2F10%2F14%2Fssh-keygen%2F</url>
    <content type="text"><![CDATA[1、在A机器上生成密钥对ssh-keygen 一路回车或者 ssh-keygen -t rsa 一路回车 2、把A的公钥复制到B上cat .ssh/id_rsa.pub，然后复制内容到B机器上的.ssh/authorized_keys 3、A上直接 ssh B机器的IP]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[午睡之想]]></title>
    <url>%2F2017%2F10%2F12%2Fmid-afternoon-think%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;早上起来外面下着蒙蒙的细雨。萌萌欲睡的我就这样到了三点半，从早上都是处于一种欲睡的感觉，可能是觉睡的有点少了吧。昨晚两点睡觉的，今天早上不到七点就起床了。（这样写有点搞笑唉～～～）起来之后洗洗脸，刷刷牙，就去食堂去吃饭，然后就去那个优美之地去读我最喜欢的英语。虽然自己英语很烂，但我就是喜欢它，没法办阿。就像自己心上的东西一样，我很想它，想每时每刻都可以看见它。可是我却还没有做到这种程度，很大程度上我都是忽略了它，还没有想过它的感受。自己有点自私了。自己在下着蒙蒙的细雨中读书真爽，只不过有点冷而已。最近淮北的天气有点不正常，时冷时热让人有点不好穿衣服。早上还冷飕飕的，中午有时候就热的让人难受。在雨中与自己心爱的笔记本度过了半个小时，然后就去教室上课了。到班级里也快打上课铃了，刚开始的上课的时候，我玩了会手机，玩着玩着自己心里就觉得很悲哀，因为突然之间发现根本没有人在听课。老师它还在上面讲它的，这是多么的令人发省啊！中国的教育怎么会这样啊，原以为自己上大学，认为大学是一个获取知识的更广阔的天地，是让自己变的更优秀的地方。如今的课堂却变成这样了。老师他自己确实应该反映自己了。如果老师讲的好的话，就算我在困，我想我也不会睡着。因为我喜欢哈。那样的课堂该是多么让人向往。希望能早点接触这样的老师。就这样跟着老师的什么步骤就上完了两节课，说实话要是没有功底的人来上他的课，基本上是摸不到头绪的。只能让你更加糊涂。在往后就到中午了，放学之后吃过午饭，回到宿舍就感觉有点困，就去睡觉了，对自己放松一下。没想到睡了快两个小时。多么幸福哦。在这两个小时之间自己有点hold不住的感觉，想了很多东西。在梦中我梦见了以前的东西，让自己有点过意不去。我是那么没有珍惜过以前的美好时光，全都流逝了，真的是自己的一大过错。梦到了死去的奶奶，不知道为什么在梦中她却变成了另外一幅模样，是那么的看上去亲切，让我心里震撼许久。自己想去触摸的时候，确实那么的遥远。接着梦到了爷爷，爷爷还是依旧那么的和蔼可亲，他带着自己的那个有点破旧的帽子，但是现实生活中他却是不带帽子的，不知道为什么会这样。看着那破旧的帽子我觉自己好像是要给他买一个新的，不知为何到现在还是没有买。在梦中自己是站在外面的，看着里面的。自己是那么的清楚，是让自己看清了好多东西，虽然梦中的东西是与现实不同的。但总觉着他与现实生活存在着某种关系，总是可以联系起来的。醒来之后感觉自己像是做了一个梦中之梦一样。在那里让自己的思想畅翔，穿错现在与过去，还是珍惜如今的大好时光，去把握好自己的时间好好孝敬他们，让自己不再有遗憾。 喜欢书、喜欢运动、喜欢文字、抬头看路、低头做人，这就是我————张旱文]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[合肥学习感受]]></title>
    <url>%2F2016%2F10%2F27%2Fhefei-websafe-train-think%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;合肥之学习第二天，谈谈自己的感受，自己心里的感受还是蛮有点失望的，感觉来到这没有学到太多的知识，心里总有一种莫名其妙的感觉，在心里作祟。这世界上说的不错的是全部都是利益驱动，大家都是为了利益，虽然表面上是没有，但是最终的目的总是利益。不知什么原因，每次看到利益的争取，我都会有一种不好的感觉，想去逃避，却又不能，总想摆脱的，却又不得不去面对它。争取做到最好。 喜欢书、喜欢运动、喜欢文字、抬头看路、低头做人，这就是我————阿文]]></content>
      <tags>
        <tag>生活</tag>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生活不该出现懒散]]></title>
    <url>%2F1017%2F12%2F01%2Fbirthday-note%2F</url>
    <content type="text"><![CDATA[不知不觉就觉得自己老了，总想着自己还是本科的日子，时间如戏对每个人都是一样，要看我们每个人怎么去演，自己如何去演出属于自己的一支华尔兹，完成自己的当初的想法。数着日子自己也老大不小了，看着自己碌碌无为的，甚是对自己的未来充满科幻。我不知道自己这是在逃避还是在做所谓的挣扎，有时候自己就在想当初的选择是不是值得，有那么几回觉得自己是错了，还有时觉得是对的，甚是无理取闹。拿三年的时间来换一些东西，所做出的抉择，其中的机会成本是我所不能想的。面对生活的百态，看着别人家的孩子，总是有一种莫名奇怪的感觉涌上心头，压的自己都快喘不过气来。心感觉都是在外面的，没有回过自己的肚子一样，生活充满着希望，但也赋予着其他的东西。有时让你感觉世界是很美好，有时很残酷。在该努力的时代，该拼搏的时候，不能放纵自己。纵使最后是最坏的打算，一事无成也好，总对的起自己,相信自己是可以的。总之散漫的自己，不能去放纵,刻苦加油。今天是自己的生日，自己对自己说声生日快乐。以后要快快乐乐。任何事情都要加油，我也希望自己能在这里找到另外的一伴，以后就可以两个人一起过生日啦(^_^)。 喜欢书、喜欢运动、喜欢文字、抬头看路、低头做人，这就是我————阿文]]></content>
      <tags>
        <tag>生活</tag>
        <tag>杂想</tag>
        <tag>状况</tag>
        <tag>心态</tag>
        <tag>努力</tag>
        <tag>未来</tag>
        <tag>迷茫</tag>
      </tags>
  </entry>
</search>
